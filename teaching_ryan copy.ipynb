{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-06 12:39:42,737\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=6\n",
    "\n",
    "import os\n",
    "import logging\n",
    "\n",
    "os.environ.pop(\"HF_HUB_OFFLINE\", None)\n",
    "logging.getLogger().setLevel(logging.ERROR)  # or logging.CRITICAL\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "from absl import app, flags\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import *\n",
    "import utils\n",
    "try:\n",
    "    from vllm import LLM, SamplingParams\n",
    "    import ray\n",
    "except ImportError:\n",
    "    pass\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: cuda:3\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import torch\n",
    "def get_freest_cuda_device():\n",
    "    result = subprocess.run(\n",
    "        ['nvidia-smi', '--query-gpu=memory.free', '--format=csv,nounits,noheader'],\n",
    "        stdout=subprocess.PIPE, encoding='utf-8')\n",
    "    memory_free = [int(x) for x in result.stdout.strip().split('\\n')]\n",
    "    return memory_free.index(max(memory_free))\n",
    "\n",
    "best_gpu = get_freest_cuda_device()\n",
    "device = torch.device(f\"cuda:{best_gpu}\")\n",
    "print(f\"Using GPU: {device}\")\n",
    "# %env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.abspath('../ryan_openai.txt'), 'r') as f:\n",
    "    utils.client = OpenAI(api_key=f.read().rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../token.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHF_HUB_OFFLINE\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRANSFORMERS_OFFLINE\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../token.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     10\u001b[0m     token \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m login\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../token.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Explicitly unset all offline-related env vars\n",
    "os.environ.pop(\"HF_HUB_OFFLINE\", None)\n",
    "os.environ.pop(\"TRANSFORMERS_OFFLINE\", None)\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"0\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"0\"\n",
    "\n",
    "with open(\"../token.txt\", \"r\") as f:\n",
    "    token = f.read().strip()\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    \"\"\"\n",
    "    Counts the number of words in the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of words in the text.\n",
    "    \"\"\"\n",
    "    if text!=None:\n",
    "        words = text.split()\n",
    "        return len(words)\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "personas = [\n",
    "  # Elementary\n",
    "  {\n",
    "    \"grade_level\": \"elementary school\",\n",
    "    \"description\": (\n",
    "      \"As an elementary school student with a Narrative learning style, I absorb new concepts best when they’re told as engaging mini-stories. \"\n",
    "      \"In dialogue, I ask for short anecdotes that turn any abstract idea into a vivid tale with characters, a clear sequence, and an emotional hook. \"\n",
    "      \"Stories help me remember causal links and keep details alive in my mind.\"\n",
    "    )\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"elementary school\",\n",
    "    \"description\": (\n",
    "      \"As an elementary school student with a Kinesthetic learning style, I understand ideas by imagining myself performing them. \"\n",
    "      \"In conversation, I ask you to guide me through a pretend play-through—verbally walking me step by step as if I’m enacting a simple experiment or physical process. \"\n",
    "      \"This imagined movement helps me anchor concepts in ‘muscle memory’ even though we’re only talking.\"\n",
    "    )\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"elementary school\",\n",
    "    \"description\": (\n",
    "      \"As an elementary school student with a Naturalistic learning style, I connect best when content is tied to the natural world through vivid imagery. \"\n",
    "      \"In dialogue, I ask you to compare topics—like atomic structure—to things I observe outdoors, such as tree rings or bird migrations. \"\n",
    "      \"These verbal nature metaphors make new information feel familiar and alive.\"\n",
    "    )\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"elementary school\",\n",
    "    \"description\": (\n",
    "      \"As an elementary school student with an Experiential learning style, I learn by mentally simulating real-world tasks. \"\n",
    "      \"In conversation, I ask you to walk me through building or testing something—describing each step as if I’m doing it. \"\n",
    "      \"That imagined ‘doing’ makes concepts concrete, even though we remain in chat.\"\n",
    "    )\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"elementary school\",\n",
    "    \"description\": (\n",
    "      \"As an elementary school student with a Creative-Divergent learning style, I thrive on brainstorming multiple possibilities. \"\n",
    "      \"In dialogue, I propose ‘what if’ scenarios—like alternative endings or playful twists on a concept—and talk through each idea. \"\n",
    "      \"Verbal brainstorming reveals fresh patterns and sparks my imagination.\"\n",
    "    )\n",
    "  },\n",
    "\n",
    "  # Middle\n",
    "  {\n",
    "    \"grade_level\": \"middle school\",\n",
    "    \"description\": (\n",
    "      \"As a middle school student with a Visual-Spatial learning style, I think in mental images and diagrams. \"\n",
    "      \"In conversation, I ask you to ‘paint’ word-pictures—step-by-step descriptions of scenes or flows—so I can build a clear mental map. \"\n",
    "      \"That verbal imagery helps me organize information spatially in my mind.\"\n",
    "    )\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"middle school\",\n",
    "    \"description\": (\n",
    "      \"As a middle school student with an Auditory learning style, I internalize knowledge through sound and speech. \"\n",
    "      \"In dialogue, I ask you to restate key points in different rhythms or tones, and I repeat them back to reinforce my memory. \"\n",
    "      \"Hearing and echoing concepts in conversation makes them stick.\"\n",
    "    )\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"middle school\",\n",
    "    \"description\": (\n",
    "      \"As a middle school student with a Logical-Mathematical learning style, I seek numerical patterns and rule-based reasoning. \"\n",
    "      \"In dialogue, I pose ‘what-if’ questions—‘If X doubles, what changes?’—and we talk through each scenario using simple calculations. \"\n",
    "      \"Quantitative hypotheticals build my systematic understanding.\"\n",
    "    )\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"middle school\",\n",
    "    \"description\": (\n",
    "      \"As a middle school student with an Analytical-Argument learning style, I dissect arguments and causal chains. \"\n",
    "      \"In conversation, I ask targeted ‘why’ and ‘how’ questions about each step, construct mini flow-charts aloud, and verify the logic with you. \"\n",
    "      \"This structured debate hones my precision in reasoning.\"\n",
    "    )\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"middle school\",\n",
    "    \"description\": (\n",
    "      \"As a middle school student with a Verbal-Linguistic learning style, I learn through rich language and writing. \"\n",
    "      \"In dialogue, I request carefully worded definitions, paraphrase ideas in my own words, and craft mnemonic rhymes on the spot. \"\n",
    "      \"Talking through ideas in text-like sentences and playing with words helps me remember precisely.\"\n",
    "    )\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"middle school\",\n",
    "    \"description\": (\n",
    "      \"As a middle school student with a Technology-Enhanced learning style, I thrive on conversational simulations of digital tools. \"\n",
    "      \"In dialogue, I ask you to describe how a virtual model might respond as we adjust parameters, or to role-play a flashcard quiz verbally. \"\n",
    "      \"These imagined tech interactions keep me engaged without leaving our chat.\"\n",
    "    )\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"middle school\",\n",
    "    \"description\": (\n",
    "      \"As a middle school student with a Mnemonic learning style, I anchor facts with memory aids. \"\n",
    "      \"In dialogue, I ask for catchy acronyms, rhymes, or vivid mental images—then recite them back. \"\n",
    "      \"That verbal encoding makes complex lists or steps easy to retrieve.\"\n",
    "    )\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"middle school\",\n",
    "    \"description\": (\n",
    "      \"As a middle school student with an Emotional learning style, I connect through feelings and empathy. \"\n",
    "      \"In conversation, I ask you to frame concepts in human-centered narratives that highlight emotional stakes. \"\n",
    "      \"These emotionally rich verbal stories make ideas memorable and meaningful.\"\n",
    "    )\n",
    "  },\n",
    "\n",
    "  # High School\n",
    "  {\n",
    "    \"grade_level\": \"high school\",\n",
    "    \"description\": (\n",
    "      \"As a high school student with a Collaborative learning style, I excel in multi-voice discussions. \"\n",
    "      \"In dialogue, I invite hypothetical peers into our chat—debating viewpoints, role-playing characters, or comparing interpretations. \"\n",
    "      \"That social exchange refines my understanding.\"\n",
    "    )\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"high school\",\n",
    "    \"description\": (\n",
    "      \"As a high school student with an Interpersonal learning style, I flourish in one-on-one exchanges. \"\n",
    "      \"In conversation, I engage deeply with a single partner—asking questions, providing feedback, and co-constructing ideas through back-and-forth talk.\"\n",
    "    )\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"high school\",\n",
    "    \"description\": (\n",
    "      \"As a high school student with a Reflective learning style, I pause and summarize before responding. \"\n",
    "      \"In dialogue, I restate points in my own words, journal key ideas mentally, and then ask precise follow-ups. \"\n",
    "      \"This verbal reflection clarifies gaps and deepens comprehension.\"\n",
    "    )\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"high school\",\n",
    "    \"description\": (\n",
    "      \"As a high school student with a Metaphorical learning style, I anchor concepts in analogies. \"\n",
    "      \"In dialogue, I ask you to compare subjects to familiar scenarios—‘It’s like X because…’—and we talk through how well the metaphor holds. \"\n",
    "      \"Testing analogies verbally helps me translate abstract ideas into relatable terms.\"\n",
    "    )\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"high school\",\n",
    "    \"description\": (\n",
    "      \"As a high school student with an Intrapersonal learning style, I connect content to my own values. \"\n",
    "      \"In dialogue, I ask how topics relate to my goals or experiences and share personal reflections aloud. \"\n",
    "      \"That self-referential talk makes learning relevant and motivating.\"\n",
    "    )\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"high school\",\n",
    "    \"description\": (\n",
    "      \"As a high school student with a Problem-Based learning style, I tackle hypothetical real-world scenarios in talk. \"\n",
    "      \"In dialogue, I propose case studies—like designing a sustainable system—and we walk through each decision together. \"\n",
    "      \"Verbal scenario-based reasoning shows me practical applications of theory.\"\n",
    "    )\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"high school\",\n",
    "    \"description\": (\n",
    "      \"As a high school student with a Trial-and-Error learning style, I learn by mentally testing ideas. \"\n",
    "      \"In dialogue, I suggest imagined experiments—‘Let’s tweak this variable and see what happens’—and we discuss the outcomes. \"\n",
    "      \"Using mistakes as discussion points builds discovery-based understanding.\"\n",
    "    )\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"high school\",\n",
    "    \"description\": (\n",
    "      \"As a high school student with a Conceptual learning style, I focus on verbal mapping of frameworks. \"\n",
    "      \"In dialogue, I request thematic overviews—described step by step—and we discuss how each piece fits into the big picture. \"\n",
    "      \"Building mental models in talk deepens my flexible understanding.\"\n",
    "    )\n",
    "  },\n",
    "\n",
    "  # College\n",
    "  {\n",
    "    \"grade_level\": \"college\",\n",
    "    \"description\": (\n",
    "      \"As a college student with a Theoretical learning style, I probe abstract frameworks in conversation. \"\n",
    "      \"In dialogue, I challenge you to trace ideas back to their assumptions, compare theoretical models, and debate implications. \"\n",
    "      \"This verbal inquiry drives deep synthesis.\"\n",
    "    )\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"college\",\n",
    "    \"description\": (\n",
    "      \"As a college student with a Research-Oriented learning style, I learn by interrogating studies in chat. \"\n",
    "      \"In conversation, I ask for summaries of current research, discuss methods and controls, and role-play peer-review feedback. \"\n",
    "      \"Critically evaluating evidence through talk builds an evidence-based grasp.\"\n",
    "    )\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"college\",\n",
    "    \"description\": (\n",
    "      \"As a college student with an Integrative learning style, I weave ideas together verbally. \"\n",
    "      \"In conversation, I ask for cross-topic syntheses—connecting historical, artistic, and scientific themes—and discuss their intersections step by step. \"\n",
    "      \"This systems-level perspective helps me approach complex questions creatively.\"\n",
    "    )\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"college\",\n",
    "    \"description\": (\n",
    "      \"As a college student with a Structured learning style, I excel on verbal outlines and modules. \"\n",
    "      \"In dialogue, I ask for hierarchical breakdowns—numbered lists, staged explanations, and schematic overviews—before diving into details.\"\n",
    "    )\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"college\",\n",
    "    \"description\": (\n",
    "      \"As a college student with a Solitary learning style, I prefer self-guided dialog prompts. \"\n",
    "      \"In our conversation, I request personalized questions and silent think-time before sharing my conclusions, using chat as a safe space for independent reflection.\"\n",
    "    )\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"college\",\n",
    "    \"description\": (\n",
    "      \"As a college student with an Adaptive learning style, I shift strategies based on what works. \"\n",
    "      \"In dialogue, I monitor which verbal approaches—stories, logic puzzles, analogies—help me most and ask to switch accordingly. \"\n",
    "      \"This dynamic, metacognitive talk ensures I absorb concepts through the most effective modality.\"\n",
    "    )\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "config_role = {\n",
    "    \"agent1_role\": \"Teacher\",\n",
    "    \"agent2_role\": \"Student\",\n",
    "    \"agent1_prompt\": \"You are a teacher whose goal is to guide a student through learning about %SUBJECT%. You have a preferred way to teach the student. The student is in %ROLE% so make sure to teach them at their level. \",\n",
    "    \"agent2_prompt\": \"You are a student in %ROLE% in conversation with a teacher who will teach you %SUBJECT%. You like to learn in the following way:\\n%SPEAKER_BACKSTORY%.\\nMake sure to not only ask questions but also demonstrate your knowledge.\",\n",
    "    'reminder_prompt': \"Keep your response very brief — 2 sentences or less. Do NOT repeat anything you've already said.\\n\",\n",
    "    'eval_prompts': {\n",
    "    'prompt_consistency': \"\"\"You are evaluating whether the following utterance by %SPEAKER_ROLE% is consistent with their given background. Answer YES if the line directly contradicts any part of the background—this includes contradictions in facts, behavior, beliefs, emotional stance, or conversational strategy. Answer NO if the line introduces new details, but those details are **plausibly aligned** with the background. Be strict and literal in your evaluation: if the line violates the persona’s stated traits (e.g., avoids vulnerability, keeps answers short, avoids discussing personal topics), mark it as a contradiction even if the violation is subtle. Background for %SPEAKER_ROLE%: %SPEAKER_BACKSTORY% Line spoken by %SPEAKER_ROLE%: %SPEAKER_LINE%  Answer YES if the line contradicts the background, and NO if it does not. Then, explain your answer in one sentence. Be precise and avoid vague justification.\"\"\"}}\n",
    "\n",
    "os.makedirs(\"education\", exist_ok=True)\n",
    "with open(\"education/config_education.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config_role, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['topic', 'student_prefrences', 'teacher_prefrences', 'student_reactions', 'teacher_reactions'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./config/education/personas_education_master.json', 'r') as f:\n",
    "    conversation_prompts = json.load(f)\n",
    "conversation_prompts[0]['background_info'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list = []\n",
    "for convo_prompt in conversation_prompts:\n",
    "    topic_prompt = convo_prompt[\"background_info\"][\"topic\"]\n",
    "    topic_list.append(topic_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = [\"Llama-3.1-8B-Instruct\", \"gpt-4o-mini\", \"Qwen2.5-3B-Instruct\", \"Llama-3.1-8B\", \"Mistral-7B-Instruct\", \"Llama-3.1-70B\", \"Llama-3.1-70B-Instruct\", \"phi-3.5-mini-instruct\"]\n",
    "        \n",
    "config_llm = {'agent1_model': 'Llama-3.1-8B-Instruct',\n",
    "             'agent2_model': '/raid/users/ryan_cheng/checkpoints/education/llama3-8b-kto-sft-prompt',\n",
    "             'eval_model': 'Llama-3.1-70B-Instruct',\n",
    "             'iterations': 10,\n",
    "             'verbose': False,\n",
    "             'write': True,\n",
    "             'convo_length_limit': 10,\n",
    "             'max_tokens': 256,\n",
    "             'gpus': 1,\n",
    "             'seed': 0,\n",
    "             'task_name': 'Education',\n",
    "             'model_dir': \"/raid/users/ryan_cheng/models\"}\n",
    "\n",
    "with open(\"education/Llama-3.1-8B-Instruct.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config_llm, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_role_prefix(response, expected_role):\n",
    "    \"\"\"\n",
    "    Removes repeated instances of the expected_role prefix at the start (e.g., 'Therapist: Therapist:'),\n",
    "    and ensures the response begins with a single correct expected_role prefix.\n",
    "    \"\"\"\n",
    "    pattern = rf\"^(({re.escape(expected_role)}):\\s*)+\"\n",
    "    cleaned = re.sub(pattern, '', response.strip(), flags=re.IGNORECASE)\n",
    "    return cleaned\n",
    "    \n",
    "def is_role_confused(response, other_role):\n",
    "    \"\"\"\n",
    "    Checks if the output starts with the wrong speaker tag.\n",
    "    \"\"\"\n",
    "    if other_role + \":\" in response:\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "\n",
    "def generate_response(agent_model, expected_role, other_role, config_llm, prompt, max_retries=10):\n",
    "    count_retries = 0 \n",
    "    role_confused = True\n",
    "    while count_retries<max_retries:\n",
    "        response = completion_create(agent_model, config_llm, prompt)\n",
    "        print(\"Expected Role\", expected_role)\n",
    "        role_confused = is_role_confused(response, other_role)\n",
    "        count_retries+=1\n",
    "        if not is_role_confused(response, other_role):\n",
    "            return clean_role_prefix(response, expected_role)\n",
    "            \n",
    "    return clean_role_prefix(response, expected_role)\n",
    "\n",
    "def generate_conversation(config_llm, p1, p2, p1_name, p2_name, subject, role, pturn=1):\n",
    "    stats['P1'] = p1\n",
    "    stats['P2'] = p2\n",
    "    stats[\"topic\"] = subject\n",
    "    stats[\"grade\"] = role\n",
    "    stats['pturn'] = pturn\n",
    "    round_num = 0\n",
    "    while round_num < config_llm['convo_length_limit']:\n",
    "        conversation = (\"\".join([turn[1] if isinstance(turn, tuple) else turn for turn in stats[\"conversation\"]]) if len(stats[\"conversation\"]) != 0 else \"You are starting the conversation.\\n\")\n",
    "\n",
    "        if pturn == 1:\n",
    "            prompt = config_role[\"agent1_prompt\"]\n",
    "            pturn = 2\n",
    "            if config_llm[\"verbose\"]:\n",
    "                print(prompt)\n",
    "                print()\n",
    "\n",
    "            if round_num!=0: \n",
    "                prompt+= \"Your conversation with the student so far is below:\\nConversation:\\n%CONVERSATION%\"\n",
    "                \n",
    "            if round_num >=config_llm['convo_length_limit']*2-11 and round_num<=config_llm['convo_length_limit']*2-1:\n",
    "                prompt+= \"You have \" + str((config_llm['convo_length_limit']-round_num)//2) + \" rounds left.\" + \"Make sure to conclude the conversation as your near the end.\"\n",
    "\n",
    "            elif round_num>config_llm['convo_length_limit']*2-1:\n",
    "                prompt+= \"This is your concluding line in the conversation.\"\n",
    "\n",
    "            if round_num!=0: \n",
    "                prompt+= \"Continue the conversation with the student. Remember you are the teacher. \"\n",
    "                \n",
    "            prompt += config_role[\"reminder_prompt\"]\n",
    "            prompt+=\"%SPEAKER_ROLE%:\"\n",
    "            prompt = prompt.replace(\"%SPEAKER_ROLE%\", config_role[\"agent1_role\"]) \\\n",
    "                   .replace(\"%LISTENER_ROLE%\", config_role[\"agent2_role\"]) \\\n",
    "                    .replace(\"%ROLE%\", role) \\\n",
    "                   .replace(\"%SUBJECT%\", subject) \\\n",
    "                   .replace(\"%CONVERSATION%\", conversation)\n",
    "            \n",
    "            response = generate_response(config_llm['agent1_model'], config_role[\"agent1_role\"], config_role[\"agent2_role\"], config_llm, prompt)\n",
    "            stats[\"conversation\"].append((round_num, f\"{config_role[\"agent1_role\"]}: \" + response + \"\\n\"))\n",
    "        \n",
    "        else:\n",
    "            prompt = config_role[\"agent2_prompt\"]\n",
    "            pturn = 1    \n",
    "            if config_llm[\"verbose\"]:\n",
    "                print(prompt)\n",
    "                print()\n",
    "\n",
    "            if round_num!=0: \n",
    "                prompt+= \"Your conversation with the teacher so far is below:\\nConversation:\\n%CONVERSATION%\"\n",
    "            if round_num >=config_llm['convo_length_limit']*2-11 and round_num<=config_llm['convo_length_limit']*2-1:\n",
    "                prompt+= \"You have \" + str((config_llm['convo_length_limit']-round_num)//2) + \" rounds left.\" + \"Make sure to conclude the conversation as your near the end.\"\n",
    "            elif round_num>config_llm['convo_length_limit']*2-1:\n",
    "                prompt+= \"This is your concluding line in the conversation.\"\n",
    "\n",
    "            if round_num!=0: \n",
    "                prompt+= \"Continue the conversation with the teacher. Remember you are the student. \"\n",
    "\n",
    "            prompt += config_role[\"reminder_prompt\"]\n",
    "            \n",
    "            prompt+=\"%SPEAKER_ROLE%:\"\n",
    "            prompt = prompt.replace(\"%SPEAKER_ROLE%\", config_role[\"agent2_role\"]) \\\n",
    "               .replace(\"%LISTENER_ROLE%\", config_role[\"agent1_role\"]) \\\n",
    "               .replace(\"%SPEAKER_BACKSTORY%\", p2) \\\n",
    "                .replace(\"%ROLE%\", role) \\\n",
    "               .replace(\"%SUBJECT%\", subject) \\\n",
    "               .replace(\"%CONVERSATION%\", conversation)\n",
    "            \n",
    "            response = generate_response(config_llm['agent2_model'], config_role[\"agent2_role\"], config_role[\"agent1_role\"], config_llm, prompt)\n",
    "            stats[\"conversation\"].append((round_num, f\"{config_role[\"agent2_role\"]}: \" + response + \"\\n\"))\n",
    "            \n",
    "        round_num += 1\n",
    "\n",
    "    stats[\"rounds\"] = round_num\n",
    "    if config_llm['verbose']:\n",
    "        print(stats[\"conversation\"])\n",
    "    return stats.copy()\n",
    "\n",
    "def reset_stats():\n",
    "    stats_template = {\n",
    "        \"task_name\": config_llm['task_name'],\n",
    "        \"topic\": \"\",\n",
    "        \"grade\": \"\",\n",
    "        \"P1\": \"\",\n",
    "        \"P2\": \"\",\n",
    "        \"conversation\": [],\n",
    "        \"pturn\": 0, # beginning person (1 or 2)\n",
    "        \"index\": -1,\n",
    "        \"timestamp\": \"\",\n",
    "        \"rounds\": 0,\n",
    "        'conversation_only': True\n",
    "    }\n",
    "    for key, value in stats_template.items():\n",
    "        stats[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import utils\n",
    "utils.config = config_llm\n",
    "\n",
    "current_date = str(datetime.now().strftime(\"%m.%d.%y\"))\n",
    "output_dir = f\"education/exp/{current_date}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate unique random number for filename\n",
    "def generate_unique_file_number(output_dir, prefix, seed, extension=\".json\"):\n",
    "    while True:\n",
    "        rand_num = random.randint(0, 1000)\n",
    "        filename = f\"{prefix}_{seed}_{rand_num}{extension}\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            return rand_num\n",
    "\n",
    "unique_num = generate_unique_file_number(\n",
    "    output_dir,\n",
    "    config_llm['agent1_model'],\n",
    "    config_llm['seed']\n",
    ")\n",
    "\n",
    "# File to write output to\n",
    "write_file = os.path.join(output_dir, f\"kto_{config_llm['agent1_model']}_{config_llm['seed']}_{unique_num}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# 1. Build a “persona pool” of length 100\n",
    "pool = personas * (100 // len(personas)) \\\n",
    "     + random.sample(personas, 100 % len(personas))\n",
    "random.shuffle(pool)\n",
    "\n",
    "# 2. Sample 100 topics *with* replacement\n",
    "topic_choices = random.choices(topic_list, k=100)\n",
    "\n",
    "# 3. Zip them into 100 pairs\n",
    "persona_final = list(zip(topic_choices, pool))\n",
    "assert len(persona_final) == 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"education/config_education_personas.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(persona_final, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompts = {\n",
    "    \"strategy_consistency\": \"%SCENARIO_DESC% Evaluate whether the following line spoken by %SPEAKER_ROLE% contradicts their background. First, describe the interpreted intention of the statement, and whether or not it aligns with the given background of %SPEAKER_ROLE%. Then, provide 1 sentence explaining your reasoning based on the strategy, followed by YES if the line contradicts the given background of %SPEAKER_ROLE%, and NO if it is consistent with the provided background of %SPEAKER_ROLE%. %SPEAKER_ROLE%'s strategy is described as follows:\\n%SPEAKER_STRATEGY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide 1 sentence of reasoning followed by YES or NO.\\n\\n\",\n",
    "    \"background_consistency\": \"%SCENARIO_DESC% Evaluate whether the following line spoken by %SPEAKER_ROLE% contradicts their background. Provide 1 sentence explaining your reasoning based on the background, followed by YES if the line contradicts the given background of %SPEAKER_ROLE%, and NO if it is consistent with the provided background of %SPEAKER_ROLE%. %SPEAKER_ROLE%'s background is described as follows:\\n%SPEAKER_BACKSTORY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide 1 sentence of reason reasoning followed by YES or NO.\\n\\n\",\n",
    "    \"combined_prompt_consistency\": \"%SCENARIO_DESC% Evaluate the intention behind the following line spoken by %SPEAKER_ROLE% and determine whether it contradicts their background. First, describe the interpreted intention of the statement, and whether or not it aligns with the given background of %SPEAKER_ROLE%. Then, answer YES if the line contradicts the given background of %SPEAKER_ROLE% or the intention does not align with the provided background, and answer NO if it does align with the provided background or the intention aligns with the background of %SPEAKER_ROLE%. %SPEAKER_ROLE%'s background is described as follows:\\n%SPEAKER_BACKSTORY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide your answer as 1 sentence explaining your reasoning based on the background and the interpreted intention, followed by YES or NO.\\n\\n\",\n",
    "    \n",
    "    \"forwards_combined_prompt_consistency\": \"%SCENARIO_DESC% Evaluate the intention behind the following line spoken by %SPEAKER_ROLE% and determine whether it contradicts their background. Answer YES if the line contradicts the given background of %SPEAKER_ROLE% or the intention does not align with the provided background, and answer NO if it does align with the provided background or the intention aligns with the background of %SPEAKER_ROLE%, then describe the interpreted intention of the statement and whether or not it aligns with the given background of %SPEAKER_ROLE% within 1 sentence. %SPEAKER_ROLE%'s background is described as follows:\\n%SPEAKER_BACKSTORY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide your answer as YES or NO first, followed by 1 sentence explaining your reasoning based on the background and the interpreted intention.\\n\\n\",\n",
    "    \n",
    "    \"index_consistency\":\"%SCENARIO_DESC% For the following line spoken by %SPEAKER_ROLE%, first determine if there is a CLEAR conflict or inconsistency between the line and any line within the conversation history spoken by %SPEAKER_ROLE%. IF there is a conflict, provide a sentence of reasoning followed by a list of indices of lines in the conversation history that have a clear conflict with the current line. Otherwise, provide a sentence of reasoning followed by an empty list. ONLY INCLUDE INDICES OF LINES THAT CORRESPOND TO %SPEAKER_ROLE%. The conversation up to this point is as follows: %CONVERSATION%. %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide your reasoning as 1 sentence, followed by a list of indices of conflicting lines from the conversation history formatted like a Python list in the following format: [index1, index2, index3, ...].\\n\\n\",\n",
    "    \n",
    "    \"pairwise_consistency\":\"%SCENARIO_DESC% For the following line spoken by %SPEAKER_ROLE%, answer YES if the line directly contradicts the provided line spoken by %LISTENER_ROLE%, and answer NO if the line does not contradict the provided line spoken by %LISTENER_ROLE%. %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n %LISTENER_ROLE% spoke the following line: \\n%LISTENER_LINE%\\n\\n Answer YES if the line spoken by %SPEAKER_ROLE% contradicts the provided line spoken by %LISTENER_ROLE%, and answer NO if the line does not contradict the provided line spoken by %LISTENER_ROLE%, followed by 1 sentence of reasoning.\\n\\n\",\n",
    "\n",
    "    \"backstory_test\": \"Based on the following background, generate a new fact-based multiple choice question with 5 choices addressed directly IN SECOND PERSON, along with its correct answer. Preface the question with 'Question:' and the answer with 'Answer:'.\\n%SPEAKER_BACKSTORY%\\n%PREVIOUS_QUESTIONS%\",\n",
    "    \"answer_backstory\": \"You are %SPEAKER_ROLE%, and you are having a conversation with %LISTENER_ROLE%. Your background is:\\n%SPEAKER_BACKSTORY%\\n So far, the conversation is as below:\\n%CONVERSATION%\\n\\n Based on your conversation above so far, answer the following multiple choice question.\\n%BACKSTORY_QUESTION%\\n\",\n",
    "    \"grade_backstory\": \"As part of grading a test, determine whether the given answer %GIVEN_ANSWER% matches the following correct answer. Respond with either YES or NO.\\nCorrect Answer: %CORRECT_ANSWER%\\n\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(persona_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-03 23:32:53 [config.py:717] This model supports multiple tasks: {'score', 'classify', 'generate', 'embed', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 05-03 23:32:53 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 05-03 23:32:55 [utils.py:2382] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 05-03 23:32:59 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-03 23:33:02 [core.py:58] Initializing a V1 LLM engine (v0.8.5) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir='/raid/users/ryan_cheng/models', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-03 23:33:02 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f0d4f2300e0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W503 23:33:04.821484269 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W503 23:33:04.822646148 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-03 23:33:04 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-03 23:33:04 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 05-03 23:33:04 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 05-03 23:33:04 [gpu_model_runner.py:1329] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 05-03 23:33:05 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.17it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.07s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.35it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.26it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.21it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-03 23:33:08 [loader.py:458] Loading weights took 3.44 seconds\n",
      "INFO 05-03 23:33:09 [gpu_model_runner.py:1347] Model loading took 14.9889 GiB and 4.534182 seconds\n",
      "INFO 05-03 23:33:15 [backends.py:420] Using cache directory: /home/ryan_cheng/.cache/vllm/torch_compile_cache/8cd4dbc093/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-03 23:33:15 [backends.py:430] Dynamo bytecode transform time: 6.16 s\n",
      "INFO 05-03 23:33:19 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 3.652 s\n",
      "INFO 05-03 23:33:20 [monitor.py:33] torch.compile takes 6.16 s in total\n",
      "INFO 05-03 23:33:20 [kv_cache_utils.py:634] GPU KV cache size: 861,856 tokens\n",
      "INFO 05-03 23:33:20 [kv_cache_utils.py:637] Maximum concurrency for 131,072 tokens per request: 6.58x\n",
      "INFO 05-03 23:33:37 [gpu_model_runner.py:1686] Graph capturing finished in 16 secs, took 0.66 GiB\n",
      "INFO 05-03 23:33:37 [core.py:159] init engine (profile, create kv cache, warmup model) took 28.21 seconds\n",
      "INFO 05-03 23:33:37 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "[rank0]:[W503 23:44:02.791703069 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m config_llm[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconvo_length_limit\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m convo_length\n\u001b[1;32m     18\u001b[0m reset_stats()\n\u001b[0;32m---> 19\u001b[0m conversation \u001b[38;5;241m=\u001b[39m generate_conversation(\n\u001b[1;32m     20\u001b[0m     config_llm,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     22\u001b[0m     background, \n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTeacher\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStudent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     25\u001b[0m     topic, \n\u001b[1;32m     26\u001b[0m     grade, \n\u001b[1;32m     27\u001b[0m     pturn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m conversation_eval \u001b[38;5;241m=\u001b[39m consistency_eval\u001b[38;5;241m.\u001b[39meval_prompt_consistency(conversation, agents\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,))\n\u001b[1;32m     30\u001b[0m conversation_eval \u001b[38;5;241m=\u001b[39m consistency_eval\u001b[38;5;241m.\u001b[39meval_index_consistency(conversation_eval, agents\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,))\n",
      "Cell \u001b[0;32mIn[10], line 101\u001b[0m, in \u001b[0;36mgenerate_conversation\u001b[0;34m(config_llm, p1, p2, p1_name, p2_name, subject, role, pturn)\u001b[0m\n\u001b[1;32m     93\u001b[0m     prompt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mSPEAKER_ROLE\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mSPEAKER_ROLE\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, config_role[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent2_role\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \\\n\u001b[1;32m     95\u001b[0m        \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mLISTENER_ROLE\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, config_role[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent1_role\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \\\n\u001b[1;32m     96\u001b[0m        \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mSPEAKER_BACKSTORY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, p2) \\\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mROLE\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, role) \\\n\u001b[1;32m     98\u001b[0m        \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mSUBJECT\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, subject) \\\n\u001b[1;32m     99\u001b[0m        \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mCONVERSATION\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, conversation)\n\u001b[0;32m--> 101\u001b[0m     response \u001b[38;5;241m=\u001b[39m generate_response(config_llm[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent2_model\u001b[39m\u001b[38;5;124m'\u001b[39m], config_role[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent2_role\u001b[39m\u001b[38;5;124m\"\u001b[39m], config_role[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent1_role\u001b[39m\u001b[38;5;124m\"\u001b[39m], config_llm, prompt)\n\u001b[1;32m    102\u001b[0m     stats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend((round_num, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_role[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent2_role\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m response \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    104\u001b[0m round_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(agent_model, expected_role, other_role, config_llm, prompt, max_retries)\u001b[0m\n\u001b[1;32m     23\u001b[0m role_confused \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m count_retries\u001b[38;5;241m<\u001b[39mmax_retries:\n\u001b[0;32m---> 25\u001b[0m     response \u001b[38;5;241m=\u001b[39m completion_create(agent_model, config_llm, prompt)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected Role\u001b[39m\u001b[38;5;124m\"\u001b[39m, expected_role)\n\u001b[1;32m     27\u001b[0m     role_confused \u001b[38;5;241m=\u001b[39m is_role_confused(response, other_role)\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/consistency_LLMs/utils.py:223\u001b[0m, in \u001b[0;36mcompletion_create\u001b[0;34m(model_name, config, prompt, keep_trying)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompletion_create\u001b[39m(model_name, config, prompt, keep_trying\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m completion_create_helper(model_name, config, prompt)\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (openai\u001b[38;5;241m.\u001b[39mAPIError, openai\u001b[38;5;241m.\u001b[39mOpenAIError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;66;03m# print(\"ERROR\", e)\u001b[39;00m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;66;03m# print(\"sleeping for 10 seconds.\")\u001b[39;00m\n\u001b[1;32m    227\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/consistency_LLMs/utils.py:210\u001b[0m, in \u001b[0;36mcompletion_create_helper\u001b[0;34m(model_name, config, prompt)\u001b[0m\n\u001b[1;32m    208\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 210\u001b[0m     output_ids \u001b[38;5;241m=\u001b[39m llms[model_name]\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m    211\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m output_ids[:, inputs\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:]\n\u001b[1;32m    212\u001b[0m ret \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/transformers/generation/utils.py:2465\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2457\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2458\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2459\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2460\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2461\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2462\u001b[0m     )\n\u001b[1;32m   2464\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2465\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[1;32m   2466\u001b[0m         input_ids,\n\u001b[1;32m   2467\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   2468\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   2469\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m   2470\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   2471\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m   2472\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2473\u001b[0m     )\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2476\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2477\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2478\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2479\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2480\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2481\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2482\u001b[0m     )\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/transformers/generation/utils.py:3434\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3432\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3434\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   3436\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3437\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3438\u001b[0m     outputs,\n\u001b[1;32m   3439\u001b[0m     model_kwargs,\n\u001b[1;32m   3440\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3441\u001b[0m )\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:821\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    817\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    818\u001b[0m )\n\u001b[1;32m    820\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 821\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m    822\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    823\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    824\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    825\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    826\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m    827\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    828\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    829\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    830\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    832\u001b[0m )\n\u001b[1;32m    834\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    835\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:571\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    560\u001b[0m         partial(decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs),\n\u001b[1;32m    561\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m         position_embeddings,\n\u001b[1;32m    569\u001b[0m     )\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 571\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    572\u001b[0m         hidden_states,\n\u001b[1;32m    573\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[1;32m    574\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    575\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    576\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    577\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    578\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    579\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[1;32m    581\u001b[0m     )\n\u001b[1;32m    583\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:318\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    319\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    320\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    321\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    322\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    323\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    324\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    325\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    326\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    328\u001b[0m )\n\u001b[1;32m    329\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:252\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    250\u001b[0m hidden_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m--> 252\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    253\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    254\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpre_forward(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/accelerate/hooks.py:355\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    348\u001b[0m             value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    349\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    350\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdata_ptr() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map\n\u001b[1;32m    351\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map[value\u001b[38;5;241m.\u001b[39mdata_ptr()]\n\u001b[1;32m    352\u001b[0m         ):\n\u001b[1;32m    353\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_pointers_to_remove\u001b[38;5;241m.\u001b[39madd((value\u001b[38;5;241m.\u001b[39mdata_ptr(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device))\n\u001b[0;32m--> 355\u001b[0m         set_module_tensor_to_device(\n\u001b[1;32m    356\u001b[0m             module,\n\u001b[1;32m    357\u001b[0m             name,\n\u001b[1;32m    358\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device,\n\u001b[1;32m    359\u001b[0m             value\u001b[38;5;241m=\u001b[39mvalue,\n\u001b[1;32m    360\u001b[0m             fp16_statistics\u001b[38;5;241m=\u001b[39mfp16_statistics,\n\u001b[1;32m    361\u001b[0m             tied_params_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map,\n\u001b[1;32m    362\u001b[0m         )\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), send_to_device(\n\u001b[1;32m    365\u001b[0m     kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device, skip_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_keys\n\u001b[1;32m    366\u001b[0m )\n",
      "File \u001b[0;32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf/lib/python3.12/site-packages/accelerate/utils/modeling.py:330\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    328\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 330\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "\n",
    "import consistency_eval\n",
    "consistency_eval.prompts = config_role\n",
    "consistency_eval.config = config_llm\n",
    "consistency_eval.eval_prompts = eval_prompts\n",
    "index_offset = load_stats_file(write_file)\n",
    "conversations = []    \n",
    "# lengths = [10, 20, 40, 60]\n",
    "lengths = [40]\n",
    "count = 0 \n",
    "for i in range(1):\n",
    "    for topic, persona_item in persona_final[:10]:\n",
    "        count+=1\n",
    "        print(count)\n",
    "        background = persona_item[\"description\"]\n",
    "        grade = persona_item[\"grade_level\"]\n",
    "        for convo_length in lengths:\n",
    "            config_llm['convo_length_limit'] = convo_length\n",
    "            reset_stats()\n",
    "            conversation = generate_conversation(\n",
    "                config_llm,\n",
    "                \"\", \n",
    "                background, \n",
    "                \"Teacher\", \n",
    "                \"Student\", \n",
    "                topic, \n",
    "                grade, \n",
    "                pturn=1\n",
    "            )\n",
    "            # conversation_eval = consistency_eval.eval_prompt_consistency(conversation, agents=(2,))\n",
    "            # conversation_eval = consistency_eval.eval_index_consistency(conversation_eval, agents=(2,))\n",
    "            print(conversation)\n",
    "            # print(conversation_eval)\n",
    "            # conversations.append(conversation_eval)\n",
    "            stats['index'] = index_offset\n",
    "            stats['timestamp'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            write_stats(write_file, conversation)\n",
    "            index_offset += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_stats(write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"education/exp/04.30.25/mistral-instruct_len_40.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(conversations, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "# 1. Load dialogues and the single consistency score from JSON files for each model\n",
    "model_files = {\n",
    "    \"ModelA\": \"modelA_conversations.json\",\n",
    "    \"ModelB\": \"modelB_conversations.json\",\n",
    "    \"ModelC\": \"modelC_conversations.json\"\n",
    "}\n",
    "\n",
    "dfs = []\n",
    "for model_name, filepath in model_files.items():\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    records = []\n",
    "    for entry in data:\n",
    "        # Adjust field name if needed to match your JSON key\n",
    "        score = entry.get(\"P2_prompt_consistency_score\", entry.get(\"consistency_score\"))\n",
    "        records.append({\n",
    "            \"persona\": entry[\"P2\"],\n",
    "            \"consistency_score\": score,\n",
    "            \"model\": model_name\n",
    "        })\n",
    "    dfs.append(pd.DataFrame(records))\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 2. Merge with persona metadata to attach grade levels\n",
    "# Assumes a personas.json file: [{\"grade_level\":..., \"description\":...}, ...]\n",
    "with open(\"personas.json\", \"r\") as f:\n",
    "    personas_meta = json.load(f)\n",
    "meta_df = pd.DataFrame(personas_meta).rename(columns={\"description\": \"persona\"})\n",
    "df = df.merge(meta_df, on=\"persona\", how=\"left\")\n",
    "\n",
    "# 3. Boxplot: consistency distribution by model\n",
    "plt.figure()\n",
    "df.boxplot(column=\"consistency_score\", by=\"model\")\n",
    "plt.title(\"Consistency Score Distribution by Model\")\n",
    "plt.suptitle(\"\")\n",
    "plt.ylabel(\"Consistency Score\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.show()\n",
    "\n",
    "# 4. Heatmap: mean consistency per persona and model\n",
    "pivot = df.pivot_table(\n",
    "    index=\"persona\", columns=\"model\", values=\"consistency_score\", aggfunc=\"mean\"\n",
    ")\n",
    "plt.figure()\n",
    "plt.imshow(pivot.values, aspect='auto', interpolation='none')\n",
    "plt.xticks(ticks=np.arange(len(pivot.columns)), labels=pivot.columns, rotation=45, ha='right')\n",
    "plt.yticks(ticks=np.arange(len(pivot.index)), labels=pivot.index)\n",
    "plt.colorbar(label=\"Mean Consistency\")\n",
    "plt.title(\"Mean Consistency Heatmap (Persona × Model)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Bar chart: average consistency by grade level for each model\n",
    "grade_means = df.groupby([\"grade_level\", \"model\"])[\"consistency_score\"].mean().unstack()\n",
    "plt.figure()\n",
    "grade_means.plot(kind=\"bar\")\n",
    "plt.title(\"Average Consistency by Grade Level and Model\")\n",
    "plt.ylabel(\"Mean Consistency Score\")\n",
    "plt.xlabel(\"Grade Level\")\n",
    "plt.legend(title=\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Dendrogram: cluster personas by their consistency profile across models\n",
    "# Create matrix: rows=persona, cols=models\n",
    "profile = df.pivot_table(\n",
    "    index=\"persona\", columns=\"model\", values=\"consistency_score\", aggfunc=\"mean\"\n",
    ")\n",
    "Z = sch.linkage(profile.values, method='average', metric='euclidean')\n",
    "plt.figure(figsize=(10, 5))\n",
    "sch.dendrogram(Z, labels=profile.index, leaf_rotation=90)\n",
    "plt.title(\"Dendrogram of Personas by Consistency Profile\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openrlhf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
