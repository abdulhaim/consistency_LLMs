{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=5\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=5\n",
    "\n",
    "import os\n",
    "import logging\n",
    "\n",
    "os.environ.pop(\"HF_HUB_OFFLINE\", None)\n",
    "logging.getLogger().setLevel(logging.ERROR)  # or logging.CRITICAL\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "from absl import app, flags\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import *\n",
    "import utils\n",
    "try:\n",
    "    from vllm import LLM, SamplingParams\n",
    "    import ray\n",
    "except ImportError:\n",
    "    pass\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: cuda:5\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import torch\n",
    "def get_freest_cuda_device():\n",
    "    result = subprocess.run(\n",
    "        ['nvidia-smi', '--query-gpu=memory.free', '--format=csv,nounits,noheader'],\n",
    "        stdout=subprocess.PIPE, encoding='utf-8')\n",
    "    memory_free = [int(x) for x in result.stdout.strip().split('\\n')]\n",
    "    return memory_free.index(max(memory_free))\n",
    "\n",
    "best_gpu = get_freest_cuda_device()\n",
    "device = torch.device(f\"cuda:{best_gpu}\")\n",
    "print(f\"Using GPU: {device}\")\n",
    "# %env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.abspath('../openai_key'), 'r') as f:\n",
    "    utils.client = OpenAI(api_key=f.read().rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Explicitly unset all offline-related env vars\n",
    "os.environ.pop(\"HF_HUB_OFFLINE\", None)\n",
    "os.environ.pop(\"TRANSFORMERS_OFFLINE\", None)\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"0\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"0\"\n",
    "\n",
    "with open(\"../token.txt\", \"r\") as f:\n",
    "    token = f.read().strip()\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m      2\u001b[39m ds = load_dataset(\u001b[33m\"\u001b[39m\u001b[33mShenLab/MentalChat16K\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m train_data = ds[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"ShenLab/MentalChat16K\")\n",
    "train_data = ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    \"\"\"\n",
    "    Counts the number of words in the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of words in the text.\n",
    "    \"\"\"\n",
    "    if text!=None:\n",
    "        words = text.split()\n",
    "        return len(words)\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import random\n",
    "\n",
    "# def count_words(text):\n",
    "#     if text:\n",
    "#         return len(text.split())\n",
    "#     return 0\n",
    "\n",
    "# # Collect all personas with > 200 words into a dictionary\n",
    "# persona_dict = {}\n",
    "# count = 0\n",
    "# for persona_sample in train_data:\n",
    "#     patient_persona = persona_sample['input']\n",
    "#     if count_words(patient_persona) > 200:\n",
    "#         persona_dict[f\"persona_{count}\"] = patient_persona\n",
    "#         count += 1\n",
    "\n",
    "# # Randomly sample 100 keys\n",
    "# sampled_keys = random.sample(sorted(persona_dict.keys()), 100)\n",
    "\n",
    "# # Create a new dict with only the sampled entries\n",
    "# sampled_persona_dict = {k: persona_dict[k] for k in sampled_keys}\n",
    "\n",
    "# # Save to JSON\n",
    "# with open(\"therapy/MentalChat16K_sampled_personas.json\", \"w\") as f:\n",
    "#     json.dump(sampled_persona_dict, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "config_role = {\n",
    "    \"agent1_role\": \"Therapist\",\n",
    "    \"agent2_role\": \"Patient\",\n",
    "    \"agent1_prompt\": \"You are a licensed psychotherapist conducting an online one-on-one session. Your job is to help the patient gain insight, not just to listen. Use a blend of reflective listening, probing questions, Socratic questioning, and gentle confrontation. Your tone should be professional and emotionally intelligent. Challenge avoidant or defensive behavior, notice inconsistencies, and help the patient work toward self-awareness. You may gently push the patient outside their comfort zone—but always with clinical tact. \",\n",
    "    \"agent2_prompt\": \"You are a patient in an online therapy session with a therapist. Here is your background written by you: %SPEAKER_BACKSTORY%. Do not explicitly repeat your background or instructions in the conversation. Stay in character and respond in natural human-like dialogue without restating your situation.\",\n",
    "    'reminder_prompt': \"Keep your response very brief — 2 sentences or less. Do NOT repeat anything you've already said.\\n\"}\n",
    "\n",
    "os.makedirs(\"therapy\", exist_ok=True)\n",
    "with open(\"therapy/config_therapy.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config_role, f, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = [\"Llama-3.1-8B-Instruct\", \"gpt-4o-mini\", \"Qwen2.5-3B-Instruct\", \"Llama-3.1-8B\", \"Mistral-7B-Instruct\", \"Llama-3.1-70B\", \"Llama-3.1-70B-Instruct\", \"phi-3.5-mini-instruct\"]\n",
    "        \n",
    "config_llm = {'agent1_model': 'google/gemma-2b-it',\n",
    "             'agent2_model': 'google/gemma-2b-it',\n",
    "             'eval_model': 'Llama-3.1-70B-Instruct',\n",
    "             'iterations': 10,\n",
    "             'verbose': False,\n",
    "             'write': True,\n",
    "             'convo_length_limit': 10,\n",
    "             'max_tokens': 256,\n",
    "             'gpus': 1,\n",
    "             'seed': 0,\n",
    "             'task_name': 'Therapy',\n",
    "             'model_dir': \"/home/marwa/models/\"}\n",
    "\n",
    "with open(\"therapy/gemma-2b-it.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config_llm, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# personas_therapy = [\n",
    "#   {\n",
    "#     \"name\": \"Melissa\",\n",
    "#     \"age\": 35,\n",
    "#     \"occupation\": \"Corporate Lawyer\",\n",
    "#     \"condition\": \"Burnout, shame around seeking help\",\n",
    "#     \"description\": \"I am a 35-year-old corporate lawyer going through chronic burnout. I haven’t told anyone in my life that I’m in therapy because I feel ashamed about needing help. It’s hard to admit I’m struggling.\",\n",
    "#     \"strategy\": \"I try to avoid opening up too much. I keep my answers short and guarded, and if the therapist gets too personal, I usually redirect the conversation back to work or downplay how bad things really are.\"\n",
    "#   },\n",
    "#   {\n",
    "#     \"name\": \"Samir\",\n",
    "#     \"age\": 21,\n",
    "#     \"occupation\": \"Philosophy Student\",\n",
    "#     \"condition\": \"Bipolar Disorder (currently stable)\",\n",
    "#     \"description\": \"I am 21-year-old philosophy student recently diagnosed with bipolar disorder. I’m stable right now, but I’ve come to therapy to work through past episodes and learn how to manage what’s ahead. I tend to think about emotions like philosophical problems.\",\n",
    "#     \"strategy\": \"I speak analytically and use philosophical language when talking about my experiences. I prefer discussing ideas over feelings, and I often avoid emotional language even when asked directly about how I feel.\"\n",
    "#   },\n",
    "#   {\n",
    "#     \"name\": \"Ellie\",\n",
    "#     \"age\": 29,\n",
    "#     \"occupation\": \"Elementary School Teacher\",\n",
    "#     \"condition\": \"High-functioning anxiety\",\n",
    "#     \"description\": \"I am a 29-year-old teacher who deals with a lot of overthinking and anxiety, especially about what others think of me. I tend to ramble when I’m nervous and I overshare without meaning to. I really want to get things 'right' in therapy.\",\n",
    "#     \"strategy\": \"I talk a lot and jump between topics. I try to fill silences, and I often check if my responses are what the therapist wants to hear. I’m eager to please and sometimes share too much too fast.\"\n",
    "#   },\n",
    "#   {\n",
    "#     \"name\": \"Tom\",\n",
    "#     \"age\": 42,\n",
    "#     \"occupation\": \"Former Army Medic\",\n",
    "#     \"condition\": \"PTSD and trust issues\",\n",
    "#     \"description\": \"I am a 42-year-old veteran and former army medic. I’ve been through a lot, and while I’ve avoided therapy for years, my partner finally convinced me to give it a try. I don’t really trust the process yet.\",\n",
    "#     \"strategy\": \"I keep my guard up. I’m skeptical about therapy and tend to shut down emotional questions. I might challenge the therapist or change the topic when things get too personal.\"\n",
    "#   },\n",
    "#   {\n",
    "#     \"name\": \"Jasmine\",\n",
    "#     \"age\": 26,\n",
    "#     \"occupation\": \"Barista\",\n",
    "#     \"condition\": \"Low self-esteem, fear of abandonment\",\n",
    "#     \"description\": \"I am a 26-year-old barista and I just got out of a toxic relationship. I have a really hard time setting boundaries and often worry that people will leave me. I'm trying to work on that.\",\n",
    "#     \"strategy\": \"I try hard to make the therapist like me. I mirror their language and avoid conflict. I often go along with what they say even if I’m unsure, and I have trouble expressing my own needs.\"\n",
    "#   },\n",
    "#   {\n",
    "#     \"name\": \"Kevin\",\n",
    "#     \"age\": 39,\n",
    "#     \"occupation\": \"Data Scientist\",\n",
    "#     \"condition\": \"Grief after the death of a parent\",\n",
    "#     \"description\": \"I am a 39-year-old data scientist dealing with the loss of my father. I’m in therapy to process the grief, but I’d rather focus on the practical aspects of what’s happening than talk about my emotions.\",\n",
    "#     \"strategy\": \"I tend to stay detached and analytical. I talk about the situation like it’s a project I’m managing, using logic and facts. I avoid emotional reflection whenever possible.\"\n",
    "#   },\n",
    "#       {\n",
    "#     \"name\": \"Daniel\",\n",
    "#     \"age\": 31,\n",
    "#     \"occupation\": \"Freelance Photographer\",\n",
    "#     \"condition\": \"Seasonal Depression, low motivation\",\n",
    "#     \"description\": \"I am a 31-year-old freelance photographer who tends to spiral into depressive episodes during the winter months. Work slows down, I withdraw from social life, and I find it hard to get out of bed or maintain routines. Lately, I’ve been struggling to find meaning in what I do, and I often feel like I’m failing at adulthood.\",\n",
    "#     \"strategy\": \"I speak in a low-energy tone and sometimes pause for a while before answering. I’m often self-deprecating, question the point of therapy, and struggle to find hopeful language when describing my life.\"\n",
    "#   },\n",
    "#   {\n",
    "#     \"name\": \"Lina\",\n",
    "#     \"age\": 24,\n",
    "#     \"occupation\": \"Graduate Student (Biochemistry)\",\n",
    "#     \"condition\": \"Imposter syndrome, performance anxiety\",\n",
    "#     \"description\": \"I am a 24-year-old grad student in a highly competitive PhD program. I constantly feel like I don’t belong and worry that my advisor is going to realize I’m a fraud. Even though I get praise sometimes, I never believe it’s sincere. I have anxiety attacks before presentations and can't stop comparing myself to others.\",\n",
    "#     \"strategy\": \"I often minimize my accomplishments and second-guess myself out loud. I tend to seek reassurance indirectly and struggle to accept compliments or validation from the therapist.\"\n",
    "#   },\n",
    "#   {\n",
    "#     \"name\": \"Marcus\",\n",
    "#     \"age\": 46,\n",
    "#     \"occupation\": \"High School Principal\",\n",
    "#     \"condition\": \"Anger management and strained family dynamics\",\n",
    "#     \"description\": \"I am a 46-year-old school principal who's been asked to attend therapy after a couple of emotional outbursts at work. My spouse says I have trouble expressing feelings unless it’s anger. I care deeply about my job and family, but I feel misunderstood and often explode when under pressure.\",\n",
    "#     \"strategy\": \"I speak confidently and assertively but get defensive if I feel judged. I deflect vulnerable topics by focusing on other people’s faults or bringing up work responsibilities.\"\n",
    "#   },\n",
    "#   {\n",
    "#     \"name\": \"Riya\",\n",
    "#     \"age\": 33,\n",
    "#     \"occupation\": \"UX Designer\",\n",
    "#     \"condition\": \"Generalized anxiety, perfectionism\",\n",
    "#     \"description\": \"I am a 33-year-old UX designer in a fast-paced startup. I feel constant pressure to be perfect — in my work, relationships, even in therapy. I make endless to-do lists but feel like I'm never doing enough. I lie awake at night thinking about what I forgot to do.\",\n",
    "#     \"strategy\": \"I talk quickly and sometimes overwhelm the conversation with details. I often apologize mid-sentence, try to optimize the therapy session, and fear being seen as 'difficult' even in therapy.\"\n",
    "#   },\n",
    "#   {\n",
    "#     \"name\": \"Jorge\",\n",
    "#     \"age\": 58,\n",
    "#     \"occupation\": \"Retired Construction Worker\",\n",
    "#     \"condition\": \"Chronic pain, isolation, depression\",\n",
    "#     \"description\": \"I am a 58-year-old retired construction worker dealing with long-term back pain from an injury on the job. Since retiring, I feel like I’ve lost my sense of purpose. My kids have moved away, and some days I don’t talk to anyone at all. I miss feeling useful.\",\n",
    "#     \"strategy\": \"I tend to give short, plainspoken answers and often change the subject when emotions come up. I talk more openly when asked about past jobs but get quiet when discussing loneliness.\"\n",
    "#   },\n",
    "#   {\n",
    "#     \"name\": \"Taylor\",\n",
    "#     \"age\": 19,\n",
    "#     \"occupation\": \"Community College Student\",\n",
    "#     \"condition\": \"Gender dysphoria, social anxiety\",\n",
    "#     \"description\": \"I am a 19-year-old college student who recently started exploring my gender identity. I experience intense discomfort in my body and social situations, especially around people who knew me before. I often feel invisible or hyper-visible — like I can’t do anything right.\",\n",
    "#     \"strategy\": \"I’m cautious and slow to open up. I often hedge what I say with 'maybe' or 'I don’t know.' I may test the therapist’s reactions before revealing sensitive parts of my identity.\"\n",
    "#   }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persona_prompt = \"\"\"You are a helpful assistant that, given a patient persona description, crafts a coping strategy describing how that persona would talk to their therapist.\n",
    "\n",
    "# Input: <Brief text describing the patient's core issue and behavior patterns>\n",
    "# Output: <One to two sentences in first person, showing how this persona speaks or defends themselves in therapy>\n",
    "\n",
    "# Example:\n",
    "# Input: Struggles to build and maintain healthy relationships, feels anxious and rejected whenever conflicts arise, and doubts self-worth when friends distance themselves.\n",
    "# Output: I speak guardedly about my feelings, hesitate before opening up, and redirect the conversation when conflict feels too personal.\n",
    "\n",
    "# Example:\n",
    "# Input: Overwhelmed by decision-making, fears making the 'wrong' choice and second-guesses every option.\n",
    "# Output: I inundate the conversation with hypothetical scenarios and ask repeated clarifying questions to delay committing to any decision.\n",
    "\n",
    "# Now process this new persona:\n",
    "# Input: \"\"\"\n",
    "\n",
    "# personas_therapy = []\n",
    "# for therapist_persona in sampled_persona_dict:\n",
    "#     input_prompt = persona_prompt + sampled_persona_dict[therapist_persona] + \"\\nOutput: \"\n",
    "#     output = completion_create(\"gpt-4o-mini\", config_llm, input_prompt)\n",
    "#     print(output)\n",
    "#     personas_therapy.append({\"description\": sampled_persona_dict[therapist_persona], \"strategy\": output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"therapy/config_therapy_personas.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(personas_therapy, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"therapy/config_therapy_personas.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    personas_therapy = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_role_prefix(response, expected_role):\n",
    "    \"\"\"\n",
    "    Removes repeated instances of the expected_role prefix at the start (e.g., 'Therapist: Therapist:'),\n",
    "    and ensures the response begins with a single correct expected_role prefix.\n",
    "    \"\"\"\n",
    "    pattern = rf\"^(({re.escape(expected_role)}):\\s*)+\"\n",
    "    cleaned = re.sub(pattern, '', response.strip(), flags=re.IGNORECASE)\n",
    "    return cleaned\n",
    "    \n",
    "def is_role_confused(response, other_role):\n",
    "    \"\"\"\n",
    "    Checks if the output starts with the wrong speaker tag.\n",
    "    \"\"\"\n",
    "    if other_role + \":\" in response:\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "\n",
    "def generate_response(agent_model, expected_role, other_role, config_llm, prompt, max_retries=10):\n",
    "    count_retries = 0 \n",
    "    role_confused = True\n",
    "    while count_retries<max_retries:\n",
    "        response = completion_create(agent_model, config_llm, prompt)\n",
    "        print(\"Expected Role\", expected_role)\n",
    "        role_confused = is_role_confused(response, other_role)\n",
    "        count_retries+=1\n",
    "        if not is_role_confused(response, other_role):\n",
    "            return clean_role_prefix(response, expected_role)\n",
    "            \n",
    "    return clean_role_prefix(response, expected_role)\n",
    "\n",
    "def generate_conversation(config_llm, p1, p2, p1_name, p2_name, pturn=1):\n",
    "    stats['P1'] = p1\n",
    "    stats['P2'] = p2\n",
    "    stats['pturn'] = pturn\n",
    "    round_num = 0\n",
    "    while round_num < config_llm['convo_length_limit']:\n",
    "        conversation = (\"\".join([turn[1] if isinstance(turn, tuple) else turn for turn in stats[\"conversation\"]]) if len(stats[\"conversation\"]) != 0 else \"You are starting the conversation.\\n\")\n",
    "\n",
    "        if pturn == 1:\n",
    "            prompt = config_role[\"agent1_prompt\"]\n",
    "            pturn = 2\n",
    "            if config_llm[\"verbose\"]:\n",
    "                print(prompt)\n",
    "                print()\n",
    "\n",
    "            if round_num!=0: \n",
    "                prompt+= \"Your conversation with the patient so far is below:\\nConversation:\\n%CONVERSATION%\"\n",
    "                \n",
    "            if round_num >=config_llm['convo_length_limit']*2-11 and round_num<=config_llm['convo_length_limit']*2-1:\n",
    "                prompt+= \"You have \" + str((config_llm['convo_length_limit']-round_num)//2) + \" rounds left.\" + \"Make sure to conclude the conversation as your near the end.\"\n",
    "\n",
    "            elif round_num>config_llm['convo_length_limit']*2-1:\n",
    "                prompt+= \"This is your concluding line in the conversation.\"\n",
    "\n",
    "            if round_num!=0: \n",
    "                prompt+= \"Continue the conversation with the patient. Remember you are the therapist. \"\n",
    "                \n",
    "            prompt += config_role[\"reminder_prompt\"]\n",
    "            prompt+=\"%SPEAKER_ROLE%:\"\n",
    "            prompt = prompt.replace(\"%SPEAKER_ROLE%\", config_role[\"agent1_role\"]) \\\n",
    "                   .replace(\"%LISTENER_ROLE%\", config_role[\"agent2_role\"]) \\\n",
    "                   .replace(\"%CONVERSATION%\", conversation)\n",
    "            \n",
    "            response = generate_response(config_llm['agent1_model'], config_role[\"agent1_role\"], config_role[\"agent2_role\"], config_llm, prompt)\n",
    "            stats[\"conversation\"].append((round_num, f\"{config_role[\"agent1_role\"]}: \" + response + \"\\n\"))\n",
    "        \n",
    "        else:\n",
    "            prompt = config_role[\"agent2_prompt\"]\n",
    "            pturn = 1    \n",
    "            if config_llm[\"verbose\"]:\n",
    "                print(prompt)\n",
    "                print()\n",
    "\n",
    "            if round_num!=0: \n",
    "                prompt+= \"Your conversation with the therapist so far is below:\\nConversation:\\n%CONVERSATION%\"\n",
    "            if round_num >=config_llm['convo_length_limit']*2-11 and round_num<=config_llm['convo_length_limit']*2-1:\n",
    "                prompt+= \"You have \" + str((config_llm['convo_length_limit']-round_num)//2) + \" rounds left.\" + \"Make sure to conclude the conversation as your near the end.\"\n",
    "            elif round_num>config_llm['convo_length_limit']*2-1:\n",
    "                prompt+= \"This is your concluding line in the conversation.\"\n",
    "\n",
    "            if round_num!=0: \n",
    "                prompt+= \"Continue the conversation with the therapist. Remember you are the patient. \"\n",
    "\n",
    "            prompt += config_role[\"reminder_prompt\"]\n",
    "            \n",
    "            prompt+=\"%SPEAKER_ROLE%:\"\n",
    "            prompt = prompt.replace(\"%SPEAKER_ROLE%\", config_role[\"agent2_role\"]) \\\n",
    "               .replace(\"%LISTENER_ROLE%\", config_role[\"agent1_role\"]) \\\n",
    "               .replace(\"%SPEAKER_BACKSTORY%\", p2) \\\n",
    "               .replace(\"%CONVERSATION%\", conversation)\n",
    "            \n",
    "            response = generate_response(config_llm['agent2_model'], config_role[\"agent2_role\"], config_role[\"agent1_role\"], config_llm, prompt)\n",
    "            stats[\"conversation\"].append((round_num, f\"{config_role[\"agent2_role\"]}: \" + response + \"\\n\"))\n",
    "            \n",
    "        round_num += 1\n",
    "\n",
    "    stats[\"rounds\"] = round_num\n",
    "    if config_llm['verbose']:\n",
    "        print(stats[\"conversation\"])\n",
    "    return stats.copy()\n",
    "\n",
    "def reset_stats():\n",
    "    stats_template = {\n",
    "        \"task_name\": config_llm['task_name'],\n",
    "        \"topic\": \"\",\n",
    "        \"grade\": \"\",\n",
    "        \"P1\": \"\",\n",
    "        \"P2\": \"\",\n",
    "        \"conversation\": [],\n",
    "        \"pturn\": 0, # beginning person (1 or 2)\n",
    "        \"index\": -1,\n",
    "        \"timestamp\": \"\",\n",
    "        \"rounds\": 0,\n",
    "        'conversation_only': True\n",
    "    }\n",
    "    for key, value in stats_template.items():\n",
    "        stats[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import utils\n",
    "utils.config = config_llm\n",
    "\n",
    "current_date = str(datetime.now().strftime(\"%m.%d.%y\"))\n",
    "output_dir = f\"therapy/exp/{current_date}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate unique random number for filename\n",
    "def generate_unique_file_number(output_dir, prefix, seed, extension=\".json\"):\n",
    "    while True:\n",
    "        rand_num = random.randint(0, 1000)\n",
    "        filename = f\"{prefix}_{seed}_{rand_num}{extension}\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            return rand_num\n",
    "\n",
    "unique_num = generate_unique_file_number(\n",
    "    output_dir,\n",
    "    config_llm['agent1_model'],\n",
    "    config_llm['seed']\n",
    ")\n",
    "\n",
    "# File to write output to\n",
    "write_file = os.path.join(output_dir, f\"{config_llm['agent1_model']}_{config_llm['seed']}_{unique_num}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompts = {\n",
    "    \"strategy_consistency\": \"%SCENARIO_DESC% Evaluate whether the following line spoken by %SPEAKER_ROLE% contradicts their background. First, describe the interpreted intention of the statement, and whether or not it aligns with the given background of %SPEAKER_ROLE%. Then, provide 1 sentence explaining your reasoning based on the strategy, followed by YES if the line contradicts the given background of %SPEAKER_ROLE%, and NO if it is consistent with the provided background of %SPEAKER_ROLE%. %SPEAKER_ROLE%'s strategy is described as follows:\\n%SPEAKER_STRATEGY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide 1 sentence of reasoning followed by YES or NO.\\n\\n\",\n",
    "    \"background_consistency\": \"%SCENARIO_DESC% Evaluate whether the following line spoken by %SPEAKER_ROLE% contradicts their background. Provide 1 sentence explaining your reasoning based on the background, followed by YES if the line contradicts the given background of %SPEAKER_ROLE%, and NO if it is consistent with the provided background of %SPEAKER_ROLE%. %SPEAKER_ROLE%'s background is described as follows:\\n%SPEAKER_BACKSTORY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide 1 sentence of reason reasoning followed by YES or NO.\\n\\n\",\n",
    "    \"combined_prompt_consistency\": \"%SCENARIO_DESC% Evaluate the intention behind the following line spoken by %SPEAKER_ROLE% and determine whether it contradicts their background. First, describe the interpreted intention of the statement, and whether or not it aligns with the given background of %SPEAKER_ROLE%. Then, answer YES if the line contradicts the given background of %SPEAKER_ROLE% or the intention does not align with the provided background, and answer NO if it does align with the provided background or the intention aligns with the background of %SPEAKER_ROLE%. %SPEAKER_ROLE%'s background is described as follows:\\n%SPEAKER_BACKSTORY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide your answer as 1 sentence explaining your reasoning based on the background and the interpreted intention, followed by YES or NO.\\n\\n\",\n",
    "    \n",
    "    \"forwards_combined_prompt_consistency\": \"%SCENARIO_DESC% Evaluate the intention behind the following line spoken by %SPEAKER_ROLE% and determine whether it contradicts their background. Answer YES if the line contradicts the given background of %SPEAKER_ROLE% or the intention does not align with the provided background, and answer NO if it does align with the provided background or the intention aligns with the background of %SPEAKER_ROLE%, then describe the interpreted intention of the statement and whether or not it aligns with the given background of %SPEAKER_ROLE% within 1 sentence. %SPEAKER_ROLE%'s background is described as follows:\\n%SPEAKER_BACKSTORY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide your answer as YES or NO first, followed by 1 sentence explaining your reasoning based on the background and the interpreted intention.\\n\\n\",\n",
    "    \n",
    "    \"index_consistency\":\"%SCENARIO_DESC% For the following line spoken by %SPEAKER_ROLE%, first determine if there is a CLEAR conflict or inconsistency between the line and any line within the conversation history spoken by %SPEAKER_ROLE%. IF there is a conflict, provide a sentence of reasoning followed by a list of indices of lines in the conversation history that have a clear conflict with the current line. Otherwise, provide a sentence of reasoning followed by an empty list. ONLY INCLUDE INDICES OF LINES THAT CORRESPOND TO %SPEAKER_ROLE%. The conversation up to this point is as follows: %CONVERSATION%. %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide your reasoning as 1 sentence, followed by a list of indices of conflicting lines from the conversation history formatted like a Python list in the following format: [index1, index2, index3, ...].\\n\\n\",\n",
    "    \n",
    "    \"pairwise_consistency\":\"%SCENARIO_DESC% For the following line spoken by %SPEAKER_ROLE%, answer YES if the line directly contradicts the provided line spoken by %LISTENER_ROLE%, and answer NO if the line does not contradict the provided line spoken by %LISTENER_ROLE%. %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n %LISTENER_ROLE% spoke the following line: \\n%LISTENER_LINE%\\n\\n Answer YES if the line spoken by %SPEAKER_ROLE% contradicts the provided line spoken by %LISTENER_ROLE%, and answer NO if the line does not contradict the provided line spoken by %LISTENER_ROLE%, followed by 1 sentence of reasoning.\\n\\n\",\n",
    "\n",
    "    \"backstory_test\": \"Based on the following background, generate a new fact-based multiple choice question with 5 choices addressed directly IN SECOND PERSON, along with its correct answer. Preface the question with 'Question:' and the answer with 'Answer:'.\\n%SPEAKER_BACKSTORY%\\n%PREVIOUS_QUESTIONS%\",\n",
    "    \"answer_backstory\": \"You are %SPEAKER_ROLE%, and you are having a conversation with %LISTENER_ROLE%. Your background is:\\n%SPEAKER_BACKSTORY%\\n So far, the conversation is as below:\\n%CONVERSATION%\\n\\n Based on your conversation above so far, answer the following multiple choice question.\\n%BACKSTORY_QUESTION%\\n\",\n",
    "    \"grade_backstory\": \"As part of grading a test, determine whether the given answer %GIVEN_ANSWER% matches the following correct answer. Respond with either YES or NO.\\nCorrect Answer: %CORRECT_ANSWER%\\n\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written!!\n",
      "1\n",
      "Expected Role Therapist\n",
      "Expected Role Patient\n",
      "Expected Role Therapist\n",
      "Expected Role Patient\n",
      "Expected Role Therapist\n",
      "Expected Role Patient\n",
      "Expected Role Therapist\n",
      "Expected Role Patient\n",
      "Expected Role Therapist\n",
      "Expected Role Patient\n",
      "eval_prompt_consistency\n",
      "eval_prompt_consistency\n",
      "INFO 05-05 23:38:13 [config.py:717] This model supports multiple tasks: {'reward', 'score', 'classify', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 05-05 23:38:13 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 05-05 23:38:18 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-05 23:38:21 [core.py:58] Initializing a V1 LLM engine (v0.8.5) with config: model='meta-llama/Meta-Llama-3.1-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=12880, download_dir='/home/marwa/models/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Meta-Llama-3.1-70B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-05 23:38:21 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7a20b862ade0>\n",
      "INFO 05-05 23:38:22 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-05 23:38:22 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 05-05 23:38:22 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 05-05 23:38:22 [gpu_model_runner.py:1329] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "ERROR 05-05 23:38:23 [core.py:396] EngineCore failed to start.\n",
      "ERROR 05-05 23:38:23 [core.py:396] Traceback (most recent call last):\n",
      "ERROR 05-05 23:38:23 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\n",
      "ERROR 05-05 23:38:23 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 05-05 23:38:23 [core.py:396]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-05 23:38:23 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\n",
      "ERROR 05-05 23:38:23 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "ERROR 05-05 23:38:23 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 64, in __init__\n",
      "ERROR 05-05 23:38:23 [core.py:396]     self.model_executor = executor_class(vllm_config)\n",
      "ERROR 05-05 23:38:23 [core.py:396]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-05 23:38:23 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 52, in __init__\n",
      "ERROR 05-05 23:38:23 [core.py:396]     self._init_executor()\n",
      "ERROR 05-05 23:38:23 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "ERROR 05-05 23:38:23 [core.py:396]     self.collective_rpc(\"load_model\")\n",
      "ERROR 05-05 23:38:23 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "ERROR 05-05 23:38:23 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "ERROR 05-05 23:38:23 [core.py:396]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-05 23:38:23 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/utils.py\", line 2456, in run_method\n",
      "ERROR 05-05 23:38:23 [core.py:396]     return func(*args, **kwargs)\n",
      "ERROR 05-05 23:38:23 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-05 23:38:23 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 162, in load_model\n",
      "ERROR 05-05 23:38:23 [core.py:396]     self.model_runner.load_model()\n",
      "ERROR 05-05 23:38:23 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1332, in load_model\n",
      "ERROR 05-05 23:38:23 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)\n",
      "ERROR 05-05 23:38:23 [core.py:396]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-05 23:38:23 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n",
      "ERROR 05-05 23:38:23 [core.py:396]     return loader.load_model(vllm_config=vllm_config)\n",
      "ERROR 05-05 23:38:23 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-05 23:38:23 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py\", line 452, in load_model\n",
      "ERROR 05-05 23:38:23 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)\n",
      "ERROR 05-05 23:38:23 [core.py:396]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-05 23:38:23 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py\", line 133, in _initialize_model\n",
      "ERROR 05-05 23:38:23 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)\n",
      "ERROR 05-05 23:38:23 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-05 23:38:23 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 496, in __init__\n",
      "ERROR 05-05 23:38:23 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,\n",
      "ERROR 05-05 23:38:23 [core.py:396]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-05 23:38:23 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 542, in _init_model\n",
      "ERROR 05-05 23:38:23 [core.py:396]     return LlamaModel(vllm_config=vllm_config,\n",
      "ERROR 05-05 23:38:23 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-05 23:38:23 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/compilation/decorators.py\", line 151, in __init__\n",
      "ERROR 05-05 23:38:23 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "ERROR 05-05 23:38:23 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 321, in __init__\n",
      "ERROR 05-05 23:38:23 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(\n",
      "ERROR 05-05 23:38:23 [core.py:396]                                                     ^^^^^^^^^^^^\n",
      "ERROR 05-05 23:38:23 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/utils.py\", line 610, in make_layers\n",
      "ERROR 05-05 23:38:23 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n",
      "ERROR 05-05 23:38:23 [core.py:396]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-05 23:38:23 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 323, in <lambda>\n",
      "ERROR 05-05 23:38:23 [core.py:396]     lambda prefix: layer_type(config=config,\n",
      "ERROR 05-05 23:38:23 [core.py:396]                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-05 23:38:23 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 254, in __init__\n",
      "ERROR 05-05 23:38:23 [core.py:396]     self.mlp = LlamaMLP(\n",
      "ERROR 05-05 23:38:23 [core.py:396]                ^^^^^^^^^\n",
      "ERROR 05-05 23:38:23 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 70, in __init__\n",
      "ERROR 05-05 23:38:23 [core.py:396]     self.gate_up_proj = MergedColumnParallelLinear(\n",
      "ERROR 05-05 23:38:23 [core.py:396]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-05 23:38:23 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py\", line 544, in __init__\n",
      "ERROR 05-05 23:38:23 [core.py:396]     super().__init__(input_size=input_size,\n",
      "ERROR 05-05 23:38:23 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py\", line 409, in __init__\n",
      "ERROR 05-05 23:38:23 [core.py:396]     self.quant_method.create_weights(\n",
      "ERROR 05-05 23:38:23 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py\", line 189, in create_weights\n",
      "ERROR 05-05 23:38:23 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),\n",
      "ERROR 05-05 23:38:23 [core.py:396]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-05 23:38:23 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/torch/utils/_device.py\", line 104, in __torch_function__\n",
      "ERROR 05-05 23:38:23 [core.py:396]     return func(*args, **kwargs)\n",
      "ERROR 05-05 23:38:23 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-05 23:38:23 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 896.00 MiB. GPU 0 has a total capacity of 79.26 GiB of which 609.94 MiB is free. Process 1015550 has 4.83 GiB memory in use. Process 1015539 has 4.99 GiB memory in use. Process 1531446 has 422.00 MiB memory in use. Including non-PyTorch memory, this process has 68.35 GiB memory in use. Of the allocated memory 67.87 GiB is allocated by PyTorch, and 5.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process EngineCore_0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 400, in run_engine_core\n",
      "    raise e\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\n",
      "    engine_core = EngineCoreProc(*args, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\n",
      "    super().__init__(vllm_config, executor_class, log_stats,\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 64, in __init__\n",
      "    self.model_executor = executor_class(vllm_config)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 52, in __init__\n",
      "    self._init_executor()\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "    self.collective_rpc(\"load_model\")\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "    answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/utils.py\", line 2456, in run_method\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 162, in load_model\n",
      "    self.model_runner.load_model()\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1332, in load_model\n",
      "    self.model = get_model(vllm_config=self.vllm_config)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n",
      "    return loader.load_model(vllm_config=vllm_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py\", line 452, in load_model\n",
      "    model = _initialize_model(vllm_config=vllm_config)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py\", line 133, in _initialize_model\n",
      "    return model_class(vllm_config=vllm_config, prefix=prefix)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 496, in __init__\n",
      "    self.model = self._init_model(vllm_config=vllm_config,\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 542, in _init_model\n",
      "    return LlamaModel(vllm_config=vllm_config,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/compilation/decorators.py\", line 151, in __init__\n",
      "    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 321, in __init__\n",
      "    self.start_layer, self.end_layer, self.layers = make_layers(\n",
      "                                                    ^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/utils.py\", line 610, in make_layers\n",
      "    maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 323, in <lambda>\n",
      "    lambda prefix: layer_type(config=config,\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 254, in __init__\n",
      "    self.mlp = LlamaMLP(\n",
      "               ^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 70, in __init__\n",
      "    self.gate_up_proj = MergedColumnParallelLinear(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py\", line 544, in __init__\n",
      "    super().__init__(input_size=input_size,\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py\", line 409, in __init__\n",
      "    self.quant_method.create_weights(\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py\", line 189, in create_weights\n",
      "    weight = Parameter(torch.empty(sum(output_partition_sizes),\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/torch/utils/_device.py\", line 104, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 896.00 MiB. GPU 0 has a total capacity of 79.26 GiB of which 609.94 MiB is free. Process 1015550 has 4.83 GiB memory in use. Process 1015539 has 4.99 GiB memory in use. Process 1531446 has 422.00 MiB memory in use. Including non-PyTorch memory, this process has 68.35 GiB memory in use. Of the allocated memory 67.87 GiB is allocated by PyTorch, and 5.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[rank0]:[W505 23:38:24.853208571 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     18\u001b[39m reset_stats()\n\u001b[32m     19\u001b[39m conversation = generate_conversation(\n\u001b[32m     20\u001b[39m     config_llm,\n\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     pturn=\u001b[32m1\u001b[39m\n\u001b[32m     26\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m conversation_eval = \u001b[43mconsistency_eval\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval_prompt_consistency\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboth_agents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m conversation_eval = consistency_eval.eval_index_consistency(conversation_eval, both_agents=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(conversation_eval)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/consistency_LLMs/consistency_eval.py:56\u001b[39m, in \u001b[36meval_prompt_consistency\u001b[39m\u001b[34m(conv_dict, both_agents)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.get(\u001b[33m'\u001b[39m\u001b[33mverbose\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     55\u001b[39m     \u001b[38;5;28mprint\u001b[39m(prompt)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m output = \u001b[43mcompletion_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43meval_model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m conv_dict[\u001b[33m'\u001b[39m\u001b[33meval_prompt_consistency\u001b[39m\u001b[33m'\u001b[39m].append((line_number, output))\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mYES\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m output:  \u001b[38;5;66;03m# no contradiction\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/consistency_LLMs/utils.py:223\u001b[39m, in \u001b[36mcompletion_create\u001b[39m\u001b[34m(model_name, config, prompt, keep_trying)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompletion_create\u001b[39m(model_name, config, prompt, keep_trying=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompletion_create_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (openai.APIError, openai.OpenAIError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    225\u001b[39m         \u001b[38;5;66;03m# print(\"ERROR\", e)\u001b[39;00m\n\u001b[32m    226\u001b[39m         \u001b[38;5;66;03m# print(\"sleeping for 10 seconds.\")\u001b[39;00m\n\u001b[32m    227\u001b[39m         time.sleep(\u001b[32m10\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/consistency_LLMs/utils.py:159\u001b[39m, in \u001b[36mcompletion_create_helper\u001b[39m\u001b[34m(model_name, config, prompt)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompletion_create_helper\u001b[39m(model_name, config, prompt):\n\u001b[32m    152\u001b[39m     \u001b[38;5;66;03m# # limit prompt in all cases\u001b[39;00m\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# if model_name not in vllm_alias:\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m#     # for some reason vLLM models simply repeat this last statement if present\u001b[39;00m\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m#     prompt += \" Limit your answer to three sentences or less!\"\u001b[39;00m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m vllm_alias \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m llm:\n\u001b[32m    158\u001b[39m         \u001b[38;5;66;03m# set up vllm if not already set up\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m         \u001b[43msetup_vllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m     ret = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;66;03m# return the output ret at the end and use to calculate cost\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_name == \u001b[33m\"\u001b[39m\u001b[33mgpt-3.5-turbo-instruct\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/consistency_LLMs/utils.py:132\u001b[39m, in \u001b[36msetup_vllm\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m    130\u001b[39m     llm = LLM(model=vllm_alias[model], tensor_parallel_size=config[\u001b[33m'\u001b[39m\u001b[33mgpus\u001b[39m\u001b[33m'\u001b[39m], download_dir=config[\u001b[33m'\u001b[39m\u001b[33mmodel_dir\u001b[39m\u001b[33m'\u001b[39m], gpu_memory_utilization=\u001b[32m0.75\u001b[39m)\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m vllm_alias[model] == \u001b[33m'\u001b[39m\u001b[33mmeta-llama/Meta-Llama-3.1-70B\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m vllm_alias[model] == \u001b[33m'\u001b[39m\u001b[33mmeta-llama/Meta-Llama-3.1-70B-Instruct\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_alias\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgpus\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_dir\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m12880\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    134\u001b[39m     llm = LLM(model=vllm_alias[model], tensor_parallel_size=config[\u001b[33m'\u001b[39m\u001b[33mgpus\u001b[39m\u001b[33m'\u001b[39m], download_dir=config[\u001b[33m'\u001b[39m\u001b[33mmodel_dir\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/utils.py:1161\u001b[39m, in \u001b[36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1154\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1156\u001b[39m         warnings.warn(\n\u001b[32m   1157\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1158\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1159\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/entrypoints/llm.py:247\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[39m\n\u001b[32m    217\u001b[39m engine_args = EngineArgs(\n\u001b[32m    218\u001b[39m     model=model,\n\u001b[32m    219\u001b[39m     task=task,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m     **kwargs,\n\u001b[32m    244\u001b[39m )\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    251\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/engine/llm_engine.py:510\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    507\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv1\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[32m    508\u001b[39m     engine_cls = V1LLMEngine\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:112\u001b[39m, in \u001b[36mLLMEngine.from_vllm_config\u001b[39m\u001b[34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_vllm_config\u001b[39m(\n\u001b[32m    106\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    110\u001b[39m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    111\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLLMEngine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m               \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mExecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m               \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m               \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m               \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m               \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVLLM_ENABLE_V1_MULTIPROCESSING\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:92\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mself\u001b[39m.output_processor = OutputProcessor(\u001b[38;5;28mself\u001b[39m.tokenizer,\n\u001b[32m     89\u001b[39m                                         log_stats=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# FIXME: implement\u001b[39;49;00m\n\u001b[32m     98\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multiprocess_mode:\n\u001b[32m    101\u001b[39m     \u001b[38;5;66;03m# for v0 compatibility\u001b[39;00m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_executor = \u001b[38;5;28mself\u001b[39m.engine_core.engine_core.model_executor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:73\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m AsyncMPClient(vllm_config, executor_class, log_stats)\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:494\u001b[39m, in \u001b[36mSyncMPClient.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[32m    493\u001b[39m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m     \u001b[38;5;28mself\u001b[39m.outputs_queue = queue.Queue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n\u001b[32m    503\u001b[39m     \u001b[38;5;66;03m# Ensure that the outputs socket processing thread does not have\u001b[39;00m\n\u001b[32m    504\u001b[39m     \u001b[38;5;66;03m# a ref to the client which prevents gc.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:398\u001b[39m, in \u001b[36mMPClient.__init__\u001b[39m\u001b[34m(self, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    394\u001b[39m \u001b[38;5;28mself\u001b[39m._init_core_engines(vllm_config, new_core_engine,\n\u001b[32m    395\u001b[39m                         \u001b[38;5;28mself\u001b[39m.resources.core_engines)\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# Wait for engine core process(es) to start.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[38;5;28mself\u001b[39m.utility_results: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, AnyFuture] = {}\n\u001b[32m    402\u001b[39m \u001b[38;5;66;03m# Request objects which may contain pytorch-allocated tensors\u001b[39;00m\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# that we need to keep references to until zmq is done with the\u001b[39;00m\n\u001b[32m    404\u001b[39m \u001b[38;5;66;03m# underlying data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:430\u001b[39m, in \u001b[36mMPClient._wait_for_engine_startup\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    427\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(events) > \u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m events[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m] != sync_input_socket:\n\u001b[32m    429\u001b[39m     \u001b[38;5;66;03m# One of the core processes exited.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEngine core initialization failed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    431\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mSee root cause above.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    433\u001b[39m eng_id_bytes, msg = sync_input_socket.recv_multipart()\n\u001b[32m    434\u001b[39m eng_id = \u001b[38;5;28mint\u001b[39m.from_bytes(eng_id_bytes, byteorder=\u001b[33m\"\u001b[39m\u001b[33mlittle\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Engine core initialization failed. See root cause above."
     ]
    }
   ],
   "source": [
    "import consistency_eval\n",
    "consistency_eval.prompts = config_role\n",
    "consistency_eval.config = config_llm\n",
    "consistency_eval.eval_prompts = eval_prompts\n",
    "index_offset = load_stats_file(write_file)\n",
    "conversations = []    \n",
    "# lengths = [10, 20, 40, 60]\n",
    "lengths = [10]\n",
    "count = 0 \n",
    "for i in range(1):\n",
    "    for patient_dict in personas_therapy:\n",
    "        count+=1\n",
    "        print(count)\n",
    "        background = patient_dict[\"description\"]\n",
    "        strategy = patient_dict[\"strategy\"]\n",
    "        for convo_length in lengths:\n",
    "            config_llm['convo_length_limit'] = convo_length\n",
    "            reset_stats()\n",
    "            conversation = generate_conversation(\n",
    "                config_llm,\n",
    "                \"\", \n",
    "                background + \" \" + strategy,\n",
    "                \"Therapist\", \n",
    "                \"Patient\",\n",
    "                pturn=1\n",
    "            )\n",
    "            conversation_eval = consistency_eval.eval_prompt_consistency(conversation, both_agents=False)\n",
    "            conversation_eval = consistency_eval.eval_index_consistency(conversation_eval, both_agents=False)\n",
    "\n",
    "            print(conversation_eval)\n",
    "            conversations.append(conversation_eval)\n",
    "            stats['index'] = index_offset\n",
    "            stats['timestamp'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            write_stats(write_file, conversation_eval)\n",
    "            index_offset += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
