{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=4\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=4\n",
    "\n",
    "import os\n",
    "import logging\n",
    "\n",
    "os.environ.pop(\"HF_HUB_OFFLINE\", None)\n",
    "logging.getLogger().setLevel(logging.ERROR)  # or logging.CRITICAL\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "from absl import app, flags\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import *\n",
    "import utils\n",
    "try:\n",
    "    from vllm import LLM, SamplingParams\n",
    "    import ray\n",
    "except ImportError:\n",
    "    pass\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import torch\n",
    "def get_freest_cuda_device():\n",
    "    result = subprocess.run(\n",
    "        ['nvidia-smi', '--query-gpu=memory.free', '--format=csv,nounits,noheader'],\n",
    "        stdout=subprocess.PIPE, encoding='utf-8')\n",
    "    memory_free = [int(x) for x in result.stdout.strip().split('\\n')]\n",
    "    return memory_free.index(max(memory_free))\n",
    "\n",
    "best_gpu = get_freest_cuda_device()\n",
    "device = torch.device(f\"cuda:{best_gpu}\")\n",
    "print(f\"Using GPU: {device}\")\n",
    "# %env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.abspath('../openai_key'), 'r') as f:\n",
    "    utils.client = OpenAI(api_key=f.read().rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Explicitly unset all offline-related env vars\n",
    "os.environ.pop(\"HF_HUB_OFFLINE\", None)\n",
    "os.environ.pop(\"TRANSFORMERS_OFFLINE\", None)\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"0\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"0\"\n",
    "\n",
    "with open(\"../token.txt\", \"r\") as f:\n",
    "    token = f.read().strip()\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    \"\"\"\n",
    "    Counts the number of words in the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of words in the text.\n",
    "    \"\"\"\n",
    "    if text!=None:\n",
    "        words = text.split()\n",
    "        return len(words)\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "personas = [\n",
    "  {\n",
    "    \"grade_level\": \"elementary school\",\n",
    "    \"description\": \"As an elementary-aged Socratic learner, I thrive on simple, concrete why-and-how questions. In dialog, I ask things like “Why does 2 + 2 equal 4?” and I need my teacher to guide me step by step through each answer before I feel confident moving on.\"\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"middle school\",\n",
    "    \"description\": \"As a middle-school Socratic learner, I enjoy unpacking concepts by probing deeper—asking “How does gravity pull objects?” and then “Why doesn’t it pull everything at once?” I need pauses to formulate follow-ups so I can connect the dots.\"\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"high school\",\n",
    "    \"description\": \"As a high-school Socratic learner, I engage best when challenged with open-ended questions like “Why did the American Revolution happen?” and “How did that shift power?” I appreciate when my teacher turns questions back to me, prompting me to defend my reasoning.\"\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"college\",\n",
    "    \"description\": \"As an undergraduate Socratic learner, I seek seminar-style questioning. I might ask “What assumptions underlie this theory?” and expect my instructor to push me further until I can articulate each premise and its implications.\"\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"graduate school\",\n",
    "    \"description\": \"As a graduate-level Socratic learner, I thrive on rigorous inquiry—probing “Why does this methodology hold?” and “How does it compare to alternatives?” I need iterative back-and-forth to refine my own hypotheses.\"\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"elementary school\",\n",
    "    \"description\": \"As an elementary-aged Narrative learner, I remember best when lessons become short, vivid stories. In dialog, I picture characters and daily-life scenarios—like a raindrop’s journey—to make abstract ideas feel real.\"\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"middle school\",\n",
    "    \"description\": \"As a middle-school Narrative learner, I connect with mini-case studies and historical anecdotes. When my teacher weaves facts into a classroom story, I stay engaged and recall details more easily.\"\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"high school\",\n",
    "    \"description\": \"As a high-school Narrative learner, I love when complex theories are framed as real-world dilemmas or biographies of thinkers. Turning a chemistry reaction into a detective tale keeps me focused.\"\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"college\",\n",
    "    \"description\": \"As an undergraduate Narrative learner, I grasp research concepts through academic stories—like the progression of a landmark study—so I can trace how each finding led to the next.\"\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"graduate school\",\n",
    "    \"description\": \"As a graduate-level Narrative learner, I synthesize journal articles into a cohesive research narrative. In dialog, I discuss each paper’s ‘plot,’ understanding how methodologies and results unfold over time.\"\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"elementary school\",\n",
    "    \"description\": \"As an elementary-aged Analogical learner, I need bright, familiar comparisons—like imagining fractions as slices of a pizza. In dialog, I ask “What is this like?” until I can see the connection clearly.\"\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"middle school\",\n",
    "    \"description\": \"As a middle-school Analogical learner, I map new ideas to everyday experiences—comparing electrical circuits to garden hoses. I ask for metaphors repeatedly so I can predict how changes will behave.\"\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"high school\",\n",
    "    \"description\": \"As a high-school Analogical learner, I craft sophisticated metaphors—likening macroeconomics to traffic flow. In dialog, I refine analogies until they capture the full complexity of the concept.\"\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"college\",\n",
    "    \"description\": \"As an undergraduate Analogical learner, I link theoretical constructs to practical systems—comparing neural networks to brain circuits. I collaboratively build and test these analogies in conversation.\"\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"graduate school\",\n",
    "    \"description\": \"As a graduate-level Analogical learner, I propose domain-expert metaphors—like equating algorithm convergence to chemical equilibrium—to critique and sharpen my understanding.\"\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"elementary school\",\n",
    "    \"description\": \"As an elementary-aged Reflective learner, I pause frequently to rephrase what I’ve heard in my own words—like restating a science fact aloud—so I know I’ve understood it correctly.\"\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"middle school\",\n",
    "    \"description\": \"As a middle-school Reflective learner, I use think-pair-share pauses to summarize each key point. I speak my summary aloud in dialog before I proceed, ensuring I haven’t missed any steps.\"\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"high school\",\n",
    "    \"description\": \"As a high-school Reflective learner, I write or say brief explanations at each lesson checkpoint, then check with my teacher. This structured reflection helps me catch misunderstandings early.\"\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"college\",\n",
    "    \"description\": \"As an undergraduate Reflective learner, I integrate self-explanation into seminar discussions—verbalizing complex theories in my own terms before engaging in critique.\"\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"graduate school\",\n",
    "    \"description\": \"As a graduate-level Reflective learner, I articulate research findings aloud, synthesizing multiple sources in dialog to verify the coherence of my conceptual framework.\"\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"elementary school\",\n",
    "    \"description\": \"As an elementary-aged Collaborative learner, I enjoy partner activities where we build solutions together—brainstorming ideas aloud and refining them with teacher feedback in real time.\"\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"middle school\",\n",
    "    \"description\": \"As a middle-school Collaborative learner, I work best in peer-teacher triads—sharing my thought process, inviting critique, and iterating on my approach through back-and-forth dialog.\"\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"high school\",\n",
    "    \"description\": \"As a high-school Collaborative learner, I thrive in Socratic circles—debating interpretations, co-authoring solutions, and using peer feedback to enhance my reasoning.\"\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"college\",\n",
    "    \"description\": \"As an undergraduate Collaborative learner, I engage in team-based seminars—jigsawing research articles and co-constructing models through interactive dialogue.\"\n",
    "  },\n",
    "  {\n",
    "    \"grade_level\": \"graduate school\",\n",
    "    \"description\": \"As a graduate-level Collaborative learner, I lead and co-facilitate research discussions—negotiating theoretical frameworks with peers and refining them through joint inquiry.\"\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "config = {\n",
    "    \"agent1_role\": \"Teacher\",\n",
    "    \"agent2_role\": \"Student\",\n",
    "    \"agent1_prompt\": \"You are a patient, knowledgeable teacher whose goal is to guide a student through learning about %SUBJECT% by first asking what they already know or find confusing, then introducing one clear concept at a time using concrete examples or analogies, checking for understanding by having the student restate ideas in their own words, and adapting your explanations based on their responses while encouraging questions and celebrating progress. The student is in %ROLE% so make sure to teach them at their level.\",\n",
    "    \"agent2_prompt\": \"You are a student in %ROLE% in conversation with a teacher who will teach you %SUBJECT%. You like to learn in the following way:%SPEAKER_BACKSTORY%\",\n",
    "    'reminder_prompt': \"Keep your response very brief — 2 sentences or less. Do NOT repeat anything you've already said: \",\n",
    "    'eval_prompts': {\n",
    "    'prompt_consistency': \"\"\"You are evaluating whether the following utterance by %SPEAKER_ROLE% is consistent with their given background. Answer YES if the line directly contradicts any part of the background—this includes contradictions in facts, behavior, beliefs, emotional stance, or conversational strategy. Answer NO if the line introduces new details, but those details are **plausibly aligned** with the background. Be strict and literal in your evaluation: if the line violates the persona’s stated traits (e.g., avoids vulnerability, keeps answers short, avoids discussing personal topics), mark it as a contradiction even if the violation is subtle. Background for %SPEAKER_ROLE%: %SPEAKER_BACKSTORY% Line spoken by %SPEAKER_ROLE%: %SPEAKER_LINE%  Answer YES if the line contradicts the background, and NO if it does not. Then, explain your answer in one sentence. Be precise and avoid vague justification.\"\"\"}}\n",
    "\n",
    "os.makedirs(\"education\", exist_ok=True)\n",
    "with open(\"education/config_education.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = [\"Llama-3.1-8B-Instruct\", \"gpt-4o-mini\", \"Qwen2.5-3B-Instruct\", \"Llama-3.1-8B\", \"Mistral-7B-Instruct\", \"Llama-3.1-70B\", \"Llama-3.1-70B-Instruct\", \"phi-3.5-mini-instruct\"]\n",
    "        \n",
    "config_llm = {'agent1_model': 'Llama-3.1-8B-Instruct',\n",
    "             'agent2_model': 'Llama-3.1-8B-Instruct',\n",
    "             'eval_model': 'Llama-3.1-70B-Instruct',\n",
    "             'iterations': 10,\n",
    "             'verbose': False,\n",
    "             'write': True,\n",
    "             'convo_length_limit': 10,\n",
    "             'max_tokens': 256,\n",
    "             'gpus': 1,\n",
    "             'seed': 0,\n",
    "             'task_name': 'Education',\n",
    "             'model_dir': \"/home/marwa/models/\"}\n",
    "\n",
    "with open(\"education/Llama-3.1-8B-Instruct.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config_llm, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"education/config_education_personas.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_role_prefix(response, expected_role):\n",
    "    \"\"\"\n",
    "    Removes repeated instances of the expected_role prefix at the start (e.g., 'Therapist: Therapist:'),\n",
    "    and ensures the response begins with a single correct expected_role prefix.\n",
    "    \"\"\"\n",
    "    pattern = rf\"^(({re.escape(expected_role)}):\\s*)+\"\n",
    "    cleaned = re.sub(pattern, '', response.strip(), flags=re.IGNORECASE)\n",
    "    return cleaned\n",
    "    \n",
    "def is_role_confused(response, other_role):\n",
    "    \"\"\"\n",
    "    Checks if the output starts with the wrong speaker tag.\n",
    "    \"\"\"\n",
    "    if other_role + \":\" in response:\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "\n",
    "def generate_response(agent_model, expected_role, other_role, config_llm, prompt, max_retries=3):\n",
    "    for _ in range(max_retries):\n",
    "        response = completion_create(agent_model, config_llm, prompt)\n",
    "        print(expected_role)\n",
    "        if not is_role_confused(response, other_role):\n",
    "            return clean_role_prefix(response, expected_role)\n",
    "            \n",
    "    return clean_role_prefix(response, expected_role)\n",
    "\n",
    "def generate_conversation(config_llm, p1, p2, p1_name, p2_name, subject, role, pturn=1):\n",
    "    stats['P1'] = p1\n",
    "    stats['P2'] = p2\n",
    "    stats['pturn'] = pturn\n",
    "    round_num = 0\n",
    "    while round_num < config_llm['convo_length_limit']:\n",
    "        conversation = (\"\".join([turn[1] if isinstance(turn, tuple) else turn for turn in stats[\"conversation\"]]) if len(stats[\"conversation\"]) != 0 else \"You are starting the conversation.\\n\")\n",
    "        \n",
    "        if pturn == 1:\n",
    "            prompt = config[\"agent1_prompt\"]\n",
    "            pturn = 2\n",
    "            if config_llm[\"verbose\"]:\n",
    "                print(prompt)\n",
    "                print()\n",
    "\n",
    "            if round_num!=0: \n",
    "                prompt+= \"Your conversation with the therapist so far is below:\\nConversation: %CONVERSATION%\"\n",
    "                \n",
    "            if round_num >=config_llm['convo_length_limit']*2-11 and round_num<=config_llm['convo_length_limit']*2-1:\n",
    "                prompt+= \"You have \" + str((config_llm['convo_length_limit']-round_num)//2) + \" rounds left.\" + \"Make sure to conclude the conversation as your near the end.\"\n",
    "\n",
    "            elif round_num>config_llm['convo_length_limit']*2-1:\n",
    "                prompt+= \"This is your concluding line in the conversation.\"\n",
    "\n",
    "            if round_num!=0: \n",
    "                prompt+= \"Continue the conversation with the student. Remember you are the teacher.\"\n",
    "                \n",
    "            prompt += config[\"reminder_prompt\"]\n",
    "            prompt = prompt.replace(\"%SPEAKER_ROLE%\", config[\"agent1_role\"]) \\\n",
    "                           .replace(\"%LISTENER_ROLE%\", config[\"agent2_role\"]) \\\n",
    "                            .replace(\"%ROLE%\", role) \\\n",
    "                           .replace(\"%SUBJECT%\", subject) \\\n",
    "                           .replace(\"%CONVERSATION%\", conversation)\n",
    "            \n",
    "            prompt+=\"%SPEAKER_ROLE%:\"\n",
    "            response = generate_response(config_llm['agent1_model'], config[\"agent1_role\"], config[\"agent2_role\"], config_llm, prompt)\n",
    "            stats[\"conversation\"].append((round_num, f\"{config[\"agent1_role\"]}: \" + response + \"\\n\"))\n",
    "        \n",
    "        else:\n",
    "            prompt = config[\"agent2_prompt\"]\n",
    "            pturn = 1    \n",
    "            if config_llm[\"verbose\"]:\n",
    "                print(prompt)\n",
    "                print()\n",
    "\n",
    "            if round_num!=0: \n",
    "                prompt+= \"Your conversation with the patient so far is below:\\nConversation: %CONVERSATION%\"\n",
    "            if round_num >=config_llm['convo_length_limit']*2-11 and round_num<=config_llm['convo_length_limit']*2-1:\n",
    "                prompt+= \"You have \" + str((config_llm['convo_length_limit']-round_num)//2) + \" rounds left.\" + \"Make sure to conclude the conversation as your near the end.\"\n",
    "            elif round_num>config_llm['convo_length_limit']*2-1:\n",
    "                prompt+= \"This is your concluding line in the conversation.\"\n",
    "\n",
    "            if round_num!=0: \n",
    "                prompt+= \"Continue the conversation with the teacher. Remember you are the student.\"\n",
    "\n",
    "            prompt += config[\"reminder_prompt\"]\n",
    "            prompt = prompt.replace(\"%SPEAKER_ROLE%\", config[\"agent1_role\"]) \\\n",
    "                           .replace(\"%LISTENER_ROLE%\", config[\"agent2_role\"]) \\\n",
    "                           .replace(\"%SPEAKER_BACKSTORY%\", subject) \\\n",
    "                            .replace(\"%ROLE%\", role) \\\n",
    "                           .replace(\"%SUBJECT%\", p1) \\\n",
    "                           .replace(\"%CONVERSATION%\", conversation)\n",
    "\n",
    "            prompt+=\"%SPEAKER_ROLE%:\"\n",
    "            response = generate_response(config_llm['agent2_model'], config[\"agent2_role\"], config[\"agent1_role\"], config_llm, prompt)\n",
    "            stats[\"conversation\"].append((round_num, f\"{config[\"agent2_role\"]}: \" + response + \"\\n\"))\n",
    "        round_num += 1\n",
    "\n",
    "    stats[\"rounds\"] = round_num\n",
    "    if config_llm['verbose']:\n",
    "        print(stats[\"conversation\"])\n",
    "    return stats.copy()\n",
    "\n",
    "def reset_stats():\n",
    "    stats_template = {\n",
    "        \"task_name\": config_llm['task_name'],\n",
    "        \"P1\": \"\",\n",
    "        \"P2\": \"\",\n",
    "        \"conversation\": [],\n",
    "        \"pturn\": 0, # beginning person (1 or 2)\n",
    "        \"index\": -1,\n",
    "        \"timestamp\": \"\",\n",
    "        \"rounds\": 0,\n",
    "        'conversation_only': True\n",
    "    }\n",
    "    for key, value in stats_template.items():\n",
    "        stats[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import utils\n",
    "utils.config = config_llm\n",
    "\n",
    "current_date = str(datetime.now().strftime(\"%m.%d.%y\"))\n",
    "output_dir = f\"education/exp/{current_date}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate unique random number for filename\n",
    "def generate_unique_file_number(output_dir, prefix, seed, extension=\".json\"):\n",
    "    while True:\n",
    "        rand_num = random.randint(0, 1000)\n",
    "        filename = f\"{prefix}_{seed}_{rand_num}{extension}\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            return rand_num\n",
    "\n",
    "unique_num = generate_unique_file_number(\n",
    "    output_dir,\n",
    "    config_llm['agent1_model'],\n",
    "    config_llm['seed']\n",
    ")\n",
    "\n",
    "# File to write output to\n",
    "write_file = os.path.join(output_dir, f\"{config_llm['agent1_model']}_{config_llm['seed']}_{unique_num}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written!!\n",
      "INFO 04-24 14:33:43 [config.py:600] This model supports multiple tasks: {'reward', 'classify', 'embed', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "INFO 04-24 14:33:43 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 04-24 14:33:45 [utils.py:2273] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 04-24 14:33:49 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 04-24 14:33:51 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir='/home/marwa/models/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 04-24 14:33:52 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7492b03559d0>\n",
      "INFO 04-24 14:33:53 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 04-24 14:33:53 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 04-24 14:33:53 [gpu_model_runner.py:1258] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "WARNING 04-24 14:33:53 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 04-24 14:33:53 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  5.48it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:01,  1.81it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.35it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.20it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.36it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-24 14:33:57 [loader.py:447] Loading weights took 3.11 seconds\n",
      "INFO 04-24 14:33:57 [gpu_model_runner.py:1273] Model loading took 14.9889 GiB and 4.289543 seconds\n",
      "INFO 04-24 14:34:05 [backends.py:416] Using cache directory: /home/marwa/.cache/vllm/torch_compile_cache/8b4ebbe309/rank_0_0 for vLLM's torch.compile\n",
      "INFO 04-24 14:34:05 [backends.py:426] Dynamo bytecode transform time: 7.60 s\n",
      "INFO 04-24 14:34:05 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "INFO 04-24 14:34:12 [monitor.py:33] torch.compile takes 7.60 s in total\n",
      "INFO 04-24 14:34:13 [kv_cache_utils.py:578] GPU KV cache size: 417,824 tokens\n",
      "INFO 04-24 14:34:13 [kv_cache_utils.py:581] Maximum concurrency for 131,072 tokens per request: 3.19x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W424 14:34:25.226759739 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m config_llm[\u001b[33m'\u001b[39m\u001b[33mconvo_length_limit\u001b[39m\u001b[33m'\u001b[39m] = convo_length\n\u001b[32m      8\u001b[39m reset_stats()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m conversation = \u001b[43mgenerate_therapy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_llm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatient_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdescription\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatient_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrategy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPatient\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTherapist\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpturn\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     16\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(conversation)\n\u001b[32m     18\u001b[39m conversations.append(conversation)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mgenerate_therapy\u001b[39m\u001b[34m(config_llm, p1, p2, p1_name, p2_name, pturn)\u001b[39m\n\u001b[32m     58\u001b[39m     prompt = prompt.replace(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mSPEAKER_ROLE\u001b[39m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m, config_therapy[\u001b[33m\"\u001b[39m\u001b[33magent1_role\u001b[39m\u001b[33m\"\u001b[39m]) \\\n\u001b[32m     59\u001b[39m                    .replace(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mLISTENER_ROLE\u001b[39m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m, config_therapy[\u001b[33m\"\u001b[39m\u001b[33magent2_role\u001b[39m\u001b[33m\"\u001b[39m]) \\\n\u001b[32m     60\u001b[39m                    .replace(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mSPEAKER_BACKSTORY\u001b[39m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m, p1) \\\n\u001b[32m     61\u001b[39m                    .replace(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mCONVERSATION\u001b[39m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m, conversation)\n\u001b[32m     63\u001b[39m     prompt+=\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mSPEAKER_ROLE\u001b[39m\u001b[33m%\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     response = \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_llm\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43magent1_model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_therapy\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43magent1_role\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_therapy\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43magent2_role\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_llm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     stats[\u001b[33m\"\u001b[39m\u001b[33mconversation\u001b[39m\u001b[33m\"\u001b[39m].append((round_num, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_therapy[\u001b[33m\"\u001b[39m\u001b[33magent1_role\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m + response + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m))\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mgenerate_response\u001b[39m\u001b[34m(agent_model, expected_role, other_role, config_llm, prompt, max_retries)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_response\u001b[39m(agent_model, expected_role, other_role, config_llm, prompt, max_retries=\u001b[32m3\u001b[39m):\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_retries):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m         response = \u001b[43mcompletion_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_llm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m         \u001b[38;5;28mprint\u001b[39m(expected_role)\n\u001b[32m     25\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_role_confused(response, other_role):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/consistency_LLMs/utils.py:222\u001b[39m, in \u001b[36mcompletion_create\u001b[39m\u001b[34m(model_name, config, prompt, keep_trying)\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompletion_create\u001b[39m(model_name, config, prompt, keep_trying=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompletion_create_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (openai.APIError, openai.OpenAIError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    224\u001b[39m         \u001b[38;5;66;03m# print(\"ERROR\", e)\u001b[39;00m\n\u001b[32m    225\u001b[39m         \u001b[38;5;66;03m# print(\"sleeping for 10 seconds.\")\u001b[39;00m\n\u001b[32m    226\u001b[39m         time.sleep(\u001b[32m10\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/consistency_LLMs/utils.py:158\u001b[39m, in \u001b[36mcompletion_create_helper\u001b[39m\u001b[34m(model_name, config, prompt)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompletion_create_helper\u001b[39m(model_name, config, prompt):\n\u001b[32m    151\u001b[39m     \u001b[38;5;66;03m# # limit prompt in all cases\u001b[39;00m\n\u001b[32m    152\u001b[39m     \u001b[38;5;66;03m# if model_name not in vllm_alias:\u001b[39;00m\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m#     # for some reason vLLM models simply repeat this last statement if present\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m#     prompt += \" Limit your answer to three sentences or less!\"\u001b[39;00m\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m vllm_alias \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m llm:\n\u001b[32m    157\u001b[39m         \u001b[38;5;66;03m# set up vllm if not already set up\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m         \u001b[43msetup_vllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m     ret = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;66;03m# return the output ret at the end and use to calculate cost\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_name == \u001b[33m\"\u001b[39m\u001b[33mgpt-3.5-turbo-instruct\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/consistency_LLMs/utils.py:133\u001b[39m, in \u001b[36msetup_vllm\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m    131\u001b[39m         llm = LLM(model=vllm_alias[model], tensor_parallel_size=config[\u001b[33m'\u001b[39m\u001b[33mgpus\u001b[39m\u001b[33m'\u001b[39m], download_dir=config[\u001b[33m'\u001b[39m\u001b[33mmodel_dir\u001b[39m\u001b[33m'\u001b[39m], gpu_memory_utilization=\u001b[32m0.9\u001b[39m, max_model_len=\u001b[32m12880\u001b[39m)\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m         llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_alias\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgpus\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_dir\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/utils.py:1096\u001b[39m, in \u001b[36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1089\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1091\u001b[39m         warnings.warn(\n\u001b[32m   1092\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1093\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1094\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1096\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/entrypoints/llm.py:243\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[39m\n\u001b[32m    214\u001b[39m engine_args = EngineArgs(\n\u001b[32m    215\u001b[39m     model=model,\n\u001b[32m    216\u001b[39m     task=task,\n\u001b[32m   (...)\u001b[39m\u001b[32m    239\u001b[39m     **kwargs,\n\u001b[32m    240\u001b[39m )\n\u001b[32m    242\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    247\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/engine/llm_engine.py:521\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    518\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv1\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[32m    519\u001b[39m     engine_cls = V1LLMEngine\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:115\u001b[39m, in \u001b[36mLLMEngine.from_vllm_config\u001b[39m\u001b[34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stat_loggers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    112\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing StatLoggers to V1 is not yet supported. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    113\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSet VLLM_USE_V1=0 and file and issue on Github.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m           \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mExecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m           \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m           \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m           \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m           \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVLLM_ENABLE_V1_MULTIPROCESSING\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:90\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28mself\u001b[39m.output_processor = OutputProcessor(\u001b[38;5;28mself\u001b[39m.tokenizer,\n\u001b[32m     87\u001b[39m                                         log_stats=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# FIXME: implement\u001b[39;49;00m\n\u001b[32m     96\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multiprocess_mode:\n\u001b[32m     99\u001b[39m     \u001b[38;5;66;03m# for v0 compatibility\u001b[39;00m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_executor = \u001b[38;5;28mself\u001b[39m.engine_core.engine_core.model_executor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:72\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m AsyncMPClient(vllm_config, executor_class, log_stats)\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:439\u001b[39m, in \u001b[36mSyncMPClient.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[32m    438\u001b[39m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    446\u001b[39m     \u001b[38;5;28mself\u001b[39m.outputs_queue: queue.Queue[EngineCoreOutputs] = queue.Queue()\n\u001b[32m    448\u001b[39m     \u001b[38;5;66;03m# Ensure that the outputs socket processing thread does not have\u001b[39;00m\n\u001b[32m    449\u001b[39m     \u001b[38;5;66;03m# a ref to the client which prevents gc.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:401\u001b[39m, in \u001b[36mMPClient.__init__\u001b[39m\u001b[34m(self, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;66;03m# Wait for engine core process(es) to start.\u001b[39;00m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m engine \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.resources.core_engines:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc_handle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_for_startup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;28mself\u001b[39m.utility_results: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, AnyFuture] = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/v1/utils.py:127\u001b[39m, in \u001b[36mBackgroundProcHandle.wait_for_startup\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwait_for_startup\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    126\u001b[39m     \u001b[38;5;66;03m# Wait for startup.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m\"\u001b[39m] != \u001b[33m\"\u001b[39m\u001b[33mREADY\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    128\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.proc.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m initialization failed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    129\u001b[39m                            \u001b[33m\"\u001b[39m\u001b[33mSee root cause above.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/connection.py:250\u001b[39m, in \u001b[36m_ConnectionBase.recv\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    249\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m buf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler.loads(buf.getbuffer())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/connection.py:430\u001b[39m, in \u001b[36mConnection._recv_bytes\u001b[39m\u001b[34m(self, maxsize)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     buf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m     size, = struct.unpack(\u001b[33m\"\u001b[39m\u001b[33m!i\u001b[39m\u001b[33m\"\u001b[39m, buf.getvalue())\n\u001b[32m    432\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m size == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/connection.py:395\u001b[39m, in \u001b[36mConnection._recv\u001b[39m\u001b[34m(self, size, read)\u001b[39m\n\u001b[32m    393\u001b[39m remaining = size\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     chunk = \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    396\u001b[39m     n = \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "index_offset = load_stats_file(write_file)\n",
    "conversations = []    \n",
    "lengths = [10, 20, 40, 60]\n",
    "for i in range(1):\n",
    "    for education_dict in personas:\n",
    "        for convo_length in lengths:\n",
    "            config_llm['convo_length_limit'] = convo_length\n",
    "            reset_stats()\n",
    "            conversation = generate_conversation(\n",
    "                config_llm,\n",
    "                \"\", \n",
    "                education_dict[\"description\"], \n",
    "                \"Teacher\", \n",
    "                \"Student\", \n",
    "                subject, \n",
    "                education_dict[\"grade_level\"], \n",
    "\n",
    "                pturn=1\n",
    "            )\n",
    "            print(conversation)\n",
    "            conversations.append(conversation)\n",
    "            stats['index'] = index_offset\n",
    "            stats['timestamp'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            write_stats(write_file)\n",
    "            index_offset += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"education/exp/04.24.25/Llama-3.1-8B-Instruct_0_250.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    conversations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompts = {\n",
    "    \"strategy_consistency\": \"%SCENARIO_DESC% Evaluate whether the following line spoken by %SPEAKER_ROLE% contradicts their background. First, describe the interpreted intention of the statement, and whether or not it aligns with the given background of %SPEAKER_ROLE%. Then, provide 1 sentence explaining your reasoning based on the strategy, followed by YES if the line contradicts the given background of %SPEAKER_ROLE%, and NO if it is consistent with the provided background of %SPEAKER_ROLE%. %SPEAKER_ROLE%'s strategy is described as follows:\\n%SPEAKER_STRATEGY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide 1 sentence of reasoning followed by YES or NO.\\n\\n\",\n",
    "    \"background_consistency\": \"%SCENARIO_DESC% Evaluate whether the following line spoken by %SPEAKER_ROLE% contradicts their background. Provide 1 sentence explaining your reasoning based on the background, followed by YES if the line contradicts the given background of %SPEAKER_ROLE%, and NO if it is consistent with the provided background of %SPEAKER_ROLE%. %SPEAKER_ROLE%'s background is described as follows:\\n%SPEAKER_BACKSTORY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide 1 sentence of reason reasoning followed by YES or NO.\\n\\n\",\n",
    "    \"combined_prompt_consistency\": \"%SCENARIO_DESC% Evaluate the intention behind the following line spoken by %SPEAKER_ROLE% and determine whether it contradicts their background. First, describe the interpreted intention of the statement, and whether or not it aligns with the given background of %SPEAKER_ROLE%. Then, answer YES if the line contradicts the given background of %SPEAKER_ROLE% or the intention does not align with the provided background, and answer NO if it does align with the provided background or the intention aligns with the background of %SPEAKER_ROLE%. %SPEAKER_ROLE%'s background is described as follows:\\n%SPEAKER_BACKSTORY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide your answer as 1 sentence explaining your reasoning based on the background and the interpreted intention, followed by YES or NO.\\n\\n\",\n",
    "\n",
    "    \"pairwise_consistency\":\"%SCENARIO_DESC% For the following line spoken by %SPEAKER_ROLE%, answer YES if the line directly contradicts the provided line spoken by %LISTENER_ROLE%, and answer NO if the line does not contradict the provided line spoken by %LISTENER_ROLE%. %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n %LISTENER_ROLE% spoke the following line: \\n%LISTENER_LINE%\\n\\n Answer YES if the line spoken by %SPEAKER_ROLE% contradicts the provided line spoken by %LISTENER_ROLE%, and answer NO if the line does not contradict the provided line spoken by %LISTENER_ROLE%, followed by 1 sentence of reasoning.\\n\\n\",\n",
    "\n",
    "    \"backstory_test\": \"Based on the following background, generate a new fact-based multiple choice question with 5 choices addressed directly IN SECOND PERSON, along with its correct answer. Preface the question with 'Question:' and the answer with 'Answer:'.\\n%SPEAKER_BACKSTORY%\\n%PREVIOUS_QUESTIONS%\",\n",
    "    \"answer_backstory\": \"You are %SPEAKER_ROLE%, and you are having a conversation with %LISTENER_ROLE%. Your background is:\\n%SPEAKER_BACKSTORY%\\n So far, the conversation is as below:\\n%CONVERSATION%\\n\\n Based on your conversation above so far, answer the following multiple choice question.\\n%BACKSTORY_QUESTION%\\n\",\n",
    "    \"grade_backstory\": \"As part of grading a test, determine whether the given answer %GIVEN_ANSWER% matches the following correct answer. Respond with either YES or NO.\\nCorrect Answer: %CORRECT_ANSWER%\\n\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prompt_consistency(conv_dict, both_agents=False):\n",
    "    conv_dict['eval_prompt_consistency'] = []\n",
    "    conv_dict['P1_prompt_consistency_score'] = 0\n",
    "    conv_dict['P2_prompt_consistency_score'] = 0\n",
    "    p1_utterances = 0\n",
    "    p2_utterances = 0\n",
    "\n",
    "    pturn = conv_dict[\"pturn\"]\n",
    "    for line in conv_dict[\"conversation\"]:\n",
    "        line_number = line[0]\n",
    "        convo_line = line[1]\n",
    "        if pturn == 1:\n",
    "            prompt = eval_prompts[\"combined_prompt_consistency\"].replace(\"%SCENARIO_DESC\", prompts[\"agent1_prompt\"]) \\\n",
    "                                                                .replace(\"%SPEAKER_ROLE%\", prompts[\"agent1_role\"]) \\\n",
    "                                                                .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P1\"]) \\\n",
    "                                                                .replace(\"%SPEAKER_LINE%\", convo_line)\n",
    "            if config.get('verbose', False):\n",
    "                print(prompt)\n",
    "            output = completion_create(config['eval_model'], config, prompt)\n",
    "            conv_dict['eval_prompt_consistency'].append((line_number, output))\n",
    "            if \"YES\" not in output:  # no contradiction\n",
    "                conv_dict['P1_prompt_consistency_score'] += 1\n",
    "            p1_utterances += 1\n",
    "            pturn = 2\n",
    "        elif pturn == 2:\n",
    "            if both_agents:\n",
    "                prompt = eval_prompts[\"combined_prompt_consistency\"].replace(\"%SCENARIO_DESC\", prompts[\"agent2_prompt\"]) \\\n",
    "                                                                    .replace(\"%SPEAKER_ROLE%\", prompts[\"agent2_role\"]) \\\n",
    "                                                                    .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P2\"]) \\\n",
    "                                                                    .replace(\"%SPEAKER_LINE%\", convo_line)\n",
    "                if config.get('verbose', False):\n",
    "                    print(prompt)\n",
    "                output = completion_create(config['eval_model'], config, prompt)\n",
    "                conv_dict['eval_prompt_consistency'].append((line_number, output))\n",
    "                if \"YES\" not in output:  # no contradiction\n",
    "                    conv_dict['P2_prompt_consistency_score']+= 1\n",
    "                p2_utterances += 1\n",
    "            pturn = 1\n",
    "\n",
    "    if p1_utterances > 0:\n",
    "        conv_dict['P1_prompt_consistency_score'] /= p1_utterances\n",
    "    if p2_utterances > 0:\n",
    "        conv_dict['P2_prompt_consistency_score'] /= p2_utterances\n",
    "\n",
    "    if config.get('verbose', False):\n",
    "        print(conv_dict)\n",
    "    return conv_dict\n",
    "# Replacement for (2) and (4), evaluates whether each pair of lines in the conversation is consistent with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_COMPILE_DISABLE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                   | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-24 15:21:23 [config.py:600] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "INFO 04-24 15:21:23 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 04-24 15:21:25 [utils.py:2273] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 04-24 15:21:29 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 04-24 15:21:31 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='meta-llama/Meta-Llama-3.1-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=12880, download_dir='/home/marwa/models/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Meta-Llama-3.1-70B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 04-24 15:21:32 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7465e63d9820>\n",
      "INFO 04-24 15:21:33 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 04-24 15:21:33 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 04-24 15:21:33 [gpu_model_runner.py:1258] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "WARNING 04-24 15:21:33 [config.py:3785] `torch.compile` is turned on, but the model meta-llama/Meta-Llama-3.1-70B-Instruct does not support it. Please open an issue on GitHub if you want it to be supported.\n",
      "ERROR 04-24 15:21:34 [core.py:390] EngineCore hit an exception: Traceback (most recent call last):\n",
      "ERROR 04-24 15:21:34 [core.py:390]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 378, in run_engine_core\n",
      "ERROR 04-24 15:21:34 [core.py:390]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 04-24 15:21:34 [core.py:390]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-24 15:21:34 [core.py:390]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 319, in __init__\n",
      "ERROR 04-24 15:21:34 [core.py:390]     super().__init__(vllm_config, executor_class, log_stats)\n",
      "ERROR 04-24 15:21:34 [core.py:390]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 67, in __init__\n",
      "ERROR 04-24 15:21:34 [core.py:390]     self.model_executor = executor_class(vllm_config)\n",
      "ERROR 04-24 15:21:34 [core.py:390]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 0CRITICAL 04-24 15:21:34 [core_client.py:361] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\n",
      "4-24 15:21:34 [core.py:390]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 52, in __init__\n",
      "ERROR 04-24 15:21:34 [core.py:390]     self._init_executor()\n",
      "ERROR 04-24 15:21:34 [core.py:390]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "ERROR 04-24 15:21:34 [core.py:390]     self.collective_rpc(\"load_model\")\n",
      "ERROR 04-24 15:21:34 [core.py:390]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "ERROR 04-24 15:21:34 [core.py:390]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "ERROR 04-24 15:21:34 [core.py:390]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-24 15:21:34 [core.py:390]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/utils.py\", line 2347, in run_method\n",
      "ERROR 04-24 15:21:34 [core.py:390]     return func(*args, **kwargs)\n",
      "ERROR 04-24 15:21:34 [core.py:390]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-24 15:21:34 [core.py:390]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 136, in load_model\n",
      "ERROR 04-24 15:21:34 [core.py:390]     self.model_runner.load_model()\n",
      "ERROR 04-24 15:21:34 [core.py:390]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1261, in load_model\n",
      "ERROR 04-24 15:21:34 [core.py:390]     self.model = get_model(vllm_config=self.vllm_config)\n",
      "ERROR 04-24 15:21:34 [core.py:390]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-24 15:21:34 [core.py:390]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n",
      "ERROR 04-24 15:21:34 [core.py:390]     return loader.load_model(vllm_config=vllm_config)\n",
      "ERROR 04-24 15:21:34 [core.py:390]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "config = config_llm\n",
    "prompts = config\n",
    "for conversation in tqdm(conversations):\n",
    "    conversation = eval_prompt_consistency(conversation, both_agents=False)\n",
    "\n",
    "write_stats(write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"education/exp/04.24.25/Llama-3.1-8B-Instruct_0_250_consistency2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(conversations, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
