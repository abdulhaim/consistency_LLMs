{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deeacae3-fc5f-4a28-9aba-a3a51dee6974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{All Metrics (Prompt & Index Consistency) by Task and LLM}\n",
      "\\label{tab:all_metrics}\n",
      "\\begin{tabular}{llrrr}\n",
      "\\toprule\n",
      " &  & line-to-prompt Consistency & line-to-line consistency & q&a consistency \\\\\n",
      "Task & LLM &  &  &  \\\\\n",
      "\\midrule\n",
      "\\multirow[t]{3}{*}{education} & Llama-3.1-8B-Instruct & 0.824 & 0.800 & 0.000 \\\\\n",
      " & gemma-2-2b-it & 0.511 & 0.928 & 0.000 \\\\\n",
      " & mistral-instruct & 0.728 & 0.975 & 0.000 \\\\\n",
      "\\cline{1-5}\n",
      "\\multirow[t]{3}{*}{therapy} & Llama-3.1-8B-Instruct & 0.566 & 0.496 & 0.000 \\\\\n",
      " & gemma-2-2b-it & 0.000 & 0.000 & 0.000 \\\\\n",
      " & mistral-instruct & 0.000 & 0.000 & 0.000 \\\\\n",
      "\\cline{1-5}\n",
      "\\multirow[t]{3}{*}{chatting} & Llama-3.1-8B-Instruct & 0.155 & 0.248 & 0.000 \\\\\n",
      " & gemma-2-2b-it & 0.871 & 0.900 & 0.000 \\\\\n",
      " & mistral-instruct & 0.955 & 0.984 & 0.000 \\\\\n",
      "\\cline{1-5}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "                                 line-to-prompt Consistency  \\\n",
      "Task      LLM                                                 \n",
      "education Llama-3.1-8B-Instruct                    0.824292   \n",
      "          gemma-2-2b-it                            0.511292   \n",
      "          mistral-instruct                         0.728125   \n",
      "therapy   Llama-3.1-8B-Instruct                    0.566310   \n",
      "          gemma-2-2b-it                            0.000000   \n",
      "          mistral-instruct                         0.000000   \n",
      "chatting  Llama-3.1-8B-Instruct                    0.154771   \n",
      "          gemma-2-2b-it                            0.870792   \n",
      "          mistral-instruct                         0.955458   \n",
      "\n",
      "                                 line-to-line consistency  q&a consistency  \n",
      "Task      LLM                                                               \n",
      "education Llama-3.1-8B-Instruct                  0.800343              0.0  \n",
      "          gemma-2-2b-it                          0.928150              0.0  \n",
      "          mistral-instruct                       0.974590              0.0  \n",
      "therapy   Llama-3.1-8B-Instruct                  0.496245              0.0  \n",
      "          gemma-2-2b-it                          0.000000              0.0  \n",
      "          mistral-instruct                       0.000000              0.0  \n",
      "chatting  Llama-3.1-8B-Instruct                  0.248101              0.0  \n",
      "          gemma-2-2b-it                          0.899901              0.0  \n",
      "          mistral-instruct                       0.984479              0.0  \n",
      "\n",
      "LaTeX table written to all_llm_metrics.tex\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuration: adjust these to your setup ---\n",
    "llms = ['Llama-3.1-8B-Instruct', 'gemma-2-2b-it', 'mistral-instruct']\n",
    "tasks = ['education', 'therapy', 'chatting']\n",
    "n_files_per_llm = 4\n",
    "\n",
    "# Original JSON keys and their desired display names\n",
    "metric_keys = ['P2_prompt_consistency_score', 'P2_index_consistency_score', 'P2_q&a_consistency_score']\n",
    "rename_map = {\n",
    "    'P2_prompt_consistency_score': 'line-to-prompt Consistency',\n",
    "    'P2_index_consistency_score':   'line-to-line consistency',\n",
    "    'P2_q&a_consistency_score':     'q&a consistency'\n",
    "}\n",
    "\n",
    "# Base directory containing one subfolder per task\n",
    "tasks_root_dir = 'results'\n",
    "records = []\n",
    "for task in tasks:\n",
    "    task_dir = os.path.join(tasks_root_dir, task)\n",
    "    if not os.path.isdir(task_dir):\n",
    "        print(f\"Warning: task directory not found: {task_dir}; skipping.\")\n",
    "        continue\n",
    "\n",
    "    # scan for JSON files once per task\n",
    "    all_files = [f for f in os.listdir(task_dir) if f.endswith('.json')]\n",
    "    for llm in llms:\n",
    "        llm_files = sorted([f for f in all_files if llm in f])[:n_files_per_llm]\n",
    "        metric_file_means = {k: [] for k in metric_keys}\n",
    "\n",
    "        for fname in llm_files:\n",
    "            path = os.path.join(task_dir, fname)\n",
    "            try:\n",
    "                data = json.load(open(path))\n",
    "            except Exception as e:\n",
    "                print(f\"  • Couldn’t read {fname} ({e}); using zeros.\")\n",
    "                for k in metric_keys:\n",
    "                    metric_file_means[k].append(0.0)\n",
    "                continue\n",
    "\n",
    "            # handle list-of-dicts format\n",
    "            if isinstance(data, list):\n",
    "                for k in metric_keys:\n",
    "                    vals = []\n",
    "                    for i, entry in enumerate(data):\n",
    "                        v = entry.get(k, 0.0)\n",
    "                        if not isinstance(v, (int, float)):\n",
    "                            print(f\"    • Bad '{k}' in element {i} of {fname}; using 0\")\n",
    "                            v = 0.0\n",
    "                        vals.append(v)\n",
    "                    metric_file_means[k].append(np.mean(vals) if vals else 0.0)\n",
    "\n",
    "            # handle single-dict-with-list format\n",
    "            elif isinstance(data, dict):\n",
    "                for k in metric_keys:\n",
    "                    arr = data.get(k, [])\n",
    "                    if isinstance(arr, list) and len(arr) > 0:\n",
    "                        metric_file_means[k].append(np.mean(arr))\n",
    "                    else:\n",
    "                        print(f\"    • Missing/invalid '{k}' in {fname}; using 0\")\n",
    "                        metric_file_means[k].append(0.0)\n",
    "            else:\n",
    "                print(f\"    • Unexpected structure in {fname}; padding zeros.\")\n",
    "                for k in metric_keys:\n",
    "                    metric_file_means[k].append(0.0)\n",
    "\n",
    "        # pad if fewer runs\n",
    "        for k in metric_keys:\n",
    "            missing = n_files_per_llm - len(metric_file_means[k])\n",
    "            if missing > 0:\n",
    "                metric_file_means[k].extend([0.0] * missing)\n",
    "\n",
    "        rec = {'Task': task, 'LLM': llm}\n",
    "        for k in metric_keys:\n",
    "            rec[k] = np.mean(metric_file_means[k])\n",
    "        records.append(rec)\n",
    "\n",
    "# Build DataFrame with multi-index and three metric columns\n",
    "df = pd.DataFrame(records)\n",
    "table = df.set_index(['Task','LLM'])[metric_keys]\n",
    "table.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "# Export to LaTeX\n",
    "latex = table.to_latex(\n",
    "    float_format=\"%.3f\",\n",
    "    caption=\"All Metrics (Prompt & Index Consistency) by Task and LLM\",\n",
    "    label=\"tab:all_metrics\"\n",
    ")\n",
    "with open('all_llm_metrics.tex', 'w') as f:\n",
    "    f.write(latex)\n",
    "\n",
    "# Print or display\n",
    "print(latex)\n",
    "print(table)\n",
    "print(\"\\nLaTeX table written to all_llm_metrics.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b4989-1cc9-48c7-b1f6-506b804d78b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
