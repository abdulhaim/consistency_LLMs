{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "deeacae3-fc5f-4a28-9aba-a3a51dee6974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{All Metrics (Prompt & Index Consistency) by Task and LLM (Mean ± Std)}\n",
      "\\label{tab:all_metrics}\n",
      "\\begin{tabular}{lllll}\n",
      "\\toprule\n",
      " &  & line-to-prompt Consistency & line-to-line Consistency & q&a Consistency \\\\\n",
      "Task & LLM &  &  &  \\\\\n",
      "\\midrule\n",
      "\\multirow[t]{3}{*}{education} & Llama-3.1-8B-Instruct & 0.824 ± 0.132 & 0.800 ± 0.148 & 0.000 ± 0.000 \\\\\n",
      " & gemma-2-2b-it & 0.511 ± 0.250 & 0.928 ± 0.092 & 0.000 ± 0.000 \\\\\n",
      " & mistral-instruct & 0.728 ± 0.191 & 0.975 ± 0.063 & 0.000 ± 0.000 \\\\\n",
      "\\cline{1-5}\n",
      "\\multirow[t]{3}{*}{therapy} & Llama-3.1-8B-Instruct & 0.740 ± 0.163 & 0.711 ± 0.158 & 0.000 ± 0.000 \\\\\n",
      " & gemma-2-2b-it & 0.665 ± 0.247 & 0.984 ± 0.040 & 0.000 ± 0.000 \\\\\n",
      " & mistral-instruct & 0.833 ± 0.142 & 0.959 ± 0.079 & 0.000 ± 0.000 \\\\\n",
      "\\cline{1-5}\n",
      "\\multirow[t]{3}{*}{chatting} & Llama-3.1-8B-Instruct & 0.619 ± 0.249 & 0.992 ± 0.025 & 0.000 ± 0.000 \\\\\n",
      " & gemma-2-2b-it & 0.871 ± 0.230 & 0.900 ± 0.123 & 0.000 ± 0.000 \\\\\n",
      " & mistral-instruct & 0.955 ± 0.097 & 0.984 ± 0.038 & 0.000 ± 0.000 \\\\\n",
      "\\cline{1-5}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "\n",
      "LaTeX table written to all_llm_metrics.tex\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "llms = ['Llama-3.1-8B-Instruct', 'gemma-2-2b-it', 'mistral-instruct']\n",
    "tasks = ['education', 'therapy', 'chatting']\n",
    "n_files_per_llm = 4\n",
    "\n",
    "metric_keys = ['P2_prompt_consistency_score', 'P2_index_consistency_score', 'P2_q&a_consistency_score']\n",
    "rename_map = {\n",
    "    'P2_prompt_consistency_score': 'line-to-prompt Consistency',\n",
    "    'P2_index_consistency_score':   'line-to-line Consistency',\n",
    "    'P2_q&a_consistency_score':     'q&a Consistency'\n",
    "}\n",
    "\n",
    "tasks_root_dir = 'results'\n",
    "records = []\n",
    "\n",
    "for task in tasks:\n",
    "    task_dir = os.path.join(tasks_root_dir, task)\n",
    "    if not os.path.isdir(task_dir):\n",
    "        print(f\"Warning: task directory not found: {task_dir}; skipping.\")\n",
    "        continue\n",
    "\n",
    "    all_files = [f for f in os.listdir(task_dir) if f.endswith('.json')]\n",
    "\n",
    "    for llm in llms:\n",
    "        llm_files = sorted([f for f in all_files if llm in f])[:n_files_per_llm]\n",
    "        metric_values = {k: [] for k in metric_keys}\n",
    "\n",
    "        for fname in llm_files:\n",
    "            path = os.path.join(task_dir, fname)\n",
    "            try:\n",
    "                data = json.load(open(path))\n",
    "            except Exception as e:\n",
    "                print(f\"  • Couldn’t read {fname} ({e}); skipping.\")\n",
    "                continue\n",
    "\n",
    "            if isinstance(data, list):\n",
    "                for entry in data:\n",
    "                    for k in metric_keys:\n",
    "                        v = entry.get(k, 0.0)\n",
    "                        if not isinstance(v, (int, float)):\n",
    "                            print(f\"    • Bad '{k}' in {fname}; using 0\")\n",
    "                            v = 0.0\n",
    "                        metric_values[k].append(v)\n",
    "            elif isinstance(data, dict):\n",
    "                for k in metric_keys:\n",
    "                    arr = data.get(k, [])\n",
    "                    if isinstance(arr, list):\n",
    "                        for v in arr:\n",
    "                            if not isinstance(v, (int, float)):\n",
    "                                print(f\"    • Bad '{k}' entry in {fname}; using 0\")\n",
    "                                v = 0.0\n",
    "                            metric_values[k].append(v)\n",
    "                    else:\n",
    "                        print(f\"    • Invalid format for '{k}' in {fname}; skipping.\")\n",
    "            else:\n",
    "                print(f\"    • Unexpected structure in {fname}; skipping.\")\n",
    "\n",
    "        rec = {'Task': task, 'LLM': llm}\n",
    "        for k in metric_keys:\n",
    "            vals = metric_values[k]\n",
    "            mean = np.mean(vals) if vals else 0.0\n",
    "            std = np.std(vals) if vals else 0.0\n",
    "            rec[k] = f\"{mean:.3f} ± {std:.3f}\"\n",
    "        records.append(rec)\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "table = df.set_index(['Task', 'LLM'])[metric_keys]\n",
    "table.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "latex = table.to_latex(\n",
    "    escape=False,\n",
    "    caption=\"All Metrics (Prompt & Index Consistency) by Task and LLM (Mean ± Std)\",\n",
    "    label=\"tab:all_metrics\"\n",
    ")\n",
    "\n",
    "with open('all_llm_metrics.tex', 'w') as f:\n",
    "    f.write(latex)\n",
    "\n",
    "print(latex)\n",
    "print(\"\\nLaTeX table written to all_llm_metrics.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b4989-1cc9-48c7-b1f6-506b804d78b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
