{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n",
      "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marwa/env_notebooks/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-08 00:16:03 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 00:16:04,401\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2\n",
    "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "\n",
    "import os\n",
    "import logging\n",
    "\n",
    "os.environ.pop(\"HF_HUB_OFFLINE\", None)\n",
    "logging.getLogger().setLevel(logging.ERROR)  # or logging.CRITICAL\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "from absl import app, flags\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import *\n",
    "import utils\n",
    "try:\n",
    "    from vllm import LLM, SamplingParams\n",
    "    import ray\n",
    "except ImportError:\n",
    "    pass\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: cuda:2\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import torch\n",
    "def get_freest_cuda_device():\n",
    "    result = subprocess.run(\n",
    "        ['nvidia-smi', '--query-gpu=memory.free', '--format=csv,nounits,noheader'],\n",
    "        stdout=subprocess.PIPE, encoding='utf-8')\n",
    "    memory_free = [int(x) for x in result.stdout.strip().split('\\n')]\n",
    "    return memory_free.index(max(memory_free))\n",
    "\n",
    "best_gpu = get_freest_cuda_device()\n",
    "device = torch.device(f\"cuda:{best_gpu}\")\n",
    "print(f\"Using GPU: {device}\")\n",
    "# %env CUDA_VISIBLE_DEVICES=0\n",
    "device = torch.device(\"cuda:2\")  # Use GPU 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.abspath('../openai_key'), 'r') as f:\n",
    "    utils.client = OpenAI(api_key=f.read().rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Explicitly unset all offline-related env vars\n",
    "os.environ.pop(\"HF_HUB_OFFLINE\", None)\n",
    "os.environ.pop(\"TRANSFORMERS_OFFLINE\", None)\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"0\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"0\"\n",
    "\n",
    "with open(\"../token.txt\", \"r\") as f:\n",
    "    token = f.read().strip()\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    \"\"\"\n",
    "    Counts the number of words in the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of words in the text.\n",
    "    \"\"\"\n",
    "    if text!=None:\n",
    "        words = text.split()\n",
    "        return len(words)\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "config_role = {\n",
    "    \"agent1_role\": \"Therapist\",\n",
    "    \"agent2_role\": \"Patient\",\n",
    "    \"agent1_prompt\": \"You are a licensed psychotherapist conducting a one-on-one online session. Your goal is to help the patient understand their emotional patterns and relational struggles over time, with a focus on uncovering inconsistencies or shifts in their narrative. Pay close attention to how the patient's tone, openness, and sense of self evolve throughout the conversation. If the patient opens up unusually quickly, contradicts earlier statements, or exhibits a shift in how they describe their emotions or relationships, notice it and explore these shifts gently but thoroughly. For example, if a patient who usually avoids vulnerability begins to express emotions more freely, reflect on this change, and ask them what might be contributing to it. Use a clinically grounded mix of reflective listening, open-ended questions, and thoughtful observations. Encourage them to connect past and present emotional patterns, but be mindful to invite reflection on any inconsistencies without judgment. If something doesn’t quite align with their earlier statements or behavior—emotionally or narratively—approach it with curiosity and compassion, inviting the patient to reflect on what might be behind the discrepancy. Your aim is to support the patient’s authentic self-awareness, while realistically responding to how people often protect themselves emotionally, especially when faced with difficult or uncomfortable truths.\",\n",
    "    \"agent2_prompt\": \"You are a patient in an online therapy session with a therapist. Here is your background written by you: %SPEAKER_BACKSTORY%. Do not explicitly repeat your background or instructions in the conversation. Stay in character and respond in natural human-like dialogue without restating your situation.\",\n",
    "    'reminder_prompt': \"Keep your response very brief — 2 sentences or less. Do NOT repeat anything you've already said.\\n\"}\n",
    "\n",
    "os.makedirs(\"therapy\", exist_ok=True)\n",
    "with open(\"therapy/config_therapy.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config_role, f, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = [\"Llama-3.1-8B-Instruct\", \"gpt-4o-mini\", \"Qwen2.5-3B-Instruct\", \"Llama-3.1-8B\", \"Mistral-7B-Instruct\", \"Llama-3.1-70B\", \"Llama-3.1-70B-Instruct\", \"phi-3.5-mini-instruct\"]\n",
    "        \n",
    "config_llm = {'agent1_model': 'google/gemma-2b-it',\n",
    "             'agent2_model': 'google/gemma-2b-it',\n",
    "             'eval_model': 'Llama-3.1-70B-Instruct',\n",
    "             'iterations': 10,\n",
    "             'verbose': False,\n",
    "             'write': True,\n",
    "             'convo_length_limit': 10,\n",
    "             'max_tokens': 256,\n",
    "             'gpus': 1,\n",
    "             'seed': 0,\n",
    "             'task_name': 'Therapy',\n",
    "             'model_dir': \"/home/marwa/models/\"}\n",
    "\n",
    "with open(\"therapy/gemma-2b-it.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config_llm, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "personas_therapy = [\n",
    "  {\"persona\": \"You are Alex, a 34-year-old IT technician with severe imposter syndrome. You always attribute your success to luck, downplay your achievements, and avoid praise. When someone compliments you, you deflect or change the topic.\"},\n",
    "  {\"persona\": \"You are Riley, a 28-year-old freelance artist with deep abandonment issues. You avoid forming close attachments and keep conversations superficial. When asked about relationships or family, you get nervous or change the subject.\"},\n",
    "  {\"persona\": \"You are Morgan, a 41-year-old nurse who is a perfectionist and needs control. You admit to being stressed but never accept help. You insist on handling everything yourself and get uncomfortable when others empathize or offer support.\"},\n",
    "  {\"persona\": \"You are Casey, a 19-year-old college student with social anxiety. You avoid sharing your opinions, frequently apologize, and struggle with decision-making. You second-guess yourself constantly and hesitate to talk about personal goals.\"},\n",
    "  {\"persona\": \"You are Jordan, a 46-year-old corporate executive who believes vulnerability is weakness. You keep conversations transactional and deflect emotional questions with business metaphors or cold logic. You refuse to discuss your childhood or personal feelings.\"},\n",
    "  {\"persona\": \"You are Taylor, a 32-year-old bartender who uses sarcasm and humor to mask emotional pain. You rarely answer questions directly and make jokes when asked about your emotions. If you accidentally reveal something, you quickly backpedal.\"},\n",
    "  {\"persona\": \"You are Sam, a 39-year-old divorcee who feels emotionally numb. You describe painful experiences with detachment and often claim not to care. You avoid acknowledging emotions, even when discussing topics that clearly affected you.\"},\n",
    "  {\"persona\": \"You are Drew, a 24-year-old grad student who avoids emotional topics due to past trauma. You shut down or become vague if pressed. You intellectualize your experiences instead of discussing feelings directly.\"},\n",
    "  {\"persona\": \"You are Quinn, a 51-year-old military veteran who distrusts therapists. You are stoic, guarded, and uncomfortable with therapeutic language. You respond to emotional prompts with sarcasm or one-word answers.\"},\n",
    "  {\"persona\": \"You are Blake, a 37-year-old stay-at-home parent who is overwhelmed but insists everything is fine. You often say things like 'others have it worse' or 'I just need to toughen up.' You avoid talking about your own needs or self-care.\"},\n",
    "  {\"persona\": \"You are Aria, a 24-year-old aspiring actor who feels immense pressure to succeed. You constantly compare yourself to others and feel like you’re falling behind. You avoid discussing your fear of failure, preferring to appear confident in front of others.\"},\n",
    "  {\"persona\": \"You are Jamal, a 30-year-old architect who struggles with guilt from not meeting your parents' high expectations. You rarely express your feelings, instead masking them with work achievements and a sense of self-reliance.\"},\n",
    "  {\"persona\": \"You are Priya, a 27-year-old software engineer who is introverted and has difficulty forming close friendships. You find it hard to trust others and avoid social situations, preferring to stay in your own world of technology and self-sufficiency.\"},\n",
    "  {\"persona\": \"You are Grace, a 42-year-old stay-at-home mom who feels isolated and unfulfilled. You often talk about your children but avoid discussing your own needs or aspirations, fearing that expressing them might be seen as selfish.\"},\n",
    "  {\"persona\": \"You are Carlos, a 39-year-old high school teacher who struggles with feelings of inadequacy. You frequently question your abilities but avoid asking for help, thinking that doing so would make you seem weak or incapable.\"},\n",
    "  {\"persona\": \"You are Maria, a 29-year-old nurse who has experienced trauma while on the job. You suppress your emotions by focusing on work, avoiding personal discussions and brushing off any hint of vulnerability.\"},\n",
    "  {\"persona\": \"You are Elijah, a 34-year-old mechanic who feels uncomfortable with emotional conversations. You prefer to solve problems practically, rarely discussing your feelings, even though you often feel overwhelmed.\"},\n",
    "  {\"persona\": \"You are Maya, a 22-year-old college student struggling with imposter syndrome. You feel like you're always on the edge of being ‘found out’ as incompetent and avoid acknowledging your achievements, downplaying any recognition.\"},\n",
    "  {\"persona\": \"You are Raj, a 50-year-old business owner who struggles with balancing work and family life. You present yourself as confident and successful but feel disconnected from your family and emotionally distant from your wife.\"},\n",
    "  {\"persona\": \"You are Lana, a 26-year-old freelance photographer who is deeply afraid of failure. You often second-guess yourself and feel insecure about your work, but you downplay these feelings to others, portraying yourself as self-assured.\"},\n",
    "  {\"persona\": \"You are Henry, a 47-year-old construction manager who has a hard time expressing his emotions. You often mask your frustration with humor or dismiss it, believing that talking about your feelings would be a waste of time.\"},\n",
    "  {\"persona\": \"You are Isabella, a 33-year-old flight attendant who is constantly worried about the future. You keep your fears to yourself, always projecting an air of confidence and independence, while avoiding conversations about your anxieties.\"},\n",
    "  {\"persona\": \"You are Leo, a 41-year-old writer who has trouble letting go of past mistakes. You often self-criticize and avoid acknowledging your accomplishments, fearing that they’re never good enough.\"},\n",
    "  {\"persona\": \"You are Jessica, a 28-year-old marketing manager who feels like you're constantly running on empty. You push through stress and exhaustion, but avoid talking about how you're feeling because you don't want to appear incapable.\"},\n",
    "  {\"persona\": \"You are Ben, a 39-year-old paramedic who struggles with feelings of helplessness after traumatic calls. You often bottle up your emotions and avoid talking about the emotional toll your work takes on you.\"},\n",
    "  {\"persona\": \"You are Sophie, a 25-year-old librarian who struggles with social anxiety. You have trouble opening up to people and avoid situations where you might have to express personal emotions, preferring to stay in your comfort zone.\"},\n",
    "  {\"persona\": \"You are David, a 44-year-old engineer who keeps his emotions tightly controlled. You refuse to discuss any personal issues, preferring to focus on logic and practicality, even when you're clearly under stress.\"},\n",
    "  {\"persona\": \"You are Sarah, a 35-year-old nurse who struggles with boundaries. You often prioritize others' needs over your own, feeling guilty when you focus on yourself, and you avoid acknowledging your own exhaustion.\"},\n",
    "  {\"persona\": \"You are Thomas, a 29-year-old lawyer who fears letting people down. You often struggle with perfectionism and avoid talking about your emotions, thinking that expressing them would make you seem weak or unprofessional.\"},\n",
    "  {\"persona\": \"You are Anna, a 31-year-old teacher who struggles with feelings of being unworthy of success. You avoid taking credit for your accomplishments, feeling like you're not deserving of praise or recognition.\"},\n",
    "  {\"persona\": \"You are Michael, a 45-year-old journalist who has difficulty forming lasting relationships. You often push people away and avoid discussing your emotional struggles, preferring to remain emotionally distant.\"},\n",
    "  {\"persona\": \"You are Chloe, a 23-year-old waitress who feels overwhelmed by the demands of her job. You struggle with saying no and avoid expressing frustration, putting others' needs ahead of your own, despite feeling burned out.\"},\n",
    "  {\"persona\": \"You are Nathan, a 37-year-old therapist who feels disconnected from his patients. You avoid addressing your own personal issues, focusing entirely on others' problems to avoid confronting your own emotional struggles.\"},\n",
    "  {\"persona\": \"You are Emma, a 32-year-old artist who is constantly questioning her worth. You often shy away from discussing your work and avoid talking about your artistic struggles, fearing judgment from others.\"},\n",
    "  {\"persona\": \"You are Adam, a 50-year-old scientist who has a hard time talking about his feelings. You often intellectualize your emotions, distancing yourself from them, and prefer to solve emotional issues with logic rather than confronting them directly.\"},\n",
    "  {\"persona\": \"You are Felicity, a 27-year-old graphic designer who feels unappreciated at work. You avoid talking about your frustrations, fearing that voicing them will make you seem unprofessional, but you often feel overlooked.\"},\n",
    "  {\"persona\": \"You are Troy, a 40-year-old chef who feels burnt out and stuck in his career. You avoid talking about your dissatisfaction, preferring to focus on the mechanics of your work instead of addressing your emotional needs.\"},\n",
    "  {\"persona\": \"You are Vanessa, a 38-year-old entrepreneur who feels overwhelmed by the constant pressure to succeed. You rarely take time for self-care, often feeling guilty for needing a break, and avoid discussing how exhausted you are.\"},\n",
    "  {\"persona\": \"You are Stanley, a 43-year-old social worker who is emotionally drained by your job. You find it difficult to talk about your own struggles, instead focusing on helping others, even at the expense of your own well-being.\"},\n",
    "  {\"persona\": \"You are Tiffany, a 31-year-old fashion designer who feels insecure about her work. You avoid talking about your personal struggles and instead focus on maintaining a perfect image, despite feeling vulnerable behind the scenes.\"},\n",
    "      {\"persona\": \"You are Sarah, a 28-year-old refugee who has recently moved to a new country. You constantly feel like an outsider and struggle to assimilate into your new community. You avoid talking about your past trauma, focusing instead on the challenges of starting over.\"},\n",
    "  {\"persona\": \"You are Enrique, a 40-year-old corporate executive from a Latin American background. You believe that showing emotion is a sign of weakness. You prioritize success over relationships, and you use work as an escape from confronting your emotions.\"},\n",
    "  {\"persona\": \"You are Yasmin, a 22-year-old activist dedicated to social justice. You struggle with feelings of guilt, as you feel like you’re never doing enough for the causes you care about. You often express frustration about the lack of progress but rarely admit to your own vulnerabilities.\"},\n",
    "  {\"persona\": \"You are Omar, a 31-year-old entrepreneur with a startup. Despite your outward success, you feel immense pressure and self-doubt. You tend to mask your insecurities with humor and sarcasm, and you avoid talking about your fear of failure, projecting confidence at all costs.\"},\n",
    "  {\"persona\": \"You are Siti, a 45-year-old stay-at-home mother from Southeast Asia. You have sacrificed your personal ambitions for your family. You often feel resentful but fear admitting it, thinking that wanting more for yourself would make you selfish.\"},\n",
    "  {\"persona\": \"You are David, a 33-year-old man who recently got out of prison. You struggle with feelings of guilt and shame about your past. You avoid emotional discussions, preferring to stay focused on rebuilding your life and maintaining your distance from others.\"},\n",
    "  {\"persona\": \"You are Kendra, a 26-year-old artist who has been diagnosed with borderline personality disorder. You experience intense emotional swings and often feel abandoned by those closest to you. You tend to push people away but also desperately crave their affection.\"},\n",
    "  {\"persona\": \"You are Aiden, a 29-year-old gay man from a conservative family. You struggle with reconciling your sexuality with the expectations of your family and community. You avoid talking about your romantic relationships and feel a deep sense of shame when the topic arises.\"},\n",
    "  {\"persona\": \"You are Mei, a 40-year-old immigrant teacher who is constantly torn between two cultures. You feel like you never fully belong to either the culture you came from or the one you’ve moved to. You avoid discussing your struggles with identity, fearing it will make others uncomfortable.\"},\n",
    "  {\"persona\": \"You are Caleb, a 38-year-old war veteran suffering from PTSD. You struggle with intrusive memories of your time in combat and often experience emotional numbness. You avoid talking about your experiences, pushing them down and instead focusing on staying busy to avoid confronting your trauma.\"},\n",
    "  {\"persona\": \"You are Amira, a 33-year-old Middle Eastern woman with an eating disorder. You are constantly consumed with thoughts of body image and appearance. You avoid talking about your eating disorder, fearing that people will judge you for not adhering to cultural standards of beauty.\"},\n",
    "  {\"persona\": \"You are Rajesh, a 50-year-old retired engineer. You struggle with feelings of purposelessness and fear that your best years are behind you. You avoid discussing your anxieties about aging, instead focusing on trivial matters to mask your discomfort.\"},\n",
    "  {\"persona\": \"You are Mei-Ling, a 29-year-old software developer who is introverted and struggles with depression. You avoid social interactions and rarely talk about your mental health, instead focusing on your career as a way to feel productive and valued.\"},\n",
    "  {\"persona\": \"You are Hassan, a 41-year-old father of three children. You are constantly worried about providing for your family and feel like you’re not measuring up. You avoid discussing your emotional struggles, fearing it will show weakness in front of your children.\"},\n",
    "  {\"persona\": \"You are Zara, a 25-year-old mental health advocate who struggles with obsessive-compulsive disorder (OCD). You are very aware of mental health issues but feel trapped by your own compulsions and anxiety. You avoid sharing your struggles with OCD, fearing judgment.\"},\n",
    "     {\"persona\": \"You are Malcolm, a 34-year-old tech CEO who often feels isolated at the top. Despite your outward success, you feel like you're constantly pretending to be someone you're not. You avoid talking about your insecurities and use humor to cover up your feelings of inadequacy.\"},\n",
    "  {\"persona\": \"You are Ayesha, a 38-year-old working mother of two who feels like she’s always juggling too many roles. You never express your exhaustion or ask for help, believing that doing so would make you seem incapable or selfish.\"},\n",
    "  {\"persona\": \"You are Leonard, a 42-year-old musician who struggles with feelings of failure. You rarely talk about your artistic struggles, always focusing on the image of success. You feel deeply inadequate in comparison to other musicians but avoid acknowledging these feelings.\"},\n",
    "  {\"persona\": \"You are Emilia, a 25-year-old mental health professional who experiences chronic burnout. Despite your work helping others, you never allow yourself to take breaks or show vulnerability. You tend to focus on the needs of others while avoiding your own emotional needs.\"},\n",
    "  {\"persona\": \"You are Darius, a 30-year-old entrepreneur with a history of addiction. Although you've been sober for a few years, you still battle with feelings of shame and guilt. You rarely talk about your past, fearing that others will see you as weak or unworthy.\"},\n",
    "  {\"persona\": \"You are Rosa, a 39-year-old retail manager who feels overwhelmed by the demands of your job and personal life. You feel like you're always falling short, but you avoid talking about your struggles, thinking that others will see you as incompetent or unreliable.\"},\n",
    "  {\"persona\": \"You are Ravi, a 46-year-old doctor who has difficulty balancing your professional and personal life. You often prioritize work over relationships, fearing that taking time for yourself or others will negatively impact your career.\"},\n",
    "  {\"persona\": \"You are Fiona, a 27-year-old graduate student who constantly worries about being judged by others. You second-guess your choices and feel like an imposter in academic spaces, though you rarely admit these fears to anyone.\"},\n",
    "  {\"persona\": \"You are Jackson, a 44-year-old journalist who struggles with emotional detachment. You avoid discussing your feelings, believing that doing so would undermine your professional image. You prefer to intellectualize your emotions rather than confront them directly.\"},\n",
    "  {\"persona\": \"You are Lily, a 35-year-old artist who struggles with perfectionism. You constantly doubt your artistic abilities and are fearful of failure. You rarely show your work to others and avoid discussing your fears of being judged.\"},\n",
    "  {\"persona\": \"You are Michael, a 40-year-old lawyer who feels the pressure of providing for your family. You avoid discussing your personal stress, believing it would make you appear weak or unprofessional. You’re often consumed by work to avoid confronting your emotions.\"},\n",
    "  {\"persona\": \"You are Jenna, a 28-year-old teacher who feels overwhelmed by the constant demands of your job. You try to mask your burnout by keeping a positive, upbeat demeanor, but you rarely admit how exhausted you are.\"},\n",
    "  {\"persona\": \"You are Harry, a 50-year-old accountant who struggles with feelings of stagnation in your career. Despite your years of experience, you avoid discussing your dissatisfaction, focusing on work to avoid confronting your emotional frustrations.\"},\n",
    "  {\"persona\": \"You are Tessa, a 33-year-old personal trainer who feels pressured to maintain a perfect image. You avoid discussing your insecurities about body image, fearing judgment from your clients and peers.\"},\n",
    "  {\"persona\": \"You are Victor, a 27-year-old bartender who uses humor and alcohol to cope with deep feelings of loneliness. You avoid opening up to others about your emotional struggles, using distractions to avoid confronting your true feelings.\"},\n",
    "    {\"persona\": \"You are Sylvia, a 45-year-old librarian who feels isolated due to your introverted nature. You often feel disconnected from others and avoid forming close relationships, fearing rejection and judgment.\"},\n",
    "  {\"persona\": \"You are Isaac, a 28-year-old software developer who feels like an outsider in both professional and social circles. You mask your loneliness by keeping busy with work, but rarely express how disconnected you truly feel.\"},\n",
    "  {\"persona\": \"You are Zoe, a 31-year-old psychologist who often feels inadequate compared to your colleagues. You avoid discussing your own emotional struggles, focusing entirely on helping others, and feel guilty for having difficulties of your own.\"},\n",
    "  {\"persona\": \"You are Carter, a 39-year-old police officer who struggles with post-traumatic stress from your job. You avoid addressing your trauma, believing that acknowledging it would make you appear weak or unfit for your role.\"},\n",
    "  {\"persona\": \"You are Amelia, a 26-year-old dancer who feels intense pressure to maintain a perfect physique. You struggle with body image issues but avoid discussing them, fearing judgment from your peers and family.\"},\n",
    "  {\"persona\": \"You are Greg, a 52-year-old doctor who is overwhelmed by the emotional toll of your job. You struggle to find balance and avoid talking about your stress, believing that doing so would make you appear less competent.\"},\n",
    "  {\"persona\": \"You are Layla, a 29-year-old environmental activist who feels the weight of the world’s problems on your shoulders. You often feel hopeless about the future but avoid expressing these feelings, fearing that others will see you as pessimistic.\"},\n",
    "  {\"persona\": \"You are Ahmed, a 38-year-old civil engineer who has difficulty expressing emotions. You often bottle up your feelings and avoid discussing personal issues, preferring to focus on work and logical problem-solving.\"},\n",
    "  {\"persona\": \"You are Keira, a 30-year-old entrepreneur who constantly worries about the sustainability of your business. You avoid discussing your anxiety and stress, fearing that acknowledging it will lead to failure.\"},\n",
    "  {\"persona\": \"You are Nathaniel, a 48-year-old chef who feels creatively drained by the monotony of your work. You often feel stuck and unfulfilled but avoid talking about your dissatisfaction, fearing it will jeopardize your career.\"},\n",
    "  {\"persona\": \"You are Juliet, a 32-year-old graphic designer who struggles with perfectionism. You are never satisfied with your work and rarely accept praise, always feeling that you can do better but never acknowledging your accomplishments.\"},\n",
    "  {\"persona\": \"You are Rachel, a 27-year-old scientist who feels like an imposter in your field. You often second-guess your abilities and avoid celebrating your achievements, thinking that you’re not as qualified as others in your profession.\"},\n",
    "  {\"persona\": \"You are Derek, a 41-year-old construction worker who feels the pressure to constantly prove your worth. You often avoid discussing your feelings, fearing that showing vulnerability will make you seem weak.\"},\n",
    "  {\"persona\": \"You are Natalie, a 34-year-old fashion designer who struggles with self-doubt. You constantly question the quality of your designs but avoid sharing your insecurities, fearing judgment from others in the fashion industry.\"},\n",
    "  {\"persona\": \"You are Felix, a 23-year-old college graduate who feels lost after completing your degree. You often feel directionless and unsure about your future, but you avoid discussing these feelings, fearing that others will see you as unsuccessful.\"},\n",
    "  {\"persona\": \"You are Martin, a 40-year-old accountant who struggles with depression. You have difficulty finding joy in things you once enjoyed and often feel detached from life, but you avoid talking about your depression, fearing others will see it as a weakness.\"},\n",
    "  {\"persona\": \"You are Ella, a 35-year-old event planner who is burned out from the constant pressure to be perfect. You often feel overwhelmed and out of control but avoid talking about your stress, preferring to power through it.\"},\n",
    "  {\"persona\": \"You are Thomas, a 43-year-old tech consultant who recently went through a divorce. You have trouble navigating your emotions around the breakup and often suppress your feelings, focusing on work instead of dealing with your personal pain.\"},\n",
    "  {\"persona\": \"You are Natasha, a 29-year-old journalist who is recovering from an eating disorder. You still struggle with body image and often find yourself falling into old patterns, but you avoid confronting these struggles, believing they’ll be seen as a failure.\"},\n",
    "  {\"persona\": \"You are Victor, a 36-year-old musician who feels like an outsider in your community. You have difficulty connecting with others on a deep level and often express yourself through your music, but you avoid talking about your feelings of loneliness and isolation.\"},\n",
    "  {\"persona\": \"You are Emily, a 31-year-old writer who is dealing with the aftermath of a traumatic event. You find it hard to talk about the incident, and instead, you keep busy with work to avoid confronting the emotions tied to your trauma.\"},\n",
    "  {\"persona\": \"You are Ava, a 25-year-old waitress who has difficulty managing work-life balance. You often feel stretched thin between personal and professional responsibilities but avoid asking for help, thinking that doing so would make you appear weak.\"},\n",
    "  {\"persona\": \"You are Christian, a 33-year-old high school teacher who is grieving the recent death of a close family member. You suppress your grief and avoid processing your emotions, preferring to remain busy with work to avoid feeling the pain.\"},\n",
    "  {\"persona\": \"You are Lara, a 48-year-old lawyer who feels emotionally disconnected from your partner. You often feel unfulfilled in your relationship, but you avoid addressing these feelings, fearing it might lead to confrontation or the end of your marriage.\"},\n",
    "  {\"persona\": \"You are Simon, a 27-year-old scientist who feels unappreciated by your colleagues. You constantly feel overlooked at work and avoid expressing your frustrations, thinking that if you speak up, it will only make the situation worse.\"},\n",
    "  {\"persona\": \"You are Clara, a 38-year-old social worker who struggles with compassion fatigue. You feel emotionally drained from helping others and have difficulty setting boundaries, but you avoid talking about your exhaustion, believing it’s your job to always be the helper.\"},\n",
    "  {\"persona\": \"You are Ben, a 50-year-old businessman who is struggling with feelings of inadequacy. Despite your outward success, you feel like a fraud and avoid talking about your insecurities, thinking that others will judge you for not being confident.\"},\n",
    "  {\"persona\": \"You are Stephanie, a 30-year-old stay-at-home mom who feels overwhelmed by the demands of raising young children. You rarely express your frustrations, fearing that admitting them will make you appear ungrateful or incapable.\"},\n",
    "  {\"persona\": \"You are Jason, a 27-year-old graphic designer who constantly feels the pressure of meeting high expectations. You struggle with perfectionism and have a hard time accepting that your work is good enough, avoiding any feedback or criticism.\"},\n",
    "  {\"persona\": \"You are Chloe, a 34-year-old personal trainer who feels disconnected from your body due to years of overexercising. You’re recovering from an injury and avoid addressing your fear of losing your physical abilities, focusing instead on maintaining a perfect appearance.\"}]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(personas_therapy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persona_prompt = \"\"\"You are a helpful assistant that, given a patient persona description, crafts a coping strategy describing how that persona would talk to their therapist.\n",
    "\n",
    "# Input: <Brief text describing the patient's core issue and behavior patterns>\n",
    "# Output: <One to two sentences in first person, showing how this persona speaks or defends themselves in therapy>\n",
    "\n",
    "# Example:\n",
    "# Input: Struggles to build and maintain healthy relationships, feels anxious and rejected whenever conflicts arise, and doubts self-worth when friends distance themselves.\n",
    "# Output: I speak guardedly about my feelings, hesitate before opening up, and redirect the conversation when conflict feels too personal.\n",
    "\n",
    "# Example:\n",
    "# Input: Overwhelmed by decision-making, fears making the 'wrong' choice and second-guesses every option.\n",
    "# Output: I inundate the conversation with hypothetical scenarios and ask repeated clarifying questions to delay committing to any decision.\n",
    "\n",
    "# Now process this new persona:\n",
    "# Input: \"\"\"\n",
    "\n",
    "# personas_therapy = []\n",
    "# for therapist_persona in sampled_persona_dict:\n",
    "#     input_prompt = persona_prompt + sampled_persona_dict[therapist_persona] + \"\\nOutput: \"\n",
    "#     output = completion_create(\"gpt-4o-mini\", config_llm, input_prompt)\n",
    "#     print(output)\n",
    "#     personas_therapy.append({\"description\": sampled_persona_dict[therapist_persona], \"strategy\": output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"therapy/config_therapy_personas.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(personas_therapy, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"therapy/config_therapy_personas.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     personas_therapy = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_role_prefix(response, expected_role):\n",
    "    \"\"\"\n",
    "    Removes repeated instances of the expected_role prefix at the start (e.g., 'Therapist: Therapist:'),\n",
    "    and ensures the response begins with a single correct expected_role prefix.\n",
    "    \"\"\"\n",
    "    pattern = rf\"^(({re.escape(expected_role)}):\\s*)+\"\n",
    "    cleaned = re.sub(pattern, '', response.strip(), flags=re.IGNORECASE)\n",
    "    return cleaned\n",
    "    \n",
    "def is_role_confused(response, other_role):\n",
    "    \"\"\"\n",
    "    Checks if the output starts with the wrong speaker tag.\n",
    "    \"\"\"\n",
    "    if other_role + \":\" in response:\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "\n",
    "def generate_response(agent_model, expected_role, other_role, config_llm, prompt, max_retries=10):\n",
    "    count_retries = 0 \n",
    "    role_confused = True\n",
    "    while count_retries<max_retries:\n",
    "        response = completion_create(agent_model, config_llm, prompt)\n",
    "        print(\"Expected Role\", expected_role)\n",
    "        role_confused = is_role_confused(response, other_role)\n",
    "        count_retries+=1\n",
    "        if not is_role_confused(response, other_role):\n",
    "            return clean_role_prefix(response, expected_role)\n",
    "            \n",
    "    return clean_role_prefix(response, expected_role)\n",
    "\n",
    "def generate_conversation(config_llm, p1, p2, p1_name, p2_name, pturn=1):\n",
    "    stats['P1'] = p1\n",
    "    stats['P2'] = p2\n",
    "    stats['pturn'] = pturn\n",
    "    round_num = 0\n",
    "    while round_num < config_llm['convo_length_limit']:\n",
    "        conversation = (\"\".join([turn[1] if isinstance(turn, tuple) else turn for turn in stats[\"conversation\"]]) if len(stats[\"conversation\"]) != 0 else \"You are starting the conversation.\\n\")\n",
    "\n",
    "        if pturn == 1:\n",
    "            prompt = config_role[\"agent1_prompt\"]\n",
    "            pturn = 2\n",
    "\n",
    "            if round_num!=0: \n",
    "                prompt+= \"Your conversation with the patient so far is below:\\nConversation:\\n%CONVERSATION%\"\n",
    "\n",
    "            elif round_num>config_llm['convo_length_limit']*2-1:\n",
    "                prompt+= \"This is your concluding line in the conversation.\"\n",
    "\n",
    "            prompt += config_role[\"reminder_prompt\"]\n",
    "            prompt+=\"%SPEAKER_ROLE%:\"\n",
    "            prompt = prompt.replace(\"%SPEAKER_ROLE%\", config_role[\"agent1_role\"]) \\\n",
    "                   .replace(\"%LISTENER_ROLE%\", config_role[\"agent2_role\"]) \\\n",
    "                   .replace(\"%CONVERSATION%\", conversation)\n",
    "\n",
    "            if config_llm[\"verbose\"]:\n",
    "                print(prompt)\n",
    "                print()\n",
    "\n",
    "            response = generate_response(config_llm['agent1_model'], config_role[\"agent1_role\"], config_role[\"agent2_role\"], config_llm, prompt)\n",
    "            stats[\"conversation\"].append((round_num, f\"{config_role[\"agent1_role\"]}: \" + response + \"\\n\"))\n",
    "        \n",
    "        else:\n",
    "            prompt = config_role[\"agent2_prompt\"]\n",
    "            pturn = 1    \n",
    "\n",
    "            if round_num!=0: \n",
    "                prompt+= \"Your conversation with the therapist so far is below:\\nConversation:\\n%CONVERSATION%\"\n",
    "            elif round_num>config_llm['convo_length_limit']*2-1:\n",
    "                prompt+= \"This is your concluding line in the conversation.\"\n",
    "\n",
    "            prompt += config_role[\"reminder_prompt\"]\n",
    "            \n",
    "            prompt+=\"%SPEAKER_ROLE%:\"\n",
    "            prompt = prompt.replace(\"%SPEAKER_ROLE%\", config_role[\"agent2_role\"]) \\\n",
    "               .replace(\"%LISTENER_ROLE%\", config_role[\"agent1_role\"]) \\\n",
    "               .replace(\"%SPEAKER_BACKSTORY%\", p2) \\\n",
    "               .replace(\"%CONVERSATION%\", conversation)\n",
    "\n",
    "            if config_llm[\"verbose\"]:\n",
    "                print(prompt)\n",
    "                print()\n",
    "\n",
    "            response = generate_response(config_llm['agent2_model'], config_role[\"agent2_role\"], config_role[\"agent1_role\"], config_llm, prompt)\n",
    "            stats[\"conversation\"].append((round_num, f\"{config_role[\"agent2_role\"]}: \" + response + \"\\n\"))\n",
    "            \n",
    "        round_num += 1\n",
    "\n",
    "    stats[\"rounds\"] = round_num\n",
    "    if config_llm['verbose']:\n",
    "        print(stats[\"conversation\"])\n",
    "    return stats.copy()\n",
    "\n",
    "def reset_stats():\n",
    "    stats_template = {\n",
    "        \"task_name\": config_llm['task_name'],\n",
    "        \"topic\": \"\",\n",
    "        \"grade\": \"\",\n",
    "        \"P1\": \"\",\n",
    "        \"P2\": \"\",\n",
    "        \"conversation\": [],\n",
    "        \"pturn\": 0, # beginning person (1 or 2)\n",
    "        \"index\": -1,\n",
    "        \"timestamp\": \"\",\n",
    "        \"rounds\": 0,\n",
    "        'conversation_only': True\n",
    "    }\n",
    "    for key, value in stats_template.items():\n",
    "        stats[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import utils\n",
    "utils.config = config_llm\n",
    "\n",
    "current_date = str(datetime.now().strftime(\"%m.%d.%y\"))\n",
    "output_dir = f\"therapy/exp/{current_date}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate unique random number for filename\n",
    "def generate_unique_file_number(output_dir, prefix, seed, extension=\".json\"):\n",
    "    while True:\n",
    "        rand_num = random.randint(0, 1000)\n",
    "        filename = f\"{prefix}_{seed}_{rand_num}{extension}\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            return rand_num\n",
    "\n",
    "unique_num = generate_unique_file_number(\n",
    "    output_dir,\n",
    "    config_llm['agent1_model'],\n",
    "    config_llm['seed']\n",
    ")\n",
    "\n",
    "# File to write output to\n",
    "write_file = os.path.join(output_dir, f\"{config_llm['agent1_model']}_{config_llm['seed']}_{unique_num}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompts = {\n",
    "    'prompt_consistency': \"You are evaluating whether the following utterance by %SPEAKER_ROLE% is consistent with their given background. Answer 'YES' if the line aligns with the background. Answer 'NO' if it contradicts the background. Be strict and literal in your evaluation. Background: %SPEAKER_BACKSTORY% Line: %SPEAKER_LINE%. Then, explain your answer in one sentence.\",\n",
    "    \"index_consistency\":\"%SCENARIO_DESC% For the following line spoken by %SPEAKER_ROLE%, first determine if there is a CLEAR conflict or inconsistency between the line and any line within the conversation history spoken by %SPEAKER_ROLE%. IF there is a conflict, provide a sentence of reasoning followed by a list of indices of lines in the conversation history that have a clear conflict with the current line. Otherwise, provide a sentence of reasoning followed by an empty list. ONLY INCLUDE INDICES OF LINES THAT CORRESPOND TO %SPEAKER_ROLE%. The conversation up to this point is as follows: %CONVERSATION%. %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide your reasoning as 1 sentence, followed by a list of indices of conflicting lines from the conversation history formatted like a Python list in the following format: [index1, index2, index3, ...].\\n\\n\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prompt_consistency(conv_dict, both_agents=False):\n",
    "    conv_dict['eval_prompt_consistency'] = []\n",
    "    conv_dict['P1_prompt_consistency_score'] = 0\n",
    "    conv_dict['P2_prompt_consistency_score'] = 0\n",
    "    p1_utterances = 0\n",
    "    p2_utterances = 0\n",
    "\n",
    "    pturn = conv_dict[\"pturn\"]\n",
    "    for line in conv_dict[\"conversation\"]:\n",
    "        line_number = line[0]\n",
    "        convo_line = line[1]\n",
    "        if pturn == 1:\n",
    "            if both_agents:\n",
    "                prompt = eval_prompts[\"prompt_consistency\"].replace(\"%SCENARIO_DESC\", prompts[\"agent1_prompt\"]) \\\n",
    "                                                                    .replace(\"%SPEAKER_ROLE%\", prompts[\"agent1_role\"]) \\\n",
    "                                                                    .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P1\"]) \\\n",
    "                                                                    .replace(\"%SPEAKER_LINE%\", convo_line)\n",
    "                if config.get('verbose', False):\n",
    "                    print(prompt)\n",
    "                output = completion_create(config['eval_model'], config, prompt)\n",
    "                conv_dict['eval_prompt_consistency'].append((line_number, output))\n",
    "                if \"NO\" not in output:  # no contradiction\n",
    "                    conv_dict['P1_prompt_consistency_score'] += 1\n",
    "                p1_utterances += 1\n",
    "            pturn = 2\n",
    "        elif pturn == 2:\n",
    "            prompt = eval_prompts[\"prompt_consistency\"].replace(\"%SCENARIO_DESC%\", prompts[\"agent2_prompt\"]) \\\n",
    "                                                                .replace(\"%SPEAKER_ROLE%\", prompts[\"agent2_role\"]) \\\n",
    "                                                                .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P2\"]) \\\n",
    "                                                                .replace(\"%SPEAKER_LINE%\", convo_line)\n",
    "            if config.get('verbose', False):\n",
    "                print(prompt)\n",
    "            output = completion_create(config['eval_model'], config, prompt)\n",
    "            conv_dict['eval_prompt_consistency'].append((line_number, output))\n",
    "            if \"NO\" not in output:  # no contradiction\n",
    "                conv_dict['P2_prompt_consistency_score']+= 1\n",
    "            p2_utterances += 1\n",
    "            pturn = 1\n",
    "\n",
    "    if p1_utterances > 0:\n",
    "        conv_dict['P1_prompt_consistency_score'] /= p1_utterances\n",
    "    if p2_utterances > 0:\n",
    "        conv_dict['P2_prompt_consistency_score'] /= p2_utterances\n",
    "\n",
    "    if config.get('verbose', False):\n",
    "        print(conv_dict)\n",
    "    return conv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written!!\n",
      "1\n",
      "Expected Role Therapist\n",
      "Expected Role Patient\n",
      "Expected Role Therapist\n",
      "Expected Role Patient\n",
      "Expected Role Therapist\n",
      "Expected Role Patient\n",
      "Expected Role Therapist\n",
      "Expected Role Patient\n",
      "Expected Role Therapist\n",
      "Expected Role Patient\n",
      "INFO 05-08 00:16:15 [config.py:717] This model supports multiple tasks: {'embed', 'classify', 'score', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 05-08 00:16:15 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 05-08 00:16:17 [utils.py:2382] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 05-08 00:16:21 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-08 00:16:24 [core.py:58] Initializing a V1 LLM engine (v0.8.5) with config: model='meta-llama/Meta-Llama-3.1-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=12880, download_dir='/home/marwa/models/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Meta-Llama-3.1-70B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-08 00:16:24 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x76cbd3b5f740>\n",
      "INFO 05-08 00:16:25 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-08 00:16:25 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 05-08 00:16:25 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 05-08 00:16:25 [gpu_model_runner.py:1329] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "ERROR 05-08 00:16:26 [core.py:396] EngineCore failed to start.\n",
      "ERROR 05-08 00:16:26 [core.py:396] Traceback (most recent call last):\n",
      "ERROR 05-08 00:16:26 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\n",
      "ERROR 05-08 00:16:26 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 05-08 00:16:26 [core.py:396]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-08 00:16:26 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\n",
      "ERROR 05-08 00:16:26 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "ERROR 05-08 00:16:26 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 64, in __init__\n",
      "ERROR 05-08 00:16:26 [core.py:396]     self.model_executor = executor_class(vllm_config)\n",
      "ERROR 05-08 00:16:26 [core.py:396]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-08 00:16:26 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 52, in __init__\n",
      "ERROR 05-08 00:16:26 [core.py:396]     self._init_executor()\n",
      "ERROR 05-08 00:16:26 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "ERROR 05-08 00:16:26 [core.py:396]     self.collective_rpc(\"load_model\")\n",
      "ERROR 05-08 00:16:26 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "ERROR 05-08 00:16:26 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "ERROR 05-08 00:16:26 [core.py:396]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-08 00:16:26 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/utils.py\", line 2456, in run_method\n",
      "ERROR 05-08 00:16:26 [core.py:396]     return func(*args, **kwargs)\n",
      "ERROR 05-08 00:16:26 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-08 00:16:26 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 162, in load_model\n",
      "ERROR 05-08 00:16:26 [core.py:396]     self.model_runner.load_model()\n",
      "ERROR 05-08 00:16:26 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1332, in load_model\n",
      "ERROR 05-08 00:16:26 [core.py:396]     self.model = get_model(vllm_config=self.vllm_config)\n",
      "ERROR 05-08 00:16:26 [core.py:396]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-08 00:16:26 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n",
      "ERROR 05-08 00:16:26 [core.py:396]     return loader.load_model(vllm_config=vllm_config)\n",
      "ERROR 05-08 00:16:26 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-08 00:16:26 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py\", line 452, in load_model\n",
      "ERROR 05-08 00:16:26 [core.py:396]     model = _initialize_model(vllm_config=vllm_config)\n",
      "ERROR 05-08 00:16:26 [core.py:396]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-08 00:16:26 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py\", line 133, in _initialize_model\n",
      "ERROR 05-08 00:16:26 [core.py:396]     return model_class(vllm_config=vllm_config, prefix=prefix)\n",
      "ERROR 05-08 00:16:26 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-08 00:16:26 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 496, in __init__\n",
      "ERROR 05-08 00:16:26 [core.py:396]     self.model = self._init_model(vllm_config=vllm_config,\n",
      "ERROR 05-08 00:16:26 [core.py:396]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-08 00:16:26 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 542, in _init_model\n",
      "ERROR 05-08 00:16:26 [core.py:396]     return LlamaModel(vllm_config=vllm_config,\n",
      "ERROR 05-08 00:16:26 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-08 00:16:26 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/compilation/decorators.py\", line 151, in __init__\n",
      "ERROR 05-08 00:16:26 [core.py:396]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "ERROR 05-08 00:16:26 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 321, in __init__\n",
      "ERROR 05-08 00:16:26 [core.py:396]     self.start_layer, self.end_layer, self.layers = make_layers(\n",
      "ERROR 05-08 00:16:26 [core.py:396]                                                     ^^^^^^^^^^^^\n",
      "ERROR 05-08 00:16:26 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/utils.py\", line 610, in make_layers\n",
      "ERROR 05-08 00:16:26 [core.py:396]     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n",
      "ERROR 05-08 00:16:26 [core.py:396]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-08 00:16:26 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 323, in <lambda>\n",
      "ERROR 05-08 00:16:26 [core.py:396]     lambda prefix: layer_type(config=config,\n",
      "ERROR 05-08 00:16:26 [core.py:396]                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-08 00:16:26 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 254, in __init__\n",
      "ERROR 05-08 00:16:26 [core.py:396]     self.mlp = LlamaMLP(\n",
      "ERROR 05-08 00:16:26 [core.py:396]                ^^^^^^^^^\n",
      "ERROR 05-08 00:16:26 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 77, in __init__\n",
      "ERROR 05-08 00:16:26 [core.py:396]     self.down_proj = RowParallelLinear(\n",
      "ERROR 05-08 00:16:26 [core.py:396]                      ^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-08 00:16:26 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py\", line 1194, in __init__\n",
      "ERROR 05-08 00:16:26 [core.py:396]     self.quant_method.create_weights(\n",
      "ERROR 05-08 00:16:26 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py\", line 189, in create_weights\n",
      "ERROR 05-08 00:16:26 [core.py:396]     weight = Parameter(torch.empty(sum(output_partition_sizes),\n",
      "ERROR 05-08 00:16:26 [core.py:396]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-08 00:16:26 [core.py:396]   File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/torch/utils/_device.py\", line 104, in __torch_function__\n",
      "ERROR 05-08 00:16:26 [core.py:396]     return func(*args, **kwargs)\n",
      "ERROR 05-08 00:16:26 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-08 00:16:26 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacity of 79.26 GiB of which 410.62 MiB is free. Including non-PyTorch memory, this process has 78.80 GiB memory in use. Of the allocated memory 78.31 GiB is allocated by PyTorch, and 17.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process EngineCore_0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 400, in run_engine_core\n",
      "    raise e\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\n",
      "    engine_core = EngineCoreProc(*args, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\n",
      "    super().__init__(vllm_config, executor_class, log_stats,\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 64, in __init__\n",
      "    self.model_executor = executor_class(vllm_config)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 52, in __init__\n",
      "    self._init_executor()\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "    self.collective_rpc(\"load_model\")\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "    answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/utils.py\", line 2456, in run_method\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 162, in load_model\n",
      "    self.model_runner.load_model()\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1332, in load_model\n",
      "    self.model = get_model(vllm_config=self.vllm_config)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n",
      "    return loader.load_model(vllm_config=vllm_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py\", line 452, in load_model\n",
      "    model = _initialize_model(vllm_config=vllm_config)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py\", line 133, in _initialize_model\n",
      "    return model_class(vllm_config=vllm_config, prefix=prefix)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 496, in __init__\n",
      "    self.model = self._init_model(vllm_config=vllm_config,\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 542, in _init_model\n",
      "    return LlamaModel(vllm_config=vllm_config,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/compilation/decorators.py\", line 151, in __init__\n",
      "    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 321, in __init__\n",
      "    self.start_layer, self.end_layer, self.layers = make_layers(\n",
      "                                                    ^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/utils.py\", line 610, in make_layers\n",
      "    maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 323, in <lambda>\n",
      "    lambda prefix: layer_type(config=config,\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 254, in __init__\n",
      "    self.mlp = LlamaMLP(\n",
      "               ^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 77, in __init__\n",
      "    self.down_proj = RowParallelLinear(\n",
      "                     ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py\", line 1194, in __init__\n",
      "    self.quant_method.create_weights(\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py\", line 189, in create_weights\n",
      "    weight = Parameter(torch.empty(sum(output_partition_sizes),\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marwa/env_notebooks/lib/python3.12/site-packages/torch/utils/_device.py\", line 104, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacity of 79.26 GiB of which 410.62 MiB is free. Including non-PyTorch memory, this process has 78.80 GiB memory in use. Of the allocated memory 78.31 GiB is allocated by PyTorch, and 17.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[rank0]:[W508 00:16:27.480026051 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     16\u001b[39m reset_stats()\n\u001b[32m     17\u001b[39m conversation = generate_conversation(\n\u001b[32m     18\u001b[39m     config_llm,\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     pturn=\u001b[32m1\u001b[39m\n\u001b[32m     24\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m conversation_eval = \u001b[43meval_prompt_consistency\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboth_agents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(conversation_eval)\n\u001b[32m     28\u001b[39m conversations.append(conversation_eval)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36meval_prompt_consistency\u001b[39m\u001b[34m(conv_dict, both_agents)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.get(\u001b[33m'\u001b[39m\u001b[33mverbose\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     32\u001b[39m     \u001b[38;5;28mprint\u001b[39m(prompt)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m output = \u001b[43mcompletion_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43meval_model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m conv_dict[\u001b[33m'\u001b[39m\u001b[33meval_prompt_consistency\u001b[39m\u001b[33m'\u001b[39m].append((line_number, output))\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mNO\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m output:  \u001b[38;5;66;03m# no contradiction\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/consistency_LLMs/utils.py:223\u001b[39m, in \u001b[36mcompletion_create\u001b[39m\u001b[34m(model_name, config, prompt, keep_trying)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompletion_create\u001b[39m(model_name, config, prompt, keep_trying=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompletion_create_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (openai.APIError, openai.OpenAIError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    225\u001b[39m         \u001b[38;5;66;03m# print(\"ERROR\", e)\u001b[39;00m\n\u001b[32m    226\u001b[39m         \u001b[38;5;66;03m# print(\"sleeping for 10 seconds.\")\u001b[39;00m\n\u001b[32m    227\u001b[39m         time.sleep(\u001b[32m10\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/consistency_LLMs/utils.py:159\u001b[39m, in \u001b[36mcompletion_create_helper\u001b[39m\u001b[34m(model_name, config, prompt)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompletion_create_helper\u001b[39m(model_name, config, prompt):\n\u001b[32m    152\u001b[39m     \u001b[38;5;66;03m# # limit prompt in all cases\u001b[39;00m\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# if model_name not in vllm_alias:\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m#     # for some reason vLLM models simply repeat this last statement if present\u001b[39;00m\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m#     prompt += \" Limit your answer to three sentences or less!\"\u001b[39;00m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m vllm_alias \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m llm:\n\u001b[32m    158\u001b[39m         \u001b[38;5;66;03m# set up vllm if not already set up\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m         \u001b[43msetup_vllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m     ret = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;66;03m# return the output ret at the end and use to calculate cost\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_name == \u001b[33m\"\u001b[39m\u001b[33mgpt-3.5-turbo-instruct\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/consistency_LLMs/utils.py:132\u001b[39m, in \u001b[36msetup_vllm\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m    130\u001b[39m     llm = LLM(model=vllm_alias[model], tensor_parallel_size=config[\u001b[33m'\u001b[39m\u001b[33mgpus\u001b[39m\u001b[33m'\u001b[39m], download_dir=config[\u001b[33m'\u001b[39m\u001b[33mmodel_dir\u001b[39m\u001b[33m'\u001b[39m], gpu_memory_utilization=\u001b[32m0.75\u001b[39m)\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m vllm_alias[model] == \u001b[33m'\u001b[39m\u001b[33mmeta-llama/Meta-Llama-3.1-70B\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m vllm_alias[model] == \u001b[33m'\u001b[39m\u001b[33mmeta-llama/Meta-Llama-3.1-70B-Instruct\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_alias\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgpus\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_dir\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m12880\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    134\u001b[39m     llm = LLM(model=vllm_alias[model], tensor_parallel_size=config[\u001b[33m'\u001b[39m\u001b[33mgpus\u001b[39m\u001b[33m'\u001b[39m], download_dir=config[\u001b[33m'\u001b[39m\u001b[33mmodel_dir\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/utils.py:1161\u001b[39m, in \u001b[36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1154\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1156\u001b[39m         warnings.warn(\n\u001b[32m   1157\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1158\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1159\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/entrypoints/llm.py:247\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[39m\n\u001b[32m    217\u001b[39m engine_args = EngineArgs(\n\u001b[32m    218\u001b[39m     model=model,\n\u001b[32m    219\u001b[39m     task=task,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m     **kwargs,\n\u001b[32m    244\u001b[39m )\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    251\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/engine/llm_engine.py:510\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    507\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv1\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[32m    508\u001b[39m     engine_cls = V1LLMEngine\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:112\u001b[39m, in \u001b[36mLLMEngine.from_vllm_config\u001b[39m\u001b[34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_vllm_config\u001b[39m(\n\u001b[32m    106\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    110\u001b[39m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    111\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLLMEngine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m               \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mExecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m               \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m               \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m               \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m               \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVLLM_ENABLE_V1_MULTIPROCESSING\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:92\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mself\u001b[39m.output_processor = OutputProcessor(\u001b[38;5;28mself\u001b[39m.tokenizer,\n\u001b[32m     89\u001b[39m                                         log_stats=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# FIXME: implement\u001b[39;49;00m\n\u001b[32m     98\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multiprocess_mode:\n\u001b[32m    101\u001b[39m     \u001b[38;5;66;03m# for v0 compatibility\u001b[39;00m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_executor = \u001b[38;5;28mself\u001b[39m.engine_core.engine_core.model_executor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:73\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m AsyncMPClient(vllm_config, executor_class, log_stats)\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:494\u001b[39m, in \u001b[36mSyncMPClient.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[32m    493\u001b[39m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m     \u001b[38;5;28mself\u001b[39m.outputs_queue = queue.Queue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n\u001b[32m    503\u001b[39m     \u001b[38;5;66;03m# Ensure that the outputs socket processing thread does not have\u001b[39;00m\n\u001b[32m    504\u001b[39m     \u001b[38;5;66;03m# a ref to the client which prevents gc.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:398\u001b[39m, in \u001b[36mMPClient.__init__\u001b[39m\u001b[34m(self, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    394\u001b[39m \u001b[38;5;28mself\u001b[39m._init_core_engines(vllm_config, new_core_engine,\n\u001b[32m    395\u001b[39m                         \u001b[38;5;28mself\u001b[39m.resources.core_engines)\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# Wait for engine core process(es) to start.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[38;5;28mself\u001b[39m.utility_results: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, AnyFuture] = {}\n\u001b[32m    402\u001b[39m \u001b[38;5;66;03m# Request objects which may contain pytorch-allocated tensors\u001b[39;00m\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# that we need to keep references to until zmq is done with the\u001b[39;00m\n\u001b[32m    404\u001b[39m \u001b[38;5;66;03m# underlying data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_notebooks/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:430\u001b[39m, in \u001b[36mMPClient._wait_for_engine_startup\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    427\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(events) > \u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m events[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m] != sync_input_socket:\n\u001b[32m    429\u001b[39m     \u001b[38;5;66;03m# One of the core processes exited.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEngine core initialization failed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    431\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mSee root cause above.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    433\u001b[39m eng_id_bytes, msg = sync_input_socket.recv_multipart()\n\u001b[32m    434\u001b[39m eng_id = \u001b[38;5;28mint\u001b[39m.from_bytes(eng_id_bytes, byteorder=\u001b[33m\"\u001b[39m\u001b[33mlittle\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Engine core initialization failed. See root cause above."
     ]
    }
   ],
   "source": [
    "prompts = config_role\n",
    "config = config_llm\n",
    "eval_prompts = eval_prompts\n",
    "index_offset = load_stats_file(write_file)\n",
    "conversations = []    \n",
    "# lengths = [10, 20, 40, 60]\n",
    "lengths = [10]\n",
    "count = 0 \n",
    "for i in range(1):\n",
    "    for patient_dict in personas_therapy:\n",
    "        count+=1\n",
    "        print(count)\n",
    "        background = patient_dict[\"persona\"]\n",
    "        for convo_length in lengths:\n",
    "            config_llm['convo_length_limit'] = convo_length\n",
    "            reset_stats()\n",
    "            conversation = generate_conversation(\n",
    "                config_llm,\n",
    "                \"\", \n",
    "                background,\n",
    "                \"Therapist\", \n",
    "                \"Patient\",\n",
    "                pturn=1\n",
    "            )\n",
    "            conversation_eval = eval_prompt_consistency(conversation, both_agents=False)\n",
    "\n",
    "            print(conversation_eval)\n",
    "            conversations.append(conversation_eval)\n",
    "            stats['index'] = index_offset\n",
    "            stats['timestamp'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            write_stats(write_file, conversation_eval)\n",
    "            index_offset += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "avg = 0 \n",
    "for conversation in conversations: \n",
    "    if conversation[\"P2_prompt_consistency_score\"] <= 0.7:\n",
    "        count+=1\n",
    "    print(conversation[\"P2_prompt_consistency_score\"])\n",
    "    avg+=conversation[\"P2_prompt_consistency_score\"]\n",
    "print(count)\n",
    "print(avg)\n",
    "print(avg/len(conversations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
