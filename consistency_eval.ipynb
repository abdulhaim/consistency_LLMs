{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ef1b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3,4\n",
      "env: TMPDIR=/raid/users/ryan_cheng/tmp\n",
      "INFO 04-28 17:55:28 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=3,4\n",
    "%env TMPDIR=/raid/users/ryan_cheng/tmp\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "from absl import app, flags\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from utils import *\n",
    "import utils\n",
    "from consistency_eval import *\n",
    "from education_generation import *\n",
    "\n",
    "try:\n",
    "    from vllm import LLM, SamplingParams\n",
    "    import ray\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d588ba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.abspath('../ryan_openai.txt'), 'r') as f:\n",
    "    utils.client = OpenAI(api_key=f.read().rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a871b942",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/nfs/kun2/users/ryan_cheng/consistency_LLMs/therapy/exp/04.22.25/Llama-3.1-8B-Instruct_0_500.json'\n",
    "\n",
    "with open(\"/nfs/kun2/users/ryan_cheng/consistency_LLMs/therapy/config_therapy.json\", 'r') as f:\n",
    "    config_therapy = json.load(f)\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(\"./config/education/gpt-4o-mini.json\", 'r') as f:\n",
    "    config_gpt4_mini = json.load(f)\n",
    "\n",
    "for key, value in config_gpt4_mini.items():\n",
    "    config[key] = value\n",
    "\n",
    "for key, value in config_therapy.items():\n",
    "    prompts[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc7878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a930a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/nfs/kun2/users/ryan_cheng/consistency_LLMs/config/eval_prompts.json\" , 'r') as f:\n",
    "    eval_prompts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4010240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['eval_model'] = 'Llama-3.1-8B-Instruct'\n",
    "config['gpus'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "776d51bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['eval_model'] = 'Llama-3.1-70B-Instruct'\n",
    "config['gpus'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4580970",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['eval_model'] = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "156dc910",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['task_name'] = 'Therapy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cf56271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'strategy_consistency': \"%SCENARIO_DESC% Evaluate whether the following line spoken by %SPEAKER_ROLE% contradicts their background. First, describe the interpreted intention of the statement, and whether or not it aligns with the given background of %SPEAKER_ROLE%. Then, provide 1 sentence explaining your reasoning based on the strategy, followed by YES if the line contradicts the given background of %SPEAKER_ROLE%, and NO if it is consistent with the provided background of %SPEAKER_ROLE%. %SPEAKER_ROLE%'s strategy is described as follows:\\n%SPEAKER_STRATEGY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide 1 sentence of reasoning followed by YES or NO.\\n\\n\",\n",
       " 'background_consistency': \"%SCENARIO_DESC% Evaluate whether the following line spoken by %SPEAKER_ROLE% contradicts their background. Provide 1 sentence explaining your reasoning based on the background, followed by YES if the line contradicts the given background of %SPEAKER_ROLE%, and NO if it is consistent with the provided background of %SPEAKER_ROLE%. %SPEAKER_ROLE%'s background is described as follows:\\n%SPEAKER_BACKSTORY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide 1 sentence of reason reasoning followed by YES or NO.\\n\\n\",\n",
       " 'combined_prompt_consistency': \"%SCENARIO_DESC% Evaluate the intention behind the following line spoken by %SPEAKER_ROLE% and determine whether it contradicts their background. First, describe the interpreted intention of the statement, and whether or not it aligns with the given background of %SPEAKER_ROLE%. Then, answer YES if the line contradicts the given background of %SPEAKER_ROLE% or the intention does not align with the provided background, and answer NO if it does align with the provided background or the intention aligns with the background of %SPEAKER_ROLE%. %SPEAKER_ROLE%'s background is described as follows:\\n%SPEAKER_BACKSTORY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide your answer as 1 sentence explaining your reasoning based on the background and the interpreted intention, followed by YES or NO.\\n\\n\",\n",
       " 'forwards_combined_prompt_consistency': \"%SCENARIO_DESC% Evaluate the intention behind the following line spoken by %SPEAKER_ROLE% and determine whether it contradicts their background. Answer YES if the line contradicts the given background of %SPEAKER_ROLE% or the intention does not align with the provided background, and answer NO if it does align with the provided background or the intention aligns with the background of %SPEAKER_ROLE%, then describe the interpreted intention of the statement and whether or not it aligns with the given background of %SPEAKER_ROLE% within 1 sentence. %SPEAKER_ROLE%'s background is described as follows:\\n%SPEAKER_BACKSTORY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide your answer as YES or NO first, followed by 1 sentence explaining your reasoning based on the background and the interpreted intention.\\n\\n\",\n",
       " 'index_consistency': '%SCENARIO_DESC% For the following line spoken by %SPEAKER_ROLE%, first determine if there is a CLEAR conflict or inconsistency between the line and any line within the conversation history spoken by %SPEAKER_ROLE%. IF there is a conflict, provide a sentence of reasoning followed by a list of indices of lines in the conversation history that have a clear conflict with the current line. Otherwise, provide a sentence of reasoning followed by an empty list. ONLY INCLUDE INDICES OF LINES THAT CORRESPOND TO %SPEAKER_ROLE%. The conversation up to this point is as follows: %CONVERSATION%. %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide your reasoning as 1 sentence, followed by a list of indices of conflicting lines from the conversation history formatted like a Python list in the following format: [index1, index2, index3, ...].\\n\\n',\n",
       " 'index_consistency_background': \"%SCENARIO_DESC% For the following line spoken by %SPEAKER_ROLE%, first determine if there is a CLEAR conflict or inconsistency between the line and any line within the conversation history spoken by %SPEAKER_ROLE%. IF there is a conflict, provide a sentence of reasoning followed by a list of indices of lines in the conversation history that have a clear conflict with the current line. Otherwise, provide a sentence of reasoning followed by an empty list. ONLY INCLUDE INDICES OF LINES THAT CORRESPOND TO %SPEAKER_ROLE%. The conversation up to this point is as follows: %CONVERSATION%. %SPEAKER_ROLE%'s background is described as follows:\\n%SPEAKER_BACKSTORY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide your reasoning as 1 sentence, followed by a list of indices of conflicting lines from the conversation history formatted like a Python list in the following format: [index1, index2, index3, ...].\\n\\n\",\n",
       " 'pairwise_consistency': '%SCENARIO_DESC% For the following line spoken by %SPEAKER_ROLE%, answer YES if the line directly contradicts the provided line spoken by %LISTENER_ROLE%, and answer NO if the line does not contradict the provided line spoken by %LISTENER_ROLE%. %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n %LISTENER_ROLE% spoke the following line: \\n%LISTENER_LINE%\\n\\n Answer YES if the line spoken by %SPEAKER_ROLE% contradicts the provided line spoken by %LISTENER_ROLE%, and answer NO if the line does not contradict the provided line spoken by %LISTENER_ROLE%, followed by 1 sentence of reasoning.\\n\\n',\n",
       " 'backstory_test': \"Based on the following background, generate a new fact-based multiple choice question with 5 choices addressed directly IN SECOND PERSON, along with its correct answer. Preface the question with 'Question:' and the answer with 'Answer:'.\\n%SPEAKER_BACKSTORY%\\n%PREVIOUS_QUESTIONS%\",\n",
       " 'answer_backstory': 'You are %SPEAKER_ROLE%, and you are having a conversation with %LISTENER_ROLE%. Your background is:\\n%SPEAKER_BACKSTORY%\\n So far, the conversation is as below:\\n%CONVERSATION%\\n\\n Based on your conversation above so far, answer the following multiple choice question.\\n%BACKSTORY_QUESTION%\\n',\n",
       " 'grade_backstory': 'As part of grading a test, determine whether the given answer %GIVEN_ANSWER% matches the following correct answer. Respond with either YES or NO.\\nCorrect Answer: %CORRECT_ANSWER%\\n'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5905f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list(text):\n",
    "    pattern = r'\\[.*?\\]'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        try:\n",
    "            return eval(match.group())\n",
    "        except (SyntaxError, NameError):\n",
    "            return []\n",
    "    return[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89c417e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts['eval_prompts'] = eval_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40cd7eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0615e0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"%SCENARIO_DESC% For the following line spoken by %SPEAKER_ROLE%, first determine if there is a CLEAR conflict or inconsistency between the line and any line within the conversation history spoken by %SPEAKER_ROLE%. IF there is a conflict, provide a sentence of reasoning followed by a list of indices of lines in the conversation history that have a clear conflict with the current line. Otherwise, provide a sentence of reasoning followed by an empty list. ONLY INCLUDE INDICES OF LINES THAT CORRESPOND TO %SPEAKER_ROLE%. The conversation up to this point is as follows: %CONVERSATION%. %SPEAKER_ROLE%'s background is described as follows:\\n%SPEAKER_BACKSTORY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide your reasoning as 1 sentence, followed by a list of indices of conflicting lines from the conversation history formatted like a Python list in the following format: [index1, index2, index3, ...].\\n\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[\"eval_prompts\"][\"index_consistency_background\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a61d2022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_conversation(conversation):\n",
    "    return \"\".join([str(i) + \": \" + line for i, line in conversation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "786f231d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Patient: I'm just here for some advice on how to manage my workload, I guess. Nothing too deep, just some practical tips to get me through the next few months.\n",
      "1: Therapist: I appreciate your desire to start with a more practical approach, but I'm curious, is there something specific that's driving your desire to manage your workload right now, or is there another layer to your concerns that you're not sharing?\n",
      "2: Patient: I don't think there's anything specific driving my desire to manage my workload, I just feel like I'm falling behind and I want to get everything under control. Can we just focus on the strategies I can use to prioritize tasks and manage my time better?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(format_conversation(data[0][\"conversation\"][:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7878b833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_index_consistency(conv_dict, both_agents=False):\n",
    "    conv_dict['eval_index_consistency'] = []\n",
    "    conv_dict['P1_index_consistency_score'] = 0\n",
    "    if both_agents:\n",
    "        conv_dict['P2_index_consistency_score'] = 0\n",
    "    p1_utterances = 0\n",
    "    p2_utterances = 0\n",
    "    pturn = conv_dict[\"pturn\"]\n",
    "    for i, line in conv_dict[\"conversation\"]:\n",
    "        if i < 2: # skip first 2 lines of dialogue\n",
    "            continue \n",
    "        if pturn == 1:\n",
    "            prompt = prompts[\"eval_prompts\"][\"index_consistency\"].replace(\"%SCENARIO_DESC%\", prompts[\"scenario\"]) \\\n",
    "                                                                 .replace(\"%SPEAKER_ROLE%\", prompts[\"agent1_role\"]) \\\n",
    "                                                                 .replace(\"%CONVERSATION%\", format_conversation(conv_dict[\"conversation\"][:i])) \\\n",
    "                                                                 .replace(\"%SPEAKER_LINE%\", line)\n",
    "            if config['verbose']:\n",
    "                print(prompt)\n",
    "            output = completion_create(config['eval_model'], config, prompt)\n",
    "            index_list = extract_list(output)\n",
    "            conv_dict['eval_index_consistency'].append((i, output))\n",
    "            conv_dict['P1_index_consistency_score'] += len(index_list)\n",
    "            p1_utterances += i // 2\n",
    "            pturn = 2\n",
    "        elif pturn == 2:\n",
    "            if both_agents:\n",
    "                prompt = prompts[\"eval_prompts\"][\"index_consistency\"].replace(\"%SCENARIO_DESC%\", prompts[\"scenario\"]) \\\n",
    "                                                                     .replace(\"%SPEAKER_ROLE%\", prompts[\"agent2_role\"]) \\\n",
    "                                                                     .replace(\"%CONVERSATION%\", format_conversation(conv_dict[\"conversation\"][:i])) \\\n",
    "                                                                     .replace(\"%SPEAKER_LINE%\", line)\n",
    "                if config['verbose']:\n",
    "                    print(prompt)\n",
    "                output = completion_create(config['eval_model'], config, prompt)\n",
    "                index_list = extract_list(output)\n",
    "                conv_dict['eval_index_consistency'].append((i, output))\n",
    "                conv_dict['P2_index_consistency_score'] += len(index_list)\n",
    "                p2_utterances += i // 2\n",
    "            pturn = 1\n",
    "\n",
    "    if p1_utterances > 0:\n",
    "        conv_dict['P1_index_consistency_score'] /= p1_utterances\n",
    "        conv_dict['P1_index_consistency_score'] = 1 - conv_dict['P1_index_consistency_score']\n",
    "    if p2_utterances > 0 and both_agents:\n",
    "        conv_dict['P2_index_consistency_score'] /= p2_utterances\n",
    "        conv_dict['P2_index_consistency_score'] = 1 - conv_dict['P2_index_consistency_score']\n",
    "\n",
    "    return conv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0986c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_index_background_consistency(conv_dict, both_agents=False):\n",
    "    conv_dict['eval_index_consistency'] = []\n",
    "    conv_dict['P1_index_consistency_score'] = 0\n",
    "    if both_agents:\n",
    "        conv_dict['P2_index_consistency_score'] = 0\n",
    "    p1_utterances = 0\n",
    "    p2_utterances = 0\n",
    "    pturn = conv_dict[\"pturn\"]\n",
    "    for i, line in conv_dict[\"conversation\"]:\n",
    "        if i < 2: # skip first 2 lines of dialogue\n",
    "            continue \n",
    "        if pturn == 1:\n",
    "            prompt = prompts[\"eval_prompts\"][\"index_consistency_background\"].replace(\"%SCENARIO_DESC%\", prompts[\"scenario\"]) \\\n",
    "                                                                 .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P1\"]) \\\n",
    "                                                                 .replace(\"%SPEAKER_ROLE%\", prompts[\"agent1_role\"]) \\\n",
    "                                                                 .replace(\"%CONVERSATION%\", format_conversation(conv_dict[\"conversation\"][:i])) \\\n",
    "                                                                 .replace(\"%SPEAKER_LINE%\", line)\n",
    "            if config['verbose']:\n",
    "                print(prompt)\n",
    "            output = completion_create(config['eval_model'], config, prompt)\n",
    "            index_list = extract_list(output)\n",
    "            conv_dict['eval_index_consistency'].append((i, output))\n",
    "            conv_dict['P1_index_consistency_score'] += len(index_list)\n",
    "            p1_utterances += i // 2\n",
    "            pturn = 2\n",
    "        elif pturn == 2:\n",
    "            if both_agents:\n",
    "                prompt = prompts[\"eval_prompts\"][\"index_consistency_background\"].replace(\"%SCENARIO_DESC%\", prompts[\"scenario\"]) \\\n",
    "                                                                     .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P2\"]) \\\n",
    "                                                                     .replace(\"%SPEAKER_ROLE%\", prompts[\"agent2_role\"]) \\\n",
    "                                                                     .replace(\"%CONVERSATION%\", format_conversation(conv_dict[\"conversation\"][:i])) \\\n",
    "                                                                     .replace(\"%SPEAKER_LINE%\", line)\n",
    "                if config['verbose']:\n",
    "                    print(prompt)\n",
    "                output = completion_create(config['eval_model'], config, prompt)\n",
    "                index_list = extract_list(output)\n",
    "                conv_dict['eval_index_consistency'].append((i, output))\n",
    "                conv_dict['P2_index_consistency_score'] += len(index_list)\n",
    "                p2_utterances += i // 2\n",
    "            pturn = 1\n",
    "\n",
    "    if p1_utterances > 0:\n",
    "        conv_dict['P1_index_consistency_score'] /= p1_utterances\n",
    "        conv_dict['P1_index_consistency_score'] = 1 - conv_dict['P1_index_consistency_score']\n",
    "    if p2_utterances > 0 and both_agents:\n",
    "        conv_dict['P2_index_consistency_score'] /= p2_utterances\n",
    "        conv_dict['P2_index_consistency_score'] = 1 - conv_dict['P2_index_consistency_score']\n",
    "\n",
    "    return conv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59104e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prompt_consistency(conv_dict):\n",
    "    conv_dict['eval_prompt_consistency'] = {}\n",
    "    conv_dict['P1_prompt_consistency_scores'] = {}\n",
    "    p1_utterances = {}\n",
    "    \n",
    "    for key in [\"strategy_consistency\", \"background_consistency\", \"combined_prompt_consistency\"]:\n",
    "        conv_dict['eval_prompt_consistency'][key] = []\n",
    "        conv_dict['P1_prompt_consistency_scores'][key] = 0\n",
    "        p1_utterances[key] = 0\n",
    "\n",
    "    pturn = conv_dict[\"pturn\"]\n",
    "    for line in conv_dict[\"conversation\"]:\n",
    "        line_number = line[0]\n",
    "        convo_line = line[1]\n",
    "        if pturn == 1:\n",
    "            for key in [\"strategy_consistency\", \"background_consistency\", \"combined_prompt_consistency\"]:\n",
    "                prompt = eval_prompts[key].replace(\"%SCENARIO_DESC\", 'There is a Patient in conversation with a Therapist.') \\\n",
    "                                          .replace(\"%SPEAKER_ROLE%\", config_therapy[\"agent1_role\"]) \\\n",
    "                                          .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P1\"]) \\\n",
    "                                          .replace(\"%SPEAKER_LINE%\", convo_line)\n",
    "                if config.get('verbose', False):\n",
    "                    print(prompt)\n",
    "                output = completion_create(config['eval_model'], config, prompt)\n",
    "                conv_dict['eval_prompt_consistency'][key].append((line_number, output))\n",
    "                if \"YES\" not in output:  # no contradiction\n",
    "                    conv_dict['P1_prompt_consistency_scores'][key] += 1\n",
    "                p1_utterances[key] += 1\n",
    "            pturn = 2\n",
    "        elif pturn == 2:\n",
    "            pturn = 1\n",
    "\n",
    "    for key in [\"strategy_consistency\", \"background_consistency\", \"combined_prompt_consistency\"]:\n",
    "        if p1_utterances[key] > 0:\n",
    "            conv_dict['P1_prompt_consistency_scores'][key] /= p1_utterances[key]\n",
    "\n",
    "    print(conv_dict)\n",
    "    return conv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b51381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prompt_consistency_ablations(conv_dict):\n",
    "    \n",
    "    p1_utterances = {}\n",
    "    keys = [\"combined_prompt_consistency\", \"forwards_combined_prompt_consistency\"]\n",
    "    if \"P1_prompt_consistency_scores\" not in conv_dict:\n",
    "        conv_dict['eval_prompt_consistency'] = {}\n",
    "        conv_dict['P1_prompt_consistency_scores'] = {}\n",
    "        index = 0\n",
    "        for key in keys:\n",
    "            conv_dict['P1_prompt_consistency_scores'][key] = {}\n",
    "            conv_dict['eval_prompt_consistency'][key] = []\n",
    "    else:\n",
    "        index = len(conv_dict['eval_prompt_consistency'][keys[0]][0]) - 1\n",
    "        print(index)\n",
    "    \n",
    "    for key in keys:\n",
    "        conv_dict['P1_prompt_consistency_scores'][key][index] = 0\n",
    "        p1_utterances[key] = 0\n",
    "\n",
    "\n",
    "    pturn = conv_dict[\"pturn\"]\n",
    "    for line in conv_dict[\"conversation\"]:\n",
    "        line_number = line[0]\n",
    "        convo_line = line[1]\n",
    "        if pturn == 1:\n",
    "            for key in keys:\n",
    "                prompt = eval_prompts[key].replace(\"%SCENARIO_DESC\", 'There is a Patient in conversation with a Therapist.') \\\n",
    "                                          .replace(\"%SPEAKER_ROLE%\", config_therapy[\"agent1_role\"]) \\\n",
    "                                          .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P1\"]) \\\n",
    "                                          .replace(\"%SPEAKER_LINE%\", convo_line)\n",
    "                if config.get('verbose', False):\n",
    "                    print(prompt)\n",
    "                output = completion_create(config['eval_model'], config, prompt)\n",
    "                if index == 0:\n",
    "                    conv_dict['eval_prompt_consistency'][key].append([line_number, output])\n",
    "                else:\n",
    "                    conv_dict['eval_prompt_consistency'][key][line_number // 2].append(output)\n",
    "\n",
    "                if \"YES\" not in output:  # no contradiction\n",
    "                    conv_dict['P1_prompt_consistency_scores'][key][index] += 1\n",
    "                p1_utterances[key] += 1\n",
    "            pturn = 2\n",
    "        elif pturn == 2:\n",
    "            pturn = 1\n",
    "\n",
    "    for key in keys:\n",
    "        if p1_utterances[key] > 0:\n",
    "            conv_dict['P1_prompt_consistency_scores'][key][index] /= p1_utterances[key]\n",
    "\n",
    "    print(conv_dict)\n",
    "    return conv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41725559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama-3.1-70B-Instruct'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['eval_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5151e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'strategy_consistency': \"%SCENARIO_DESC% Evaluate whether the following line spoken by %SPEAKER_ROLE% contradicts their background. First, describe the interpreted intention of the statement, and whether or not it aligns with the given background of %SPEAKER_ROLE%. Then, provide 1 sentence explaining your reasoning based on the strategy, followed by YES if the line contradicts the given background of %SPEAKER_ROLE%, and NO if it is consistent with the provided background of %SPEAKER_ROLE%. %SPEAKER_ROLE%'s strategy is described as follows:\\n%SPEAKER_STRATEGY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide 1 sentence of reasoning followed by YES or NO.\\n\\n\",\n",
       " 'background_consistency': \"%SCENARIO_DESC% Evaluate whether the following line spoken by %SPEAKER_ROLE% contradicts their background. Provide 1 sentence explaining your reasoning based on the background, followed by YES if the line contradicts the given background of %SPEAKER_ROLE%, and NO if it is consistent with the provided background of %SPEAKER_ROLE%. %SPEAKER_ROLE%'s background is described as follows:\\n%SPEAKER_BACKSTORY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide 1 sentence of reason reasoning followed by YES or NO.\\n\\n\",\n",
       " 'combined_prompt_consistency': \"%SCENARIO_DESC% Evaluate the intention behind the following line spoken by %SPEAKER_ROLE% and determine whether it contradicts their background. First, describe the interpreted intention of the statement, and whether or not it aligns with the given background of %SPEAKER_ROLE%. Then, answer YES if the line contradicts the given background of %SPEAKER_ROLE% or the intention does not align with the provided background, and answer NO if it does align with the provided background or the intention aligns with the background of %SPEAKER_ROLE%. %SPEAKER_ROLE%'s background is described as follows:\\n%SPEAKER_BACKSTORY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide your answer as 1 sentence explaining your reasoning based on the background and the interpreted intention, followed by YES or NO.\\n\\n\",\n",
       " 'forwards_combined_prompt_consistency': \"%SCENARIO_DESC% Evaluate the intention behind the following line spoken by %SPEAKER_ROLE% and determine whether it contradicts their background. Answer YES if the line contradicts the given background of %SPEAKER_ROLE% or the intention does not align with the provided background, and answer NO if it does align with the provided background or the intention aligns with the background of %SPEAKER_ROLE%, then describe the interpreted intention of the statement and whether or not it aligns with the given background of %SPEAKER_ROLE% within 1 sentence. %SPEAKER_ROLE%'s background is described as follows:\\n%SPEAKER_BACKSTORY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide your answer as YES or NO first, followed by 1 sentence explaining your reasoning based on the background and the interpreted intention.\\n\\n\",\n",
       " 'index_consistency': '%SCENARIO_DESC% For the following line spoken by %SPEAKER_ROLE%, first determine if there is a CLEAR conflict or inconsistency between the line and any line within the conversation history spoken by %SPEAKER_ROLE%. IF there is a conflict, provide a sentence of reasoning followed by a list of indices of lines in the conversation history that have a clear conflict with the current line. Otherwise, provide a sentence of reasoning followed by an empty list. ONLY INCLUDE INDICES OF LINES THAT CORRESPOND TO %SPEAKER_ROLE%. The conversation up to this point is as follows: %CONVERSATION%. %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide your reasoning as 1 sentence, followed by a list of indices of conflicting lines from the conversation history formatted like a Python list in the following format: [index1, index2, index3, ...].\\n\\n',\n",
       " 'index_consistency_background': \"%SCENARIO_DESC% For the following line spoken by %SPEAKER_ROLE%, first determine if there is a CLEAR conflict or inconsistency between the line and any line within the conversation history spoken by %SPEAKER_ROLE%. IF there is a conflict, provide a sentence of reasoning followed by a list of indices of lines in the conversation history that have a clear conflict with the current line. Otherwise, provide a sentence of reasoning followed by an empty list. ONLY INCLUDE INDICES OF LINES THAT CORRESPOND TO %SPEAKER_ROLE%. The conversation up to this point is as follows: %CONVERSATION%. %SPEAKER_ROLE%'s background is described as follows:\\n%SPEAKER_BACKSTORY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide your reasoning as 1 sentence, followed by a list of indices of conflicting lines from the conversation history formatted like a Python list in the following format: [index1, index2, index3, ...].\\n\\n\",\n",
       " 'pairwise_consistency': '%SCENARIO_DESC% For the following line spoken by %SPEAKER_ROLE%, answer YES if the line directly contradicts the provided line spoken by %LISTENER_ROLE%, and answer NO if the line does not contradict the provided line spoken by %LISTENER_ROLE%. %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n %LISTENER_ROLE% spoke the following line: \\n%LISTENER_LINE%\\n\\n Answer YES if the line spoken by %SPEAKER_ROLE% contradicts the provided line spoken by %LISTENER_ROLE%, and answer NO if the line does not contradict the provided line spoken by %LISTENER_ROLE%, followed by 1 sentence of reasoning.\\n\\n',\n",
       " 'backstory_test': \"Based on the following background, generate a new fact-based multiple choice question with 5 choices addressed directly IN SECOND PERSON, along with its correct answer. Preface the question with 'Question:' and the answer with 'Answer:'.\\n%SPEAKER_BACKSTORY%\\n%PREVIOUS_QUESTIONS%\",\n",
       " 'answer_backstory': 'You are %SPEAKER_ROLE%, and you are having a conversation with %LISTENER_ROLE%. Your background is:\\n%SPEAKER_BACKSTORY%\\n So far, the conversation is as below:\\n%CONVERSATION%\\n\\n Based on your conversation above so far, answer the following multiple choice question.\\n%BACKSTORY_QUESTION%\\n',\n",
       " 'grade_backstory': 'As part of grading a test, determine whether the given answer %GIVEN_ANSWER% matches the following correct answer. Respond with either YES or NO.\\nCorrect Answer: %CORRECT_ANSWER%\\n'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e090b832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 17:55:42,252\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-28 17:55:59 config.py:542] This model supports multiple tasks: {'embed', 'classify', 'score', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 04-28 17:55:59 config.py:1401] Defaulting to use mp for distributed inference\n",
      "INFO 04-28 17:55:59 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='meta-llama/Meta-Llama-3.1-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=12880, download_dir='/raid/users/ryan_cheng/models/', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-70B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 04-28 17:56:00 multiproc_worker_utils.py:300] Reducing Torch parallelism from 112 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 04-28 17:56:00 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1878170)\u001b[0;0m INFO 04-28 17:56:00 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "INFO 04-28 17:56:04 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1878170)\u001b[0;0m INFO 04-28 17:56:04 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 04-28 17:56:06 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 04-28 17:56:06 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1878170)\u001b[0;0m INFO 04-28 17:56:06 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1878170)\u001b[0;0m INFO 04-28 17:56:06 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 04-28 17:56:08 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/ryan_cheng/.cache/vllm/gpu_p2p_access_cache_for_3,4.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1878170)\u001b[0;0m INFO 04-28 17:56:08 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/ryan_cheng/.cache/vllm/gpu_p2p_access_cache_for_3,4.json\n",
      "INFO 04-28 17:56:08 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_75d93800'), local_subscribe_port=46067, remote_subscribe_port=None)\n",
      "INFO 04-28 17:56:08 model_runner.py:1110] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1878170)\u001b[0;0m INFO 04-28 17:56:08 model_runner.py:1110] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 04-28 17:56:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1878170)\u001b[0;0m INFO 04-28 17:56:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4312e0b770e14a86851a0f9b9191ad26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/30 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-28 17:56:31 model_runner.py:1115] Loading model weights took 65.7409 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1878170)\u001b[0;0m INFO 04-28 17:56:31 model_runner.py:1115] Loading model weights took 65.7409 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1878170)\u001b[0;0m INFO 04-28 17:56:35 worker.py:267] Memory profiling takes 4.24 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1878170)\u001b[0;0m INFO 04-28 17:56:35 worker.py:267] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.90) = 125.75GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1878170)\u001b[0;0m INFO 04-28 17:56:35 worker.py:267] model weights take 65.74GiB; non_torch_memory takes 1.56GiB; PyTorch activation peak memory takes 1.66GiB; the rest of the memory reserved for KV Cache is 56.79GiB.\n",
      "INFO 04-28 17:56:35 worker.py:267] Memory profiling takes 4.25 seconds\n",
      "INFO 04-28 17:56:35 worker.py:267] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.90) = 125.75GiB\n",
      "INFO 04-28 17:56:35 worker.py:267] model weights take 65.74GiB; non_torch_memory takes 1.82GiB; PyTorch activation peak memory takes 1.66GiB; the rest of the memory reserved for KV Cache is 56.53GiB.\n",
      "INFO 04-28 17:56:35 executor_base.py:110] # CUDA blocks: 23156, # CPU blocks: 1638\n",
      "INFO 04-28 17:56:35 executor_base.py:115] Maximum concurrency for 12880 tokens per request: 28.77x\n",
      "INFO 04-28 17:56:38 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1878170)\u001b[0;0m INFO 04-28 17:56:38 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:17<00:00,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-28 17:56:56 custom_all_reduce.py:226] Registering 5635 cuda graph addresses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=1878170)\u001b[0;0m INFO 04-28 17:56:56 custom_all_reduce.py:226] Registering 5635 cuda graph addresses\n",
      "INFO 04-28 17:56:56 model_runner.py:1562] Graph capturing finished in 18 secs, took 0.49 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1878170)\u001b[0;0m INFO 04-28 17:56:56 model_runner.py:1562] Graph capturing finished in 18 secs, took 0.49 GiB\n",
      "INFO 04-28 17:56:56 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 25.25 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it, est. speed input: 238.35 toks/s, output: 37.47 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.41s/it, est. speed input: 213.44 toks/s, output: 41.26 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.27s/it, est. speed input: 250.85 toks/s, output: 41.15 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it, est. speed input: 283.42 toks/s, output: 41.00 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.31s/it, est. speed input: 244.50 toks/s, output: 41.26 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.31s/it, est. speed input: 244.34 toks/s, output: 41.10 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it, est. speed input: 245.64 toks/s, output: 41.07 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it, est. speed input: 261.23 toks/s, output: 40.98 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it, est. speed input: 267.42 toks/s, output: 40.88 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.52s/it, est. speed input: 207.19 toks/s, output: 41.44 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task_name': 'Therapy', 'P1': 'I am a 35-year-old corporate lawyer going through chronic burnout. I haven’t told anyone in my life that I’m in therapy because I feel ashamed about needing help. It’s hard to admit I’m struggling.I try to avoid opening up too much. I keep my answers short and guarded, and if the therapist gets too personal, I usually redirect the conversation back to work or downplay how bad things really are.', 'P2': '', 'conversation': [[0, \"Patient: I'm just here for some advice on how to manage my workload, I guess. Nothing too deep, just some practical tips to get me through the next few months.\\n\"], [1, \"Therapist: I appreciate your desire to start with a more practical approach, but I'm curious, is there something specific that's driving your desire to manage your workload right now, or is there another layer to your concerns that you're not sharing?\\n\"], [2, \"Patient: I don't think there's anything specific driving my desire to manage my workload, I just feel like I'm falling behind and I want to get everything under control. Can we just focus on the strategies I can use to prioritize tasks and manage my time better?\\n\"], [3, 'Therapist: You\\'re prioritizing control and feeling overwhelmed, which can be really uncomfortable. Can you tell me more about what it feels like when you say you\\'re \"falling behind\" – what\\'s that like for you emotionally?\\n'], [4, \"Patient: It's just a constant feeling of dread, like there's a weight on my chest whenever I'm thinking about work, and I feel like I'm just barely keeping my head above water. I don't want to think about what would happen if I didn't keep up.\\n\"], [5, \"Therapist: It sounds like the weight of responsibility is really pressing on you, and the fear of not meeting expectations is a heavy burden to carry; can you tell me more about who or what specifically you feel you're trying to meet the expectations of?\\n\"], [6, \"Patient: I guess it's just my partners at the firm, and the clients, and the whole sense of being accountable to them for our cases. If I don't deliver, it reflects poorly on everyone.\\n\"], [7, 'Therapist: It sounds like the pressure to perform is not only about meeting expectations, but also about maintaining a certain identity or reputation, which can be a heavy weight to bear; can you think of a time when you felt like you met expectations and felt truly supported and appreciated, rather than just burdened by responsibility?\\n'], [8, \"Patient: I'm drawing a complete blank, to be honest – it's been a long time since I can remember feeling like I was succeeding without feeling overwhelmed by it. I've just always associated success with a sense of pressure to keep up the pace.\\n\"], [9, \"Therapist: It sounds like there's a pattern of self-identification with your professional success that may be tied to a sense of worth, and I'm curious about how that narrative has developed over time. Can you imagine what it would be like to begin to separate your sense of identity from the demands of your work?\\n\"]], 'pturn': 1, 'index': -1, 'timestamp': '', 'rounds': 10, 'conversation_only': True, 'eval_prompt_consistency': {'combined_prompt_consistency': [[0, \"The patient's intention behind the statement is to downplay their true struggles and set boundaries, which aligns with their background of feeling ashamed and avoiding opening up, as they attempt to minimize the true purpose of their therapy sessions. NO\"], [2, \"The intention behind the patient's statement is to deflect the therapist's potential probing into the underlying causes of their burnout and steer the conversation towards a more superficial, solution-focused approach, which aligns with their background of avoiding openness and vulnerability, so NO.\"], [4, 'The intention behind the line is for the Patient to express their intense emotional distress and feelings of overwhelm related to their work, but this intention seems to contradict the given background as the Patient is typically guarded and avoids opening up, suggesting a rare moment of vulnerability; YES.'], [6, \"The intention behind the Patient's line is to shift the focus from their personal feelings to external pressures at work, aligning with their background of downplaying personal struggles and redirecting conversations to work-related topics, thus not contradicting the given background.\\n\\nNO\"], [8, 'The interpreted intention of the statement is that the patient is expressing a genuine emotional struggle and opening up about their feelings of being overwhelmed by success, which does not align with their background of being guarded and avoiding opening up too much. YES.']], 'forwards_combined_prompt_consistency': [[0, 'NO\\nThe intention behind the statement, to downplay the true reasons for seeking therapy and focus on a more manageable issue like workload, aligns with the provided background of the patient, as it reflects their tendency to avoid opening up and redirecting the conversation to a more superficial topic.'], [2, \"NO \\nThe intention behind the patient's statement is to avoid discussing the emotional drivers of their burnout and instead focus on practical solutions, which aligns with their background of being guarded and resistant to opening up about their struggles.\"], [4, \"NO. The patient's statement reveals their intense emotional struggle with work-related burnout, aligning with their background of feeling overwhelmed and struggling to admit their true emotions, but contradicts their tendency to downplay their issues, as this statement is unusually candid and vulnerable.\"], [6, \"NO\\n\\nThe intention behind the patient's statement appears to be deflecting from their personal struggles by attributing their stress to external pressures at work, which aligns with their background of being guarded and redirecting conversations to work-related topics.\"], [8, \"NO\\n\\nThe statement aligns with the patient's background, as the intention behind it is to express their struggle with feeling overwhelmed despite being successful, which is a symptom of their chronic burnout, and the vulnerability of this admission is tempered by the patient's characteristic guardedness in framing it as a distant memory.\"]]}, 'P1_prompt_consistency_score': 1.0, 'P1_prompt_consistency_scores': {'combined_prompt_consistency': {0: 0.6}, 'forwards_combined_prompt_consistency': {0: 1.0}}}\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it, est. speed input: 271.45 toks/s, output: 40.85 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it, est. speed input: 250.83 toks/s, output: 40.97 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it, est. speed input: 260.50 toks/s, output: 41.09 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it, est. speed input: 256.53 toks/s, output: 41.14 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 317.30 toks/s, output: 40.65 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.43s/it, est. speed input: 224.35 toks/s, output: 41.24 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.31s/it, est. speed input: 232.78 toks/s, output: 41.21 toks/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conversation \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m         eval_prompt_consistency_ablations(conversation)\n\u001b[1;32m      7\u001b[0m     test_convs\u001b[38;5;241m.\u001b[39mappend(eval_prompt_consistency_ablations(conversation))\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/nfs/kun2/users/ryan_cheng/consistency_LLMs/therapy/exp/04.22.25/ablation_llama70b_Llama-3.1-8B-Instruct_0_500.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[20], line 37\u001b[0m, in \u001b[0;36meval_prompt_consistency_ablations\u001b[0;34m(conv_dict)\u001b[0m\n\u001b[1;32m     35\u001b[0m     conv_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_prompt_consistency\u001b[39m\u001b[38;5;124m'\u001b[39m][key]\u001b[38;5;241m.\u001b[39mappend([line_number, output])\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 37\u001b[0m     conv_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_prompt_consistency\u001b[39m\u001b[38;5;124m'\u001b[39m][key][line_number]\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYES\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m output:  \u001b[38;5;66;03m# no contradiction\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     conv_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP1_prompt_consistency_scores\u001b[39m\u001b[38;5;124m'\u001b[39m][key][index] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# %%capture \n",
    "\n",
    "test_convs = []\n",
    "for conversation in data:\n",
    "    for i in range(4):\n",
    "        eval_prompt_consistency_ablations(conversation)\n",
    "    test_convs.append(eval_prompt_consistency_ablations(conversation))\n",
    "    with open(\"/nfs/kun2/users/ryan_cheng/consistency_LLMs/therapy/exp/04.22.25/ablation_llama70b_Llama-3.1-8B-Instruct_0_500.json\", 'w') as f:\n",
    "        json.dump(test_convs, f, indent=4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f5b669",
   "metadata": {},
   "source": [
    "57 min 21 sec Llama-3.1-70B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965979b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 06:58:11,702 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.537 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 06:58:21,707 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.537 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 06:58:31,710 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.537 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 06:58:41,717 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.535 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 06:58:51,721 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.535 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 06:59:01,726 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.535 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 06:59:11,732 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.535 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 06:59:21,737 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.535 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 06:59:31,743 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.535 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 06:59:41,748 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.533 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 06:59:51,754 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.533 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:00:01,760 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.533 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:00:11,767 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.533 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:00:21,774 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.533 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:00:31,780 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.533 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:00:41,784 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.532 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:00:51,789 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.532 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:01:01,794 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.532 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:01:11,799 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.532 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:01:21,803 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.532 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:01:31,806 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.532 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:01:41,812 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.53 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:01:51,816 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.53 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:02:01,819 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.53 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:02:11,824 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.53 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:02:21,829 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.53 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:02:31,833 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.53 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:02:41,837 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.529 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:02:51,841 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.529 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:03:01,846 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.529 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:03:11,851 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.529 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:03:21,855 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.529 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:03:31,859 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.529 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:03:41,866 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.528 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:03:51,870 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.528 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:04:01,874 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.528 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:04:11,878 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.528 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:04:21,883 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.528 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:04:31,887 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.528 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:04:41,891 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.526 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:04:51,894 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.526 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:05:01,899 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.526 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:05:11,905 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.526 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:05:21,910 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.526 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:05:31,915 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.526 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:05:41,921 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.525 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:05:51,927 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.525 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:06:01,931 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.525 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:06:11,936 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.525 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:06:21,940 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.525 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:06:31,944 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.525 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:06:41,949 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.523 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:06:51,953 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.523 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:07:01,957 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.523 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:07:11,961 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.523 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:07:21,966 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.523 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:07:31,969 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.523 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:07:41,972 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.521 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:07:51,976 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.521 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:08:01,980 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.521 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:08:11,983 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.521 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:08:21,987 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.521 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:08:31,992 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.521 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:08:41,998 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.52 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:08:52,004 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.52 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:09:02,009 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.52 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:09:12,015 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.52 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:09:22,019 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.52 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:09:32,023 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.52 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:09:42,028 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.518 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:09:52,032 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.518 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:10:02,037 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.518 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:10:12,041 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.518 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:10:22,045 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.518 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:10:32,050 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.518 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-04-27 07:10:42,054 E 1168096 1168128] file_system_monitor.cc:116: /raid/users/ryan_cheng/tmp/ray/session_2025-04-27_06-49-29_278705_3512463 is over 95% full, available space: 356.517 GB; capacity: 28500 GB. Object creation will fail if spilling is required.\n"
     ]
    }
   ],
   "source": [
    "test_convs = []\n",
    "for conversation in data:\n",
    "    test_convs.append(eval_index_consistency(conversation))\n",
    "    with open(\"/nfs/kun2/users/ryan_cheng/consistency_LLMs/therapy/exp/04.22.25/index_llama70b_Llama-3.1-8B-Instruct_0_500.json\", 'w') as f:\n",
    "        json.dump(test_convs, f, indent=4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d706f555",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_convs = []\n",
    "for conversation in data:\n",
    "    test_convs.append(eval_prompt_consistency(conversation))\n",
    "    with open(\"/nfs/kun2/users/ryan_cheng/consistency_LLMs/therapy/exp/04.22.25/llama8beval_Llama-3.1-8B-Instruct_0_500.json\", 'w') as f:\n",
    "        json.dump(test_convs, f, indent=4)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9667deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343e702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_therapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ce369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prompt_consistency(conv_dict):\n",
    "    #assert 'eval_prompt_consistency' not in conv_dict # warn if we are replacing metrics we don't mean to overwrite\n",
    "    conv_dict['eval_prompt_consistency'] = []\n",
    "    conv_dict['P1_prompt_consistency_score'] = 0\n",
    "    p1_utterances = 0\n",
    "    pturn = conv_dict[\"pturn\"]\n",
    "    for line in conv_dict[\"conversation\"]:\n",
    "        line_number = line[0]\n",
    "        convo_line = line[1]\n",
    "        if pturn == 1:\n",
    "            prompt = config_therapy[\"eval_prompts\"][\"prompt_consistency\"].replace(\"%SPEAKER_ROLE%\", config_therapy[\"agent1_role\"]) \\\n",
    "                                                                          .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P1\"]) \\\n",
    "                                                                          .replace(\"%SPEAKER_LINE%\", convo_line)\n",
    "            if config_llm['verbose']:\n",
    "                print(prompt)\n",
    "            output = completion_create(config_llm['eval_model'], config, prompt)\n",
    "            conv_dict['eval_prompt_consistency'].append((line_number, output))\n",
    "            if \"YES\" not in output: # no contradiction\n",
    "                conv_dict['P1_prompt_consistency_score'] += 1\n",
    "            p1_utterances += 1\n",
    "            pturn = 2\n",
    "        elif pturn == 2:\n",
    "            pturn = 1\n",
    "    if p1_utterances > 0:\n",
    "        conv_dict['P1_prompt_consistency_score'] /= p1_utterances\n",
    "    print(conv_dict)\n",
    "\n",
    "    return conv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e4525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prompt_consistency(conv_dict):\n",
    "    #assert 'eval_prompt_consistency' not in conv_dict # warn if we are replacing metrics we don't mean to overwrite\n",
    "    conv_dict['eval_prompt_consistency'] = []\n",
    "    conv_dict['P1_prompt_consistency_score'] = 0\n",
    "    p1_utterances = 0\n",
    "    pturn = conv_dict[\"pturn\"]\n",
    "    for line in conv_dict[\"conversation\"]:\n",
    "        line_number = line[0]\n",
    "        convo_line = line[1]\n",
    "        if pturn == 1:\n",
    "            prompt = config_therapy[\"eval_prompts\"][\"prompt_consistency\"].replace(\"%SPEAKER_ROLE%\", config_therapy[\"agent1_role\"]) \\\n",
    "                                                                          .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P1\"]) \\\n",
    "                                                                          .replace(\"%SPEAKER_LINE%\", convo_line)\n",
    "            if config_llm['verbose']:\n",
    "                print(prompt)\n",
    "            output = completion_create(config_llm['eval_model'], config, prompt)\n",
    "            conv_dict['eval_prompt_consistency'].append((line_number, output))\n",
    "            if \"YES\" not in output: # no contradiction\n",
    "                conv_dict['P1_prompt_consistency_score'] += 1\n",
    "            p1_utterances += 1\n",
    "            pturn = 2\n",
    "        elif pturn == 2:\n",
    "            pturn = 1\n",
    "    if p1_utterances > 0:\n",
    "        conv_dict['P1_prompt_consistency_score'] /= p1_utterances\n",
    "    print(conv_dict)\n",
    "\n",
    "    return conv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e8189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prompt_consistency(conv_dict):\n",
    "    #assert 'eval_prompt_consistency' not in conv_dict # warn if we are replacing metrics we don't mean to overwrite\n",
    "    conv_dict['eval_prompt_consistency'] = []\n",
    "    conv_dict['P1_prompt_consistency_score'] = 0\n",
    "    conv_dict['P2_prompt_consistency_score'] = 0\n",
    "    p1_utterances = 0\n",
    "    p2_utterances = 0\n",
    "    pturn = conv_dict[\"pturn\"]\n",
    "    for line in conv_dict[\"conversation\"]:\n",
    "        if pturn == 1:\n",
    "            prompt = prompts[\"eval_prompts\"][\"prompt_consistency\"].replace(\"%SPEAKER_ROLE%\", prompts[\"agent1_role\"]) \\\n",
    "                                                                  .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P1\"]) \\\n",
    "                                                                  .replace(\"%SPEAKER_LINE%\", line)\n",
    "            if config['verbose']:\n",
    "                print(prompt)\n",
    "            output = completion_create(config['eval_model'], config, prompt)\n",
    "            conv_dict['eval_prompt_consistency'].append(output)\n",
    "            if \"YES\" not in output: # no contradiction\n",
    "                conv_dict['P1_prompt_consistency_score'] += 1\n",
    "            p1_utterances += 1\n",
    "            pturn = 2\n",
    "        else:\n",
    "            prompt = prompts[\"eval_prompts\"][\"prompt_consistency\"].replace(\"%SPEAKER_ROLE%\", prompts[\"agent2_role\"]) \\\n",
    "                                                                  .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P2\"]) \\\n",
    "                                                                  .replace(\"%SPEAKER_LINE%\", line)\n",
    "            if config['verbose']:\n",
    "                print(prompt)\n",
    "            output = completion_create(config['eval_model'], config, prompt)\n",
    "            conv_dict['eval_prompt_consistency'].append(output)\n",
    "            if \"YES\" not in output: # no contradiction\n",
    "                conv_dict['P2_prompt_consistency_score'] += 1\n",
    "            p2_utterances += 1\n",
    "            pturn = 1\n",
    "    \n",
    "    if p1_utterances > 0:\n",
    "        conv_dict['P1_prompt_consistency_score'] /= p1_utterances\n",
    "    if p2_utterances > 0:\n",
    "        conv_dict['P2_prompt_consistency_score'] /= p2_utterances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75da720",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google/gemma-2b-it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb1b11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": 'hello world'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1f6a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a437f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
