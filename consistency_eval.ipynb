{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ef1b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3,4\n",
      "env: TMPDIR=/raid/users/ryan_cheng/tmp\n",
      "INFO 04-28 21:49:38 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=3,4\n",
    "%env TMPDIR=/raid/users/ryan_cheng/tmp\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "from absl import app, flags\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from utils import *\n",
    "import utils\n",
    "from consistency_eval import *\n",
    "from education_generation import *\n",
    "\n",
    "try:\n",
    "    from vllm import LLM, SamplingParams\n",
    "    import ray\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d588ba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.abspath('../ryan_openai.txt'), 'r') as f:\n",
    "    utils.client = OpenAI(api_key=f.read().rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a871b942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old therapy convs\n",
    "# filename = '/nfs/kun2/users/ryan_cheng/consistency_LLMs/therapy/exp/04.22.25/Llama-3.1-8B-Instruct_0_500.json'\n",
    "# education convs\n",
    "filename = '/nfs/kun2/users/ryan_cheng/consistency_LLMs/data/education/exp/04.28.25/Llama-3.1-8B-Instruct_0_395.json'\n",
    "\n",
    "with open(\"/nfs/kun2/users/ryan_cheng/consistency_LLMs/therapy/config_therapy.json\", 'r') as f:\n",
    "    config_therapy = json.load(f)\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(\"./config/education/gpt-4o-mini.json\", 'r') as f:\n",
    "    config_gpt4_mini = json.load(f)\n",
    "\n",
    "for key, value in config_gpt4_mini.items():\n",
    "    config[key] = value\n",
    "\n",
    "\n",
    "\n",
    "for key, value in config_therapy.items():\n",
    "    prompts[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4560ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc7878",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"/nfs/kun2/users/ryan_cheng/consistency_LLMs/data/education/exp/04.28.25/Llama-3.1-8B-Instruct_0_395.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a930a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/nfs/kun2/users/ryan_cheng/consistency_LLMs/config/eval_prompts.json\" , 'r') as f:\n",
    "    eval_prompts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4010240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['eval_model'] = 'Llama-3.1-8B-Instruct'\n",
    "config['gpus'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "776d51bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['eval_model'] = 'Llama-3.1-70B-Instruct'\n",
    "config['gpus'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4580970",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['eval_model'] = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156dc910",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['task_name'] = 'Therapy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf56271",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5905f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list(text):\n",
    "    pattern = r'\\[.*?\\]'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        try:\n",
    "            return eval(match.group())\n",
    "        except (SyntaxError, NameError):\n",
    "            return []\n",
    "    return[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89c417e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts['eval_prompts'] = eval_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cd7eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0615e0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts[\"eval_prompts\"][\"index_consistency_background\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61d2022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_conversation(conversation):\n",
    "    return \"\".join([str(i) + \": \" + line for i, line in conversation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786f231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(format_conversation(data[0][\"conversation\"][:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7878b833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_index_consistency(conv_dict, both_agents=False):\n",
    "    conv_dict['eval_index_consistency'] = []\n",
    "    conv_dict['P1_index_consistency_score'] = 0\n",
    "    if both_agents:\n",
    "        conv_dict['P2_index_consistency_score'] = 0\n",
    "    p1_utterances = 0\n",
    "    p2_utterances = 0\n",
    "    pturn = conv_dict[\"pturn\"]\n",
    "    for i, line in conv_dict[\"conversation\"]:\n",
    "        if i < 2: # skip first 2 lines of dialogue\n",
    "            continue \n",
    "        if pturn == 1:\n",
    "            prompt = prompts[\"eval_prompts\"][\"index_consistency\"].replace(\"%SCENARIO_DESC%\", prompts[\"scenario\"]) \\\n",
    "                                                                 .replace(\"%SPEAKER_ROLE%\", prompts[\"agent1_role\"]) \\\n",
    "                                                                 .replace(\"%CONVERSATION%\", format_conversation(conv_dict[\"conversation\"][:i])) \\\n",
    "                                                                 .replace(\"%SPEAKER_LINE%\", line)\n",
    "            if config['verbose']:\n",
    "                print(prompt)\n",
    "            output = completion_create(config['eval_model'], config, prompt)\n",
    "            index_list = extract_list(output)\n",
    "            conv_dict['eval_index_consistency'].append((i, output))\n",
    "            conv_dict['P1_index_consistency_score'] += len(index_list)\n",
    "            p1_utterances += i // 2\n",
    "            pturn = 2\n",
    "        elif pturn == 2:\n",
    "            if both_agents:\n",
    "                prompt = prompts[\"eval_prompts\"][\"index_consistency\"].replace(\"%SCENARIO_DESC%\", prompts[\"scenario\"]) \\\n",
    "                                                                     .replace(\"%SPEAKER_ROLE%\", prompts[\"agent2_role\"]) \\\n",
    "                                                                     .replace(\"%CONVERSATION%\", format_conversation(conv_dict[\"conversation\"][:i])) \\\n",
    "                                                                     .replace(\"%SPEAKER_LINE%\", line)\n",
    "                if config['verbose']:\n",
    "                    print(prompt)\n",
    "                output = completion_create(config['eval_model'], config, prompt)\n",
    "                index_list = extract_list(output)\n",
    "                conv_dict['eval_index_consistency'].append((i, output))\n",
    "                conv_dict['P2_index_consistency_score'] += len(index_list)\n",
    "                p2_utterances += i // 2\n",
    "            pturn = 1\n",
    "\n",
    "    if p1_utterances > 0:\n",
    "        conv_dict['P1_index_consistency_score'] /= p1_utterances\n",
    "        conv_dict['P1_index_consistency_score'] = 1 - conv_dict['P1_index_consistency_score']\n",
    "    if p2_utterances > 0 and both_agents:\n",
    "        conv_dict['P2_index_consistency_score'] /= p2_utterances\n",
    "        conv_dict['P2_index_consistency_score'] = 1 - conv_dict['P2_index_consistency_score']\n",
    "\n",
    "    return conv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0986c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_index_background_consistency(conv_dict, both_agents=False):\n",
    "    conv_dict['eval_index_consistency'] = []\n",
    "    conv_dict['P1_index_consistency_score'] = 0\n",
    "    if both_agents:\n",
    "        conv_dict['P2_index_consistency_score'] = 0\n",
    "    p1_utterances = 0\n",
    "    p2_utterances = 0\n",
    "    pturn = conv_dict[\"pturn\"]\n",
    "    for i, line in conv_dict[\"conversation\"]:\n",
    "        if i < 2: # skip first 2 lines of dialogue\n",
    "            continue \n",
    "        if pturn == 1:\n",
    "            prompt = prompts[\"eval_prompts\"][\"index_consistency_background\"].replace(\"%SCENARIO_DESC%\", prompts[\"scenario\"]) \\\n",
    "                                                                 .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P1\"]) \\\n",
    "                                                                 .replace(\"%SPEAKER_ROLE%\", prompts[\"agent1_role\"]) \\\n",
    "                                                                 .replace(\"%CONVERSATION%\", format_conversation(conv_dict[\"conversation\"][:i])) \\\n",
    "                                                                 .replace(\"%SPEAKER_LINE%\", line)\n",
    "            if config['verbose']:\n",
    "                print(prompt)\n",
    "            output = completion_create(config['eval_model'], config, prompt)\n",
    "            index_list = extract_list(output)\n",
    "            conv_dict['eval_index_consistency'].append((i, output))\n",
    "            conv_dict['P1_index_consistency_score'] += len(index_list)\n",
    "            p1_utterances += i // 2\n",
    "            pturn = 2\n",
    "        elif pturn == 2:\n",
    "            if both_agents:\n",
    "                prompt = prompts[\"eval_prompts\"][\"index_consistency_background\"].replace(\"%SCENARIO_DESC%\", prompts[\"scenario\"]) \\\n",
    "                                                                     .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P2\"]) \\\n",
    "                                                                     .replace(\"%SPEAKER_ROLE%\", prompts[\"agent2_role\"]) \\\n",
    "                                                                     .replace(\"%CONVERSATION%\", format_conversation(conv_dict[\"conversation\"][:i])) \\\n",
    "                                                                     .replace(\"%SPEAKER_LINE%\", line)\n",
    "                if config['verbose']:\n",
    "                    print(prompt)\n",
    "                output = completion_create(config['eval_model'], config, prompt)\n",
    "                index_list = extract_list(output)\n",
    "                conv_dict['eval_index_consistency'].append((i, output))\n",
    "                conv_dict['P2_index_consistency_score'] += len(index_list)\n",
    "                p2_utterances += i // 2\n",
    "            pturn = 1\n",
    "\n",
    "    if p1_utterances > 0:\n",
    "        conv_dict['P1_index_consistency_score'] /= p1_utterances\n",
    "        conv_dict['P1_index_consistency_score'] = 1 - conv_dict['P1_index_consistency_score']\n",
    "    if p2_utterances > 0 and both_agents:\n",
    "        conv_dict['P2_index_consistency_score'] /= p2_utterances\n",
    "        conv_dict['P2_index_consistency_score'] = 1 - conv_dict['P2_index_consistency_score']\n",
    "\n",
    "    return conv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59104e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prompt_consistency(conv_dict):\n",
    "    conv_dict['eval_prompt_consistency'] = {}\n",
    "    conv_dict['P1_prompt_consistency_scores'] = {}\n",
    "    p1_utterances = {}\n",
    "    \n",
    "    for key in [\"strategy_consistency\", \"background_consistency\", \"combined_prompt_consistency\"]:\n",
    "        conv_dict['eval_prompt_consistency'][key] = []\n",
    "        conv_dict['P1_prompt_consistency_scores'][key] = 0\n",
    "        p1_utterances[key] = 0\n",
    "\n",
    "    pturn = conv_dict[\"pturn\"]\n",
    "    for line in conv_dict[\"conversation\"]:\n",
    "        line_number = line[0]\n",
    "        convo_line = line[1]\n",
    "        if pturn == 1:\n",
    "            for key in [\"strategy_consistency\", \"background_consistency\", \"combined_prompt_consistency\"]:\n",
    "                prompt = eval_prompts[key].replace(\"%SCENARIO_DESC\", 'There is a Patient in conversation with a Therapist.') \\\n",
    "                                          .replace(\"%SPEAKER_ROLE%\", config_therapy[\"agent1_role\"]) \\\n",
    "                                          .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P1\"]) \\\n",
    "                                          .replace(\"%SPEAKER_LINE%\", convo_line)\n",
    "                if config.get('verbose', False):\n",
    "                    print(prompt)\n",
    "                output = completion_create(config['eval_model'], config, prompt)\n",
    "                conv_dict['eval_prompt_consistency'][key].append((line_number, output))\n",
    "                if \"YES\" not in output:  # no contradiction\n",
    "                    conv_dict['P1_prompt_consistency_scores'][key] += 1\n",
    "                p1_utterances[key] += 1\n",
    "            pturn = 2\n",
    "        elif pturn == 2:\n",
    "            pturn = 1\n",
    "\n",
    "    for key in [\"strategy_consistency\", \"background_consistency\", \"combined_prompt_consistency\"]:\n",
    "        if p1_utterances[key] > 0:\n",
    "            conv_dict['P1_prompt_consistency_scores'][key] /= p1_utterances[key]\n",
    "\n",
    "    print(conv_dict)\n",
    "    return conv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b51381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prompt_consistency_ablations(conv_dict):\n",
    "    \n",
    "    p1_utterances = {}\n",
    "    keys = [\"combined_prompt_consistency\", \"forwards_combined_prompt_consistency\"]\n",
    "    if \"P1_prompt_consistency_scores\" not in conv_dict:\n",
    "        conv_dict['eval_prompt_consistency'] = {}\n",
    "        conv_dict['P1_prompt_consistency_scores'] = {}\n",
    "        index = 0\n",
    "        for key in keys:\n",
    "            conv_dict['P1_prompt_consistency_scores'][key] = {}\n",
    "            conv_dict['eval_prompt_consistency'][key] = []\n",
    "    else:\n",
    "        index = len(conv_dict['eval_prompt_consistency'][keys[0]][0]) - 1\n",
    "        print(index)\n",
    "    \n",
    "    for key in keys:\n",
    "        conv_dict['P1_prompt_consistency_scores'][key][index] = 0\n",
    "        p1_utterances[key] = 0\n",
    "\n",
    "\n",
    "    pturn = conv_dict[\"pturn\"]\n",
    "    for line in conv_dict[\"conversation\"]:\n",
    "        line_number = line[0]\n",
    "        convo_line = line[1]\n",
    "        if pturn == 1:\n",
    "            for key in keys:\n",
    "                prompt = eval_prompts[key].replace(\"%SCENARIO_DESC\", 'There is a Patient in conversation with a Therapist.') \\\n",
    "                                          .replace(\"%SPEAKER_ROLE%\", config_therapy[\"agent1_role\"]) \\\n",
    "                                          .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P1\"]) \\\n",
    "                                          .replace(\"%SPEAKER_LINE%\", convo_line)\n",
    "                if config.get('verbose', False):\n",
    "                    print(prompt)\n",
    "                output = completion_create(config['eval_model'], config, prompt)\n",
    "                if index == 0:\n",
    "                    conv_dict['eval_prompt_consistency'][key].append([line_number, output])\n",
    "                else:\n",
    "                    conv_dict['eval_prompt_consistency'][key][line_number // 2].append(output)\n",
    "\n",
    "                if \"YES\" not in output:  # no contradiction\n",
    "                    conv_dict['P1_prompt_consistency_scores'][key][index] += 1\n",
    "                p1_utterances[key] += 1\n",
    "            pturn = 2\n",
    "        elif pturn == 2:\n",
    "            pturn = 1\n",
    "\n",
    "    for key in keys:\n",
    "        if p1_utterances[key] > 0:\n",
    "            conv_dict['P1_prompt_consistency_scores'][key][index] /= p1_utterances[key]\n",
    "\n",
    "    print(conv_dict)\n",
    "    return conv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e787492e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'strategy_consistency': \"%SCENARIO_DESC% Evaluate whether the following line spoken by %SPEAKER_ROLE% contradicts their background. First, describe the interpreted intention of the statement, and whether or not it aligns with the given background of %SPEAKER_ROLE%. Then, provide 1 sentence explaining your reasoning based on the strategy, followed by YES if the line contradicts the given background of %SPEAKER_ROLE%, and NO if it is consistent with the provided background of %SPEAKER_ROLE%. %SPEAKER_ROLE%'s strategy is described as follows:\\n%SPEAKER_STRATEGY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide 1 sentence of reasoning followed by YES or NO.\\n\\n\",\n",
       " 'background_consistency': \"%SCENARIO_DESC% Evaluate whether the following line spoken by %SPEAKER_ROLE% contradicts their background. Provide 1 sentence explaining your reasoning based on the background, followed by YES if the line contradicts the given background of %SPEAKER_ROLE%, and NO if it is consistent with the provided background of %SPEAKER_ROLE%. %SPEAKER_ROLE%'s background is described as follows:\\n%SPEAKER_BACKSTORY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide 1 sentence of reason reasoning followed by YES or NO.\\n\\n\",\n",
       " 'combined_prompt_consistency': \"%SCENARIO_DESC% Evaluate the intention behind the following line spoken by %SPEAKER_ROLE% and determine whether it contradicts their background. First, describe the interpreted intention of the statement, and whether or not it aligns with the given background of %SPEAKER_ROLE%. Then, answer YES if the line contradicts the given background of %SPEAKER_ROLE% or the intention does not align with the provided background, and answer NO if it does align with the provided background or the intention aligns with the background of %SPEAKER_ROLE%. %SPEAKER_ROLE%'s background is described as follows:\\n%SPEAKER_BACKSTORY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide your answer as 1 sentence explaining your reasoning based on the background and the interpreted intention, followed by YES or NO.\\n\\n\",\n",
       " 'forwards_combined_prompt_consistency': \"%SCENARIO_DESC% Evaluate the intention behind the following line spoken by %SPEAKER_ROLE% and determine whether it contradicts their background. Answer YES if the line contradicts the given background of %SPEAKER_ROLE% or the intention does not align with the provided background, and answer NO if it does align with the provided background or the intention aligns with the background of %SPEAKER_ROLE%, then describe the interpreted intention of the statement and whether or not it aligns with the given background of %SPEAKER_ROLE% within 1 sentence. %SPEAKER_ROLE%'s background is described as follows:\\n%SPEAKER_BACKSTORY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide your answer as YES or NO first, followed by 1 sentence explaining your reasoning based on the background and the interpreted intention.\\n\\n\",\n",
       " 'index_consistency': '%SCENARIO_DESC% For the following line spoken by %SPEAKER_ROLE%, first determine if there is a CLEAR conflict or inconsistency between the line and any line within the conversation history spoken by %SPEAKER_ROLE%. IF there is a conflict, provide a sentence of reasoning followed by a list of indices of lines in the conversation history that have a clear conflict with the current line. Otherwise, provide a sentence of reasoning followed by an empty list. ONLY INCLUDE INDICES OF LINES THAT CORRESPOND TO %SPEAKER_ROLE%. The conversation up to this point is as follows: %CONVERSATION%. %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide your reasoning as 1 sentence, followed by a list of indices of conflicting lines from the conversation history formatted like a Python list in the following format: [index1, index2, index3, ...].\\n\\n',\n",
       " 'index_consistency_background': \"%SCENARIO_DESC% For the following line spoken by %SPEAKER_ROLE%, first determine if there is a CLEAR conflict or inconsistency between the line and any line within the conversation history spoken by %SPEAKER_ROLE%. IF there is a conflict, provide a sentence of reasoning followed by a list of indices of lines in the conversation history that have a clear conflict with the current line. Otherwise, provide a sentence of reasoning followed by an empty list. ONLY INCLUDE INDICES OF LINES THAT CORRESPOND TO %SPEAKER_ROLE%. The conversation up to this point is as follows: %CONVERSATION%. %SPEAKER_ROLE%'s background is described as follows:\\n%SPEAKER_BACKSTORY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide your reasoning as 1 sentence, followed by a list of indices of conflicting lines from the conversation history formatted like a Python list in the following format: [index1, index2, index3, ...].\\n\\n\",\n",
       " 'pairwise_consistency': '%SCENARIO_DESC% For the following line spoken by %SPEAKER_ROLE%, answer YES if the line directly contradicts the provided line spoken by %LISTENER_ROLE%, and answer NO if the line does not contradict the provided line spoken by %LISTENER_ROLE%. %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n %LISTENER_ROLE% spoke the following line: \\n%LISTENER_LINE%\\n\\n Answer YES if the line spoken by %SPEAKER_ROLE% contradicts the provided line spoken by %LISTENER_ROLE%, and answer NO if the line does not contradict the provided line spoken by %LISTENER_ROLE%, followed by 1 sentence of reasoning.\\n\\n',\n",
       " 'backstory_test': \"Based on the following background, generate a new fact-based multiple choice question with 5 choices addressed directly IN SECOND PERSON, along with its correct answer. Preface the question with 'Question:' and the answer with 'Answer:'.\\n%SPEAKER_BACKSTORY%\\n%PREVIOUS_QUESTIONS%\",\n",
       " 'answer_backstory': 'You are %SPEAKER_ROLE%, and you are having a conversation with %LISTENER_ROLE%. Your background is:\\n%SPEAKER_BACKSTORY%\\n So far, the conversation is as below:\\n%CONVERSATION%\\n\\n Based on your conversation above so far, answer the following multiple choice question.\\n%BACKSTORY_QUESTION%\\n',\n",
       " 'grade_backstory': 'As part of grading a test, determine whether the given answer %GIVEN_ANSWER% matches the following correct answer. Respond with either YES or NO.\\nCorrect Answer: %CORRECT_ANSWER%\\n'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7def349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prompt_consistency_ablations_education(conv_dict):\n",
    "    p1_utterances = {}\n",
    "    keys = [\"combined_prompt_consistency\", \"forwards_combined_prompt_consistency\"]\n",
    "    if \"P2_prompt_consistency_scores\" not in conv_dict:\n",
    "        conv_dict['eval_prompt_consistency'] = {}\n",
    "        conv_dict['P2_prompt_consistency_scores'] = {}\n",
    "        index = 0\n",
    "        for key in keys:\n",
    "            conv_dict['P2_prompt_consistency_scores'][key] = {}\n",
    "            conv_dict['eval_prompt_consistency'][key] = []\n",
    "    else:\n",
    "        index = len(conv_dict['eval_prompt_consistency'][keys[0]][0]) - 1\n",
    "        print(index)\n",
    "    \n",
    "    for key in keys:\n",
    "        conv_dict['P2_prompt_consistency_scores'][key][index] = 0\n",
    "        p1_utterances[key] = 0\n",
    "\n",
    "\n",
    "    pturn = conv_dict[\"pturn\"]\n",
    "    for line in conv_dict[\"conversation\"]:\n",
    "        line_number = line[0]\n",
    "        convo_line = line[1]\n",
    "        if pturn == 2:\n",
    "            for key in keys:\n",
    "                prompt = eval_prompts[key].replace(\"%SCENARIO_DESC\", 'A Teacher is trying to teach a Student about a topic. ') \\\n",
    "                                          .replace(\"%SPEAKER_ROLE%\", \"Student\") \\\n",
    "                                          .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P2\"]) \\\n",
    "                                          .replace(\"%SPEAKER_LINE%\", convo_line)\n",
    "                if config.get('verbose', False):\n",
    "                    print(prompt)\n",
    "                output = completion_create(config['eval_model'], config, prompt)\n",
    "                if index == 0:\n",
    "                    conv_dict['eval_prompt_consistency'][key].append([line_number, output])\n",
    "                else:\n",
    "                    conv_dict['eval_prompt_consistency'][key][line_number // 2].append(output)\n",
    "\n",
    "                if \"YES\" not in output:  # no contradiction\n",
    "                    conv_dict['P2_prompt_consistency_scores'][key][index] += 1\n",
    "                p1_utterances[key] += 1\n",
    "            pturn = 1\n",
    "        elif pturn == 1:\n",
    "            pturn = 2\n",
    "\n",
    "    for key in keys:\n",
    "        if p1_utterances[key] > 0:\n",
    "            conv_dict['P2_prompt_consistency_scores'][key][index] /= p1_utterances[key]\n",
    "\n",
    "    print(conv_dict)\n",
    "    return conv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee325de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 21:51:02,044\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-28 21:51:19 config.py:542] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed', 'score'}. Defaulting to 'generate'.\n",
      "INFO 04-28 21:51:20 config.py:1401] Defaulting to use mp for distributed inference\n",
      "INFO 04-28 21:51:20 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='meta-llama/Meta-Llama-3.1-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=12880, download_dir='/raid/users/ryan_cheng/models/', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-70B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 04-28 21:51:20 multiproc_worker_utils.py:300] Reducing Torch parallelism from 112 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 04-28 21:51:20 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 04-28 21:51:20 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "INFO 04-28 21:51:24 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 04-28 21:51:24 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 04-28 21:51:26 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 04-28 21:51:26 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 04-28 21:51:26 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 04-28 21:51:26 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 04-28 21:51:28 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/ryan_cheng/.cache/vllm/gpu_p2p_access_cache_for_3,4.json\n",
      "INFO 04-28 21:51:28 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/ryan_cheng/.cache/vllm/gpu_p2p_access_cache_for_3,4.json\n",
      "INFO 04-28 21:51:28 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_6bd94c28'), local_subscribe_port=44221, remote_subscribe_port=None)\n",
      "INFO 04-28 21:51:28 model_runner.py:1110] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 04-28 21:51:28 model_runner.py:1110] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 04-28 21:51:29 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "INFO 04-28 21:51:29 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "INFO 04-28 21:51:52 model_runner.py:1115] Loading model weights took 65.7409 GB\n",
      "INFO 04-28 21:51:52 model_runner.py:1115] Loading model weights took 65.7409 GB\n",
      "INFO 04-28 21:51:56 worker.py:267] Memory profiling takes 3.57 seconds\n",
      "INFO 04-28 21:51:56 worker.py:267] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.90) = 125.75GiB\n",
      "INFO 04-28 21:51:56 worker.py:267] model weights take 65.74GiB; non_torch_memory takes 1.56GiB; PyTorch activation peak memory takes 1.66GiB; the rest of the memory reserved for KV Cache is 56.79GiB.\n",
      "INFO 04-28 21:51:56 worker.py:267] Memory profiling takes 3.61 seconds\n",
      "INFO 04-28 21:51:56 worker.py:267] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.90) = 125.75GiB\n",
      "INFO 04-28 21:51:56 worker.py:267] model weights take 65.74GiB; non_torch_memory takes 1.82GiB; PyTorch activation peak memory takes 1.66GiB; the rest of the memory reserved for KV Cache is 56.53GiB.\n",
      "INFO 04-28 21:51:56 executor_base.py:110] # CUDA blocks: 23156, # CPU blocks: 1638\n",
      "INFO 04-28 21:51:56 executor_base.py:115] Maximum concurrency for 12880 tokens per request: 28.77x\n",
      "INFO 04-28 21:51:59 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-28 21:51:59 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-28 21:52:17 custom_all_reduce.py:226] Registering 5635 cuda graph addresses\n",
      "INFO 04-28 21:52:17 custom_all_reduce.py:226] Registering 5635 cuda graph addresses\n",
      "INFO 04-28 21:52:17 model_runner.py:1562] Graph capturing finished in 18 secs, took 0.49 GiB\n",
      "INFO 04-28 21:52:17 model_runner.py:1562] Graph capturing finished in 18 secs, took 0.49 GiB\n",
      "INFO 04-28 21:52:17 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 24.38 seconds\n",
      "INFO 04-29 03:47:44 multiproc_worker_utils.py:253] Worker exiting\n",
      "INFO 04-29 03:47:45 multiproc_worker_utils.py:128] Killing local vLLM worker processes\n"
     ]
    }
   ],
   "source": [
    "%%capture \n",
    "\n",
    "test_convs = []\n",
    "for conversation in data:\n",
    "    for i in range(4):\n",
    "        eval_prompt_consistency_ablations_education(conversation)\n",
    "    test_convs.append(eval_prompt_consistency_ablations_education(conversation))\n",
    "    with open(\"/nfs/kun2/users/ryan_cheng/consistency_LLMs/data/education/exp/04.28.25/ablation_llama70b_Llama-3.1-8B-Instruct_0_395.json\", 'w') as f:\n",
    "        json.dump(test_convs, f, indent=4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41725559",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['eval_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5151e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e090b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "test_convs = []\n",
    "for conversation in data:\n",
    "    for i in range(4):\n",
    "        eval_prompt_consistency_ablations(conversation)\n",
    "    test_convs.append(eval_prompt_consistency_ablations(conversation))\n",
    "    with open(\"/nfs/kun2/users/ryan_cheng/consistency_LLMs/therapy/exp/04.22.25/ablation_llama70b_Llama-3.1-8B-Instruct_0_500.json\", 'w') as f:\n",
    "        json.dump(test_convs, f, indent=4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f5b669",
   "metadata": {},
   "source": [
    "57 min 21 sec Llama-3.1-70B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965979b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_convs = []\n",
    "for conversation in data:\n",
    "    test_convs.append(eval_index_consistency(conversation))\n",
    "    with open(\"/nfs/kun2/users/ryan_cheng/consistency_LLMs/therapy/exp/04.22.25/index_llama70b_Llama-3.1-8B-Instruct_0_500.json\", 'w') as f:\n",
    "        json.dump(test_convs, f, indent=4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d706f555",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_convs = []\n",
    "for conversation in data:\n",
    "    test_convs.append(eval_prompt_consistency(conversation))\n",
    "    with open(\"/nfs/kun2/users/ryan_cheng/consistency_LLMs/therapy/exp/04.22.25/llama8beval_Llama-3.1-8B-Instruct_0_500.json\", 'w') as f:\n",
    "        json.dump(test_convs, f, indent=4)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9667deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343e702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_therapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ce369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prompt_consistency(conv_dict):\n",
    "    #assert 'eval_prompt_consistency' not in conv_dict # warn if we are replacing metrics we don't mean to overwrite\n",
    "    conv_dict['eval_prompt_consistency'] = []\n",
    "    conv_dict['P1_prompt_consistency_score'] = 0\n",
    "    p1_utterances = 0\n",
    "    pturn = conv_dict[\"pturn\"]\n",
    "    for line in conv_dict[\"conversation\"]:\n",
    "        line_number = line[0]\n",
    "        convo_line = line[1]\n",
    "        if pturn == 1:\n",
    "            prompt = config_therapy[\"eval_prompts\"][\"prompt_consistency\"].replace(\"%SPEAKER_ROLE%\", config_therapy[\"agent1_role\"]) \\\n",
    "                                                                          .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P1\"]) \\\n",
    "                                                                          .replace(\"%SPEAKER_LINE%\", convo_line)\n",
    "            if config_llm['verbose']:\n",
    "                print(prompt)\n",
    "            output = completion_create(config_llm['eval_model'], config, prompt)\n",
    "            conv_dict['eval_prompt_consistency'].append((line_number, output))\n",
    "            if \"YES\" not in output: # no contradiction\n",
    "                conv_dict['P1_prompt_consistency_score'] += 1\n",
    "            p1_utterances += 1\n",
    "            pturn = 2\n",
    "        elif pturn == 2:\n",
    "            pturn = 1\n",
    "    if p1_utterances > 0:\n",
    "        conv_dict['P1_prompt_consistency_score'] /= p1_utterances\n",
    "    print(conv_dict)\n",
    "\n",
    "    return conv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e4525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prompt_consistency(conv_dict):\n",
    "    #assert 'eval_prompt_consistency' not in conv_dict # warn if we are replacing metrics we don't mean to overwrite\n",
    "    conv_dict['eval_prompt_consistency'] = []\n",
    "    conv_dict['P1_prompt_consistency_score'] = 0\n",
    "    p1_utterances = 0\n",
    "    pturn = conv_dict[\"pturn\"]\n",
    "    for line in conv_dict[\"conversation\"]:\n",
    "        line_number = line[0]\n",
    "        convo_line = line[1]\n",
    "        if pturn == 1:\n",
    "            prompt = config_therapy[\"eval_prompts\"][\"prompt_consistency\"].replace(\"%SPEAKER_ROLE%\", config_therapy[\"agent1_role\"]) \\\n",
    "                                                                          .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P1\"]) \\\n",
    "                                                                          .replace(\"%SPEAKER_LINE%\", convo_line)\n",
    "            if config_llm['verbose']:\n",
    "                print(prompt)\n",
    "            output = completion_create(config_llm['eval_model'], config, prompt)\n",
    "            conv_dict['eval_prompt_consistency'].append((line_number, output))\n",
    "            if \"YES\" not in output: # no contradiction\n",
    "                conv_dict['P1_prompt_consistency_score'] += 1\n",
    "            p1_utterances += 1\n",
    "            pturn = 2\n",
    "        elif pturn == 2:\n",
    "            pturn = 1\n",
    "    if p1_utterances > 0:\n",
    "        conv_dict['P1_prompt_consistency_score'] /= p1_utterances\n",
    "    print(conv_dict)\n",
    "\n",
    "    return conv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e8189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prompt_consistency(conv_dict):\n",
    "    #assert 'eval_prompt_consistency' not in conv_dict # warn if we are replacing metrics we don't mean to overwrite\n",
    "    conv_dict['eval_prompt_consistency'] = []\n",
    "    conv_dict['P1_prompt_consistency_score'] = 0\n",
    "    conv_dict['P2_prompt_consistency_score'] = 0\n",
    "    p1_utterances = 0\n",
    "    p2_utterances = 0\n",
    "    pturn = conv_dict[\"pturn\"]\n",
    "    for line in conv_dict[\"conversation\"]:\n",
    "        if pturn == 1:\n",
    "            prompt = prompts[\"eval_prompts\"][\"prompt_consistency\"].replace(\"%SPEAKER_ROLE%\", prompts[\"agent1_role\"]) \\\n",
    "                                                                  .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P1\"]) \\\n",
    "                                                                  .replace(\"%SPEAKER_LINE%\", line)\n",
    "            if config['verbose']:\n",
    "                print(prompt)\n",
    "            output = completion_create(config['eval_model'], config, prompt)\n",
    "            conv_dict['eval_prompt_consistency'].append(output)\n",
    "            if \"YES\" not in output: # no contradiction\n",
    "                conv_dict['P1_prompt_consistency_score'] += 1\n",
    "            p1_utterances += 1\n",
    "            pturn = 2\n",
    "        else:\n",
    "            prompt = prompts[\"eval_prompts\"][\"prompt_consistency\"].replace(\"%SPEAKER_ROLE%\", prompts[\"agent2_role\"]) \\\n",
    "                                                                  .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P2\"]) \\\n",
    "                                                                  .replace(\"%SPEAKER_LINE%\", line)\n",
    "            if config['verbose']:\n",
    "                print(prompt)\n",
    "            output = completion_create(config['eval_model'], config, prompt)\n",
    "            conv_dict['eval_prompt_consistency'].append(output)\n",
    "            if \"YES\" not in output: # no contradiction\n",
    "                conv_dict['P2_prompt_consistency_score'] += 1\n",
    "            p2_utterances += 1\n",
    "            pturn = 1\n",
    "    \n",
    "    if p1_utterances > 0:\n",
    "        conv_dict['P1_prompt_consistency_score'] /= p1_utterances\n",
    "    if p2_utterances > 0:\n",
    "        conv_dict['P2_prompt_consistency_score'] /= p2_utterances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75da720",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google/gemma-2b-it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb1b11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": 'hello world'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1f6a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a437f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openrlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
