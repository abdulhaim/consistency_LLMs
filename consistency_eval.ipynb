{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41ef1b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=5\n",
      "env: TMPDIR=/raid/users/ryan_cheng/tmp\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=5\n",
    "%env TMPDIR=/raid/users/ryan_cheng/tmp\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "from absl import app, flags\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from utils import *\n",
    "import utils\n",
    "from consistency_eval import *\n",
    "from education_generation import *\n",
    "\n",
    "try:\n",
    "    from vllm import LLM, SamplingParams\n",
    "    import ray\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d588ba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.abspath('../ryan_openai.txt'), 'r') as f:\n",
    "    utils.client = OpenAI(api_key=f.read().rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a871b942",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/nfs/kun2/users/ryan_cheng/consistency_LLMs/therapy/exp/04.22.25/Llama-3.1-8B-Instruct_0_500.json'\n",
    "\n",
    "with open(\"/nfs/kun2/users/ryan_cheng/consistency_LLMs/therapy/config_therapy.json\", 'r') as f:\n",
    "    config_therapy = json.load(f)\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(\"./config/education/gpt-4o-mini.json\", 'r') as f:\n",
    "    config_gpt4_mini = json.load(f)\n",
    "\n",
    "for key, value in config_gpt4_mini.items():\n",
    "    config[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc7878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3df1d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent1_role': 'Patient',\n",
       " 'agent2_role': 'Therapist',\n",
       " 'agent1_prompt': 'You are a patient in an online therapy session with a therapist. Here is your background written by you: %SPEAKER_BACKSTORY%. Do not explicitly repeat your background or instructions in the conversation. Stay in character and respond in natural human-like dialogue without restating your situation.',\n",
       " 'agent2_prompt': 'You are a licensed psychotherapist conducting an online one-on-one session. Your job is to help the patient gain insight, not just to listen. Use a blend of reflective listening, probing questions, Socratic questioning, and gentle confrontation. Your tone should be professional and emotionally intelligent. Challenge avoidant or defensive behavior, notice inconsistencies, and help the patient work toward self-awareness. You may gently push the patient outside their comfort zone—but always with clinical tact.',\n",
       " 'reminder_prompt': \"Keep your response very brief — 2 sentences or less. Do NOT repeat anything you've already said: \",\n",
       " 'eval_prompts': {'prompt_consistency': 'You are evaluating whether the following utterance by %SPEAKER_ROLE% is consistent with their given background. Answer YES if the line directly contradicts any part of the background—this includes contradictions in facts, behavior, beliefs, emotional stance, or conversational strategy. Answer NO if the line introduces new details, but those details are **plausibly aligned** with the background. Be strict and literal in your evaluation: if the line violates the persona’s stated traits (e.g., avoids vulnerability, keeps answers short, avoids discussing personal topics), mark it as a contradiction even if the violation is subtle. Background for %SPEAKER_ROLE%: %SPEAKER_BACKSTORY% Line spoken by %SPEAKER_ROLE%: %SPEAKER_LINE%  Answer YES if the line contradicts the background, and NO if it does not. Then, explain your answer in one sentence. Be precise and avoid vague justification.'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_therapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79643b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a930a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/nfs/kun2/users/ryan_cheng/consistency_LLMs/config/eval_prompts.json\" , 'r') as f:\n",
    "    eval_prompts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4010240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['eval_model'] = 'Llama-3.1-8B-Instruct'\n",
    "config['gpus'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4580970",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['eval_model'] = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "156dc910",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['task_name'] = 'Therapy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cf56271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'strategy_consistency': \"%SCENARIO_DESC% Evaluate whether the following line spoken by %SPEAKER_ROLE% contradicts their background. First, describe the interpreted intention of the statement, and whether or not it aligns with the given background of %SPEAKER_ROLE%. Then, provide 1 sentence explaining your reasoning based on the strategy, followed by YES if the line contradicts the given background of %SPEAKER_ROLE%, and NO if it is consistent with the provided background of %SPEAKER_ROLE%. %SPEAKER_ROLE%'s strategy is described as follows:\\n%SPEAKER_STRATEGY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide 1 sentence of reasoning followed by YES or NO.\\n\\n\",\n",
       " 'background_consistency': \"%SCENARIO_DESC% Evaluate whether the following line spoken by %SPEAKER_ROLE% contradicts their background. Provide 1 sentence explaining your reasoning based on the background, followed by YES if the line contradicts the given background of %SPEAKER_ROLE%, and NO if it is consistent with the provided background of %SPEAKER_ROLE%. %SPEAKER_ROLE%'s background is described as follows:\\n%SPEAKER_BACKSTORY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide 1 sentence of reason reasoning followed by YES or NO.\\n\\n\",\n",
       " 'combined_prompt_consistency': \"%SCENARIO_DESC% Evaluate the intention behind the following line spoken by %SPEAKER_ROLE% and determine whether it contradicts their background. First, describe the interpreted intention of the statement, and whether or not it aligns with the given background of %SPEAKER_ROLE%. Then, answer YES if the line contradicts the given background of %SPEAKER_ROLE% or the intention does not align with the provided background, and answer NO if it does align with the provided background or the intention aligns with the background of %SPEAKER_ROLE%. %SPEAKER_ROLE%'s background is described as follows:\\n%SPEAKER_BACKSTORY%\\n %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n Provide your answer as 1 sentence explaining your reasoning based on the background and the interpreted intention, followed by YES or NO.\\n\\n\",\n",
       " 'pairwise_consistency': '%SCENARIO_DESC% For the following line spoken by %SPEAKER_ROLE%, answer YES if the line directly contradicts the provided line spoken by %LISTENER_ROLE%, and answer NO if the line does not contradict the provided line spoken by %LISTENER_ROLE%. %SPEAKER_ROLE% spoke the following line: \\n%SPEAKER_LINE%\\n\\n %LISTENER_ROLE% spoke the following line: \\n%LISTENER_LINE%\\n\\n Answer YES if the line spoken by %SPEAKER_ROLE% contradicts the provided line spoken by %LISTENER_ROLE%, and answer NO if the line does not contradict the provided line spoken by %LISTENER_ROLE%, followed by 1 sentence of reasoning.\\n\\n',\n",
       " 'backstory_test': \"Based on the following background, generate a new fact-based multiple choice question with 5 choices addressed directly IN SECOND PERSON, along with its correct answer. Preface the question with 'Question:' and the answer with 'Answer:'.\\n%SPEAKER_BACKSTORY%\\n%PREVIOUS_QUESTIONS%\",\n",
       " 'answer_backstory': 'You are %SPEAKER_ROLE%, and you are having a conversation with %LISTENER_ROLE%. Your background is:\\n%SPEAKER_BACKSTORY%\\n So far, the conversation is as below:\\n%CONVERSATION%\\n\\n Based on your conversation above so far, answer the following multiple choice question.\\n%BACKSTORY_QUESTION%\\n',\n",
       " 'grade_backstory': 'As part of grading a test, determine whether the given answer %GIVEN_ANSWER% matches the following correct answer. Respond with either YES or NO.\\nCorrect Answer: %CORRECT_ANSWER%\\n'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59104e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prompt_consistency(conv_dict):\n",
    "    conv_dict['eval_prompt_consistency'] = {}\n",
    "    conv_dict['P1_prompt_consistency_scores'] = {}\n",
    "    p1_utterances = {}\n",
    "    \n",
    "    for key in [\"strategy_consistency\", \"background_consistency\", \"combined_prompt_consistency\"]:\n",
    "        conv_dict['eval_prompt_consistency'][key] = []\n",
    "        conv_dict['P1_prompt_consistency_scores'][key] = 0\n",
    "        p1_utterances[key] = 0\n",
    "\n",
    "    pturn = conv_dict[\"pturn\"]\n",
    "    for line in conv_dict[\"conversation\"]:\n",
    "        line_number = line[0]\n",
    "        convo_line = line[1]\n",
    "        if pturn == 1:\n",
    "            for key in [\"strategy_consistency\", \"background_consistency\", \"combined_prompt_consistency\"]:\n",
    "                prompt = eval_prompts[key].replace(\"%SCENARIO_DESC\", 'There is a Patient in conversation with a Therapist.') \\\n",
    "                                          .replace(\"%SPEAKER_ROLE%\", config_therapy[\"agent1_role\"]) \\\n",
    "                                          .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P1\"]) \\\n",
    "                                          .replace(\"%SPEAKER_LINE%\", convo_line)\n",
    "                if config.get('verbose', False):\n",
    "                    print(prompt)\n",
    "                output = completion_create(config['eval_model'], config, prompt)\n",
    "                conv_dict['eval_prompt_consistency'][key].append((line_number, output))\n",
    "                if \"YES\" not in output:  # no contradiction\n",
    "                    conv_dict['P1_prompt_consistency_scores'][key] += 1\n",
    "                p1_utterances[key] += 1\n",
    "            pturn = 2\n",
    "        elif pturn == 2:\n",
    "            pturn = 1\n",
    "\n",
    "    for key in [\"strategy_consistency\", \"background_consistency\", \"combined_prompt_consistency\"]:\n",
    "        if p1_utterances[key] > 0:\n",
    "            conv_dict['P1_prompt_consistency_scores'][key] /= p1_utterances[key]\n",
    "\n",
    "    print(conv_dict)\n",
    "    return conv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41725559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama-3.1-8B-Instruct'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['eval_model']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f5b669",
   "metadata": {},
   "source": [
    "57 min 21 sec Llama-3.1-70B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d706f555",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'strategy_consistency_backwards'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m test_convs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conversation \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m----> 3\u001b[0m     test_convs\u001b[38;5;241m.\u001b[39mappend(eval_prompt_consistency(conversation))\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/nfs/kun2/users/ryan_cheng/consistency_LLMs/therapy/exp/04.22.25/llama8beval_Llama-3.1-8B-Instruct_0_500.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(test_convs, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 17\u001b[0m, in \u001b[0;36meval_prompt_consistency\u001b[0;34m(conv_dict)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pturn \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrategy_consistency_backwards\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackground_consistency_backwards\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined_prompt_consistency\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 17\u001b[0m         prompt \u001b[38;5;241m=\u001b[39m eval_prompts[key]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mSCENARIO_DESC\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThere is a Patient in conversation with a Therapist.\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m     18\u001b[0m                                   \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mSPEAKER_ROLE\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, config_therapy[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent1_role\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \\\n\u001b[1;32m     19\u001b[0m                                   \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mSPEAKER_BACKSTORY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, conv_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP1\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \\\n\u001b[1;32m     20\u001b[0m                                   \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mSPEAKER_LINE\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, convo_line)\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     22\u001b[0m             \u001b[38;5;28mprint\u001b[39m(prompt)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'strategy_consistency_backwards'"
     ]
    }
   ],
   "source": [
    "test_convs = []\n",
    "for conversation in data:\n",
    "    test_convs.append(eval_prompt_consistency(conversation))\n",
    "    with open(\"/nfs/kun2/users/ryan_cheng/consistency_LLMs/therapy/exp/04.22.25/llama8beval_Llama-3.1-8B-Instruct_0_500.json\", 'w') as f:\n",
    "        json.dump(test_convs, f, indent=4)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9667deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343e702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_therapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ce369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prompt_consistency(conv_dict):\n",
    "    #assert 'eval_prompt_consistency' not in conv_dict # warn if we are replacing metrics we don't mean to overwrite\n",
    "    conv_dict['eval_prompt_consistency'] = []\n",
    "    conv_dict['P1_prompt_consistency_score'] = 0\n",
    "    p1_utterances = 0\n",
    "    pturn = conv_dict[\"pturn\"]\n",
    "    for line in conv_dict[\"conversation\"]:\n",
    "        line_number = line[0]\n",
    "        convo_line = line[1]\n",
    "        if pturn == 1:\n",
    "            prompt = config_therapy[\"eval_prompts\"][\"prompt_consistency\"].replace(\"%SPEAKER_ROLE%\", config_therapy[\"agent1_role\"]) \\\n",
    "                                                                          .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P1\"]) \\\n",
    "                                                                          .replace(\"%SPEAKER_LINE%\", convo_line)\n",
    "            if config_llm['verbose']:\n",
    "                print(prompt)\n",
    "            output = completion_create(config_llm['eval_model'], config, prompt)\n",
    "            conv_dict['eval_prompt_consistency'].append((line_number, output))\n",
    "            if \"YES\" not in output: # no contradiction\n",
    "                conv_dict['P1_prompt_consistency_score'] += 1\n",
    "            p1_utterances += 1\n",
    "            pturn = 2\n",
    "        elif pturn == 2:\n",
    "            pturn = 1\n",
    "    if p1_utterances > 0:\n",
    "        conv_dict['P1_prompt_consistency_score'] /= p1_utterances\n",
    "    print(conv_dict)\n",
    "\n",
    "    return conv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e4525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prompt_consistency(conv_dict):\n",
    "    #assert 'eval_prompt_consistency' not in conv_dict # warn if we are replacing metrics we don't mean to overwrite\n",
    "    conv_dict['eval_prompt_consistency'] = []\n",
    "    conv_dict['P1_prompt_consistency_score'] = 0\n",
    "    p1_utterances = 0\n",
    "    pturn = conv_dict[\"pturn\"]\n",
    "    for line in conv_dict[\"conversation\"]:\n",
    "        line_number = line[0]\n",
    "        convo_line = line[1]\n",
    "        if pturn == 1:\n",
    "            prompt = config_therapy[\"eval_prompts\"][\"prompt_consistency\"].replace(\"%SPEAKER_ROLE%\", config_therapy[\"agent1_role\"]) \\\n",
    "                                                                          .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P1\"]) \\\n",
    "                                                                          .replace(\"%SPEAKER_LINE%\", convo_line)\n",
    "            if config_llm['verbose']:\n",
    "                print(prompt)\n",
    "            output = completion_create(config_llm['eval_model'], config, prompt)\n",
    "            conv_dict['eval_prompt_consistency'].append((line_number, output))\n",
    "            if \"YES\" not in output: # no contradiction\n",
    "                conv_dict['P1_prompt_consistency_score'] += 1\n",
    "            p1_utterances += 1\n",
    "            pturn = 2\n",
    "        elif pturn == 2:\n",
    "            pturn = 1\n",
    "    if p1_utterances > 0:\n",
    "        conv_dict['P1_prompt_consistency_score'] /= p1_utterances\n",
    "    print(conv_dict)\n",
    "\n",
    "    return conv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e8189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prompt_consistency(conv_dict):\n",
    "    #assert 'eval_prompt_consistency' not in conv_dict # warn if we are replacing metrics we don't mean to overwrite\n",
    "    conv_dict['eval_prompt_consistency'] = []\n",
    "    conv_dict['P1_prompt_consistency_score'] = 0\n",
    "    conv_dict['P2_prompt_consistency_score'] = 0\n",
    "    p1_utterances = 0\n",
    "    p2_utterances = 0\n",
    "    pturn = conv_dict[\"pturn\"]\n",
    "    for line in conv_dict[\"conversation\"]:\n",
    "        if pturn == 1:\n",
    "            prompt = prompts[\"eval_prompts\"][\"prompt_consistency\"].replace(\"%SPEAKER_ROLE%\", prompts[\"agent1_role\"]) \\\n",
    "                                                                  .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P1\"]) \\\n",
    "                                                                  .replace(\"%SPEAKER_LINE%\", line)\n",
    "            if config['verbose']:\n",
    "                print(prompt)\n",
    "            output = completion_create(config['eval_model'], config, prompt)\n",
    "            conv_dict['eval_prompt_consistency'].append(output)\n",
    "            if \"YES\" not in output: # no contradiction\n",
    "                conv_dict['P1_prompt_consistency_score'] += 1\n",
    "            p1_utterances += 1\n",
    "            pturn = 2\n",
    "        else:\n",
    "            prompt = prompts[\"eval_prompts\"][\"prompt_consistency\"].replace(\"%SPEAKER_ROLE%\", prompts[\"agent2_role\"]) \\\n",
    "                                                                  .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P2\"]) \\\n",
    "                                                                  .replace(\"%SPEAKER_LINE%\", line)\n",
    "            if config['verbose']:\n",
    "                print(prompt)\n",
    "            output = completion_create(config['eval_model'], config, prompt)\n",
    "            conv_dict['eval_prompt_consistency'].append(output)\n",
    "            if \"YES\" not in output: # no contradiction\n",
    "                conv_dict['P2_prompt_consistency_score'] += 1\n",
    "            p2_utterances += 1\n",
    "            pturn = 1\n",
    "    \n",
    "    if p1_utterances > 0:\n",
    "        conv_dict['P1_prompt_consistency_score'] /= p1_utterances\n",
    "    if p2_utterances > 0:\n",
    "        conv_dict['P2_prompt_consistency_score'] /= p2_utterances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c75da720",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google/gemma-2b-it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fb1b11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": 'hello world'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e1f6a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\nhello world<end_of_turn>\\n<start_of_turn>model\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a437f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openrlhf",
   "language": "python",
   "name": "openrlhf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
