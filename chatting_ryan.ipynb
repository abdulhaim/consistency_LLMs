{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,2\n",
      "INFO 05-07 19:15:52 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,2\n",
    "\n",
    "import os\n",
    "import logging\n",
    "\n",
    "os.environ.pop(\"HF_HUB_OFFLINE\", None)\n",
    "logging.getLogger().setLevel(logging.ERROR)  # or logging.CRITICAL\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "from absl import app, flags\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import *\n",
    "import utils\n",
    "try:\n",
    "    from vllm import LLM, SamplingParams\n",
    "    import ray\n",
    "except ImportError:\n",
    "    pass\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: cuda:4\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import torch\n",
    "def get_freest_cuda_device():\n",
    "    result = subprocess.run(\n",
    "        ['nvidia-smi', '--query-gpu=memory.free', '--format=csv,nounits,noheader'],\n",
    "        stdout=subprocess.PIPE, encoding='utf-8')\n",
    "    memory_free = [int(x) for x in result.stdout.strip().split('\\n')]\n",
    "    return memory_free.index(max(memory_free))\n",
    "\n",
    "best_gpu = get_freest_cuda_device()\n",
    "device = torch.device(f\"cuda:{best_gpu}\")\n",
    "print(f\"Using GPU: {device}\")\n",
    "# %env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.abspath('../ryan_openai.txt'), 'r') as f:\n",
    "    utils.client = OpenAI(api_key=f.read().rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../token.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mHF_HUB_OFFLINE\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mTRANSFORMERS_OFFLINE\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../token.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     10\u001b[39m     token = f.read().strip()\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m login\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf2/lib/python3.12/site-packages/IPython/core/interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../token.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Explicitly unset all offline-related env vars\n",
    "os.environ.pop(\"HF_HUB_OFFLINE\", None)\n",
    "os.environ.pop(\"TRANSFORMERS_OFFLINE\", None)\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"0\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"0\"\n",
    "\n",
    "with open(\"../token.txt\", \"r\") as f:\n",
    "    token = f.read().strip()\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"Tianyi-Lab/Personas\")\n",
    "personas_chatting_array = ds['train']['Llama-3.1-70B-Instruct_descriptive_persona']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "random_indices = np.random.choice(len(personas_chatting_array), size=200, replace=False).astype(int)\n",
    "personas_chatting_array = np.array(personas_chatting_array)[random_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "import os\n",
    "config_chatting = {'agent1_role': 'Person A',\n",
    "                  'agent2_role': 'Person B',\n",
    "                  'agent1_prompt': \"You are %SPEAKER_ROLE%, and you are having an online conversation with %LISTENER_ROLE%. Each of you is chatting to get to know each other, taking turns asking questions, and sharing stories about your lives, careers, and experiences. The goal is to try to find something in common. Do not only ask questions, you should also share details about yourself. This is a brief story that you have written about yourself: %SPEAKER_BACKSTORY%\",\n",
    "                  'agent2_prompt': \"You are %SPEAKER_ROLE%, and you are having an online conversation with %LISTENER_ROLE%. Each of you is chatting to get to know each other, taking turns asking questions, and sharing stories about your lives, careers, and experiences. The goal is to try to find something in common. Do not only ask questions, you should also share details about yourself. This is a brief story that you have written about yourself: %SPEAKER_BACKSTORY%\",\n",
    "                  'reminder_prompt': \"Keep your response very brief — 2 sentences or less. Do NOT repeat anything you've already said: \\n\",\n",
    "                  \"scenario\": \"Two people are getting to know one another through an online conversation. \",\n",
    "                  'eval_prompts': {\n",
    "                  'prompt_consistency': \"\"\"You are evaluating whether the following utterance by %SPEAKER_ROLE% is consistent with their given background and the task instructions. Answer YES if the line directly contradicts any part of the background or task descriptio—this includes contradictions in facts, behavior, beliefs, emotional stance, or conversational strategy. Answer NO if the line introduces new details, but those details are **plausibly aligned** with the background. Be strict and literal in your evaluation: if the line violates the persona’s stated traits, mark it as a contradiction even if the violation is subtle. Background for %SPEAKER_ROLE%: %SPEAKER_BACKSTORY% Line spoken by %SPEAKER_ROLE%: %SPEAKER_LINE%  Answer YES if the line contradicts the background, and NO if it does not. Then, explain your answer in one sentence. Be precise and avoid vague justification.\"\"\"}}\n",
    "\n",
    "os.makedirs(\"chatting\", exist_ok=True)\n",
    "with open(\"chatting/config_chatting.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config_chatting, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = [\"Llama-3.1-8B-Instruct\", \"gpt-4o-mini\", \"Qwen2.5-3B-Instruct\", \"Llama-3.1-8B\", \"Mistral-7B-Instruct\", \"Llama-3.1-70B\", \"Llama-3.1-70B-Instruct\", \"phi-3.5-mini-instruct\"]\n",
    "        \n",
    "config_llm = {'agent1_model': '/nfs/kun2/users/ryan_cheng/checkpoints_raid/Chatting/llama3-8b-sft',\n",
    "             'agent2_model': 'Llama-3.1-8B-Instruct',\n",
    "             'eval_model': 'Llama-3.1-8B-Instruct',\n",
    "             'iterations': 10,\n",
    "             'verbose': False,\n",
    "             'write': True,\n",
    "             'convo_length_limit': 10,\n",
    "             'max_tokens': 256,\n",
    "             'gpus': 2,\n",
    "             'seed': 0,\n",
    "             'fp8': True,\n",
    "             'task_name': 'Chatting',\n",
    "             'model_dir': \"/raid/users/ryan_cheng/models/\",\n",
    "             'tmp_dir': \"/raid/users/ryan_cheng/tmp/\"}\n",
    "\n",
    "# with open(\"chatting/Llama-3.1-8B-Instruct.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(config_llm, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_text_after_colon(label, text):\n",
    "#     \"\"\"\n",
    "#     Extracts the text that appears after a given label followed by a colon.\n",
    "#     Example: get_text_after_colon(\"Name\", \"Name: John Doe\") returns \"John Doe\"\n",
    "#     \"\"\"\n",
    "#     pattern = rf\"{re.escape(label)}:\\s*(.*)\"\n",
    "#     match = re.search(pattern, text)\n",
    "#     return match.group(1).strip() if match else text\n",
    "\n",
    "# personas_chatting = []\n",
    "\n",
    "# for persona in personas_chatting_array:\n",
    "#     name_prompt = \"Given the following biography below, parse the name of the person. \\nOnly output the name.\\n\" + persona + \"\\nName: \"\n",
    "#     name = completion_create(\"gpt-4o-mini\", config_llm, name_prompt)\n",
    "#     name = get_text_after_colon(\"Name\", name)\n",
    "#     personas_chatting.append({'persona': persona, 'name': name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('chatting/config_chatting_personas.json', 'w') as f:\n",
    "#     json.dump(personas_chatting, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chatting/config_chatting_personas.json\", \"r\") as f:\n",
    "    personas_chatting = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_first_name(full_name):\n",
    "    return full_name.strip().split()[0]\n",
    "    \n",
    "def clean_role_prefix(response, expected_role):\n",
    "    \"\"\"\n",
    "    Removes repeated instances of the expected_role prefix at the start (e.g., 'Therapist: Therapist:'),\n",
    "    and ensures the response begins with a single correct expected_role prefix.\n",
    "    \"\"\"\n",
    "    pattern = rf\"^(({re.escape(expected_role)}):\\s*)+\"\n",
    "    cleaned = re.sub(pattern, '', response.strip(), flags=re.IGNORECASE)\n",
    "    return cleaned\n",
    "    \n",
    "def is_role_confused(response, other_role):\n",
    "    \"\"\"\n",
    "    Checks if the output starts with the wrong speaker tag.\n",
    "    \"\"\"\n",
    "    if other_role + \":\" in response:\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "\n",
    "def generate_response(agent_model, expected_role, other_role, config_llm, prompt, max_retries=3):\n",
    "    for _ in range(max_retries):\n",
    "        response = completion_create(agent_model, config_llm, prompt)\n",
    "        print(expected_role)\n",
    "        if not is_role_confused(response, other_role):\n",
    "            return clean_role_prefix(response, expected_role)\n",
    "            \n",
    "    return clean_role_prefix(response, expected_role)\n",
    "\n",
    "def generate_chat(config_llm, p1, p2, p1_name, p2_name, pturn=1):\n",
    "    stats['P1'] = p1\n",
    "    stats['P2'] = p2\n",
    "    stats['pturn'] = pturn\n",
    "    config_chatting[\"agent1_role\"] = get_first_name(p1_name)\n",
    "    config_chatting[\"agent2_role\"] = get_first_name(p2_name)\n",
    "\n",
    "    round_num = 0\n",
    "    while round_num < config_llm['convo_length_limit']:\n",
    "        conversation = (\"\".join([turn[1] if isinstance(turn, tuple) else turn for turn in stats[\"conversation\"]]) if len(stats[\"conversation\"]) != 0 else \"You are starting the conversation.\\n\")\n",
    "        \n",
    "        if pturn == 1:\n",
    "            prompt = config_chatting[\"agent1_prompt\"]\n",
    "            pturn = 2\n",
    "            if config_llm[\"verbose\"]:\n",
    "                print(prompt)\n",
    "                print()\n",
    "\n",
    "            if round_num!=0: \n",
    "                prompt+= \"Your conversation so far is below:\\nConversation: %CONVERSATION%\"\n",
    "                \n",
    "            if round_num >=config_llm['convo_length_limit']*2-11 and round_num<=config_llm['convo_length_limit']*2-1:\n",
    "                prompt+= \"You have \" + str((config_llm['convo_length_limit']-round_num)//2) + \" rounds left.\" + \"Make sure to conclude the conversation as you're near the end.\"\n",
    "\n",
    "            elif round_num>config_llm['convo_length_limit']*2-1:\n",
    "                prompt+= \"This is your concluding line in the conversation.\"\n",
    "\n",
    "            if round_num!=0: \n",
    "                prompt+= \"Continue the conversation with \" + config_chatting[\"agent2_role\"] +  \". Remember you are \" +  config_chatting[\"agent1_role\"] + \".\"\n",
    "                 \n",
    "            prompt += config_chatting[\"reminder_prompt\"] + \"DO NOT PREFACE THE RESPONSE WITH THIRD-PERSON STATEMENTS SUCH AS \\\"Sure, here's a response from...\\\"\\n\"\n",
    "            prompt+=\"%SPEAKER_ROLE%:\"\n",
    "            prompt = prompt.replace(\"%SPEAKER_ROLE%\", config_chatting[\"agent1_role\"]) \\\n",
    "                           .replace(\"%LISTENER_ROLE%\", config_chatting[\"agent2_role\"]) \\\n",
    "                           .replace(\"%SPEAKER_BACKSTORY%\", p1) \\\n",
    "                           .replace(\"%CONVERSATION%\", conversation)\n",
    "            \n",
    "            response = generate_response(config_llm['agent1_model'], config_chatting[\"agent1_role\"], config_chatting[\"agent2_role\"], config_llm, prompt)\n",
    "            # while \"\\n\\n\" in response:\n",
    "            #     print(response)\n",
    "            #     response = generate_response(config_llm['agent1_model'], config_chatting[\"agent1_role\"], config_chatting[\"agent2_role\"], config_llm, prompt)\n",
    "            stats[\"conversation\"].append((round_num, f\"{config_chatting[\"agent1_role\"]}: \" + response + \"\\n\"))\n",
    "        \n",
    "        else:\n",
    "            prompt = config_chatting[\"agent2_prompt\"]\n",
    "            pturn = 1    \n",
    "            if config_llm[\"verbose\"]:\n",
    "                print(prompt)\n",
    "                print()\n",
    "\n",
    "            if round_num!=0: \n",
    "                prompt+= \"Your conversation so far is below:\\nConversation: %CONVERSATION%\"\n",
    "            if round_num >=config_llm['convo_length_limit']*2-11 and round_num<=config_llm['convo_length_limit']*2-1:\n",
    "                prompt+= \"You have \" + str((config_llm['convo_length_limit']-round_num)//2) + \" rounds left.\" + \"Make sure to conclude the conversation as you're near the end.\"\n",
    "            elif round_num>config_llm['convo_length_limit']*2-1:\n",
    "                prompt+= \"This is your concluding line in the conversation.\"\n",
    "\n",
    "            if round_num!=0: \n",
    "                prompt+= \"Continue the conversation with \" + config_chatting[\"agent1_role\"] +  \". Remember you are \" +  config_chatting[\"agent2_role\"] + \".\"\n",
    "\n",
    "            prompt += config_chatting[\"reminder_prompt\"] + \"DO NOT PREFACE THE RESPONSE WITH THIRD-PERSON STATEMENTS SUCH AS \\\"Sure, here's a response from...\\\"\\n\"\n",
    "            prompt+=\"%SPEAKER_ROLE%:\"\n",
    "            prompt = prompt.replace(\"%SPEAKER_ROLE%\", config_chatting[\"agent2_role\"]) \\\n",
    "                           .replace(\"%LISTENER_ROLE%\", config_chatting[\"agent1_role\"]) \\\n",
    "                           .replace(\"%SPEAKER_BACKSTORY\", p2) \\\n",
    "                           .replace(\"%CONVERSATION%\", conversation)\n",
    "\n",
    "            response = generate_response(config_llm['agent2_model'], config_chatting[\"agent1_role\"], config_chatting[\"agent2_role\"], config_llm, prompt)\n",
    "            \n",
    "            # while \"\\n\\n\" in response:\n",
    "            #     print(response)\n",
    "            #     response = generate_response(config_llm['agent2_model'], config_chatting[\"agent2_role\"], config_chatting[\"agent1_role\"], config_llm, prompt)\n",
    "            stats[\"conversation\"].append((round_num, f\"{config_chatting[\"agent2_role\"]}: \" + response + \"\\n\"))\n",
    "        round_num += 1\n",
    "\n",
    "    stats[\"rounds\"] = round_num\n",
    "    if config_llm['verbose']:\n",
    "        print(stats[\"conversation\"])\n",
    "    return stats.copy()\n",
    "\n",
    "def reset_stats():\n",
    "    stats_template = {\n",
    "        \"task_name\": config_llm['task_name'],\n",
    "        \"P1\": \"\",\n",
    "        \"P2\": \"\",\n",
    "        \"conversation\": [],\n",
    "        \"pturn\": 0, # beginning person (1 or 2)\n",
    "        \"index\": -1,\n",
    "        \"timestamp\": \"\",\n",
    "        \"rounds\": 0,\n",
    "        'conversation_only': True\n",
    "    }\n",
    "    for key, value in stats_template.items():\n",
    "        stats[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import utils\n",
    "utils.config = config_llm\n",
    "\n",
    "current_date = str(datetime.now().strftime(\"%m.%d.%y\"))\n",
    "output_dir = f\"chatting/exp/{current_date}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate unique random number for filename\n",
    "def generate_unique_file_number(output_dir, prefix, seed, extension=\".json\"):\n",
    "    while True:\n",
    "        rand_num = random.randint(0, 1000)\n",
    "        filename = f\"{prefix}_{seed}_{rand_num}{extension}\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            return rand_num\n",
    "\n",
    "unique_num = generate_unique_file_number(\n",
    "    output_dir,\n",
    "    config_llm['agent1_model'],\n",
    "    config_llm['seed']\n",
    ")\n",
    "\n",
    "# File to write output to\n",
    "write_file = os.path.join(output_dir, f\"{config_llm['agent1_model']}_{config_llm['seed']}_{unique_num}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [10, 20, 40, 60]\n",
    "write_files = [os.path.join(output_dir, f\"sft_{config_llm['agent1_model'][config_llm['agent1_model'].rfind('/')+1:]}_{config_llm['seed']}_{length}_{unique_num}.json\") for length in lengths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chatting/exp/05.07.25/sft_llama3-8b-sft_0_10_754.json',\n",
       " 'chatting/exp/05.07.25/sft_llama3-8b-sft_0_20_754.json',\n",
       " 'chatting/exp/05.07.25/sft_llama3-8b-sft_0_40_754.json',\n",
       " 'chatting/exp/05.07.25/sft_llama3-8b-sft_0_60_754.json']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(personas_chatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent1_model': '/nfs/kun2/users/ryan_cheng/checkpoints_raid/Chatting/llama3-8b-sft',\n",
       " 'agent2_model': 'Llama-3.1-8B-Instruct',\n",
       " 'eval_model': 'Llama-3.1-8B-Instruct',\n",
       " 'iterations': 10,\n",
       " 'verbose': False,\n",
       " 'write': True,\n",
       " 'convo_length_limit': 10,\n",
       " 'max_tokens': 256,\n",
       " 'gpus': 2,\n",
       " 'seed': 0,\n",
       " 'fp8': True,\n",
       " 'task_name': 'Chatting',\n",
       " 'model_dir': '/raid/users/ryan_cheng/models/',\n",
       " 'tmp_dir': '/raid/users/ryan_cheng/tmp/'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 19:16:05,230\tINFO worker.py:1888 -- Started a local Ray instance.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-05-07 19:16:31,188\tINFO worker.py:1718 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-07 19:16:42 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 05-07 19:16:42 [config.py:1770] Defaulting to use mp for distributed inference\n",
      "INFO 05-07 19:16:42 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 05-07 19:16:44 [utils.py:2382] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 05-07 19:16:47 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-07 19:16:50 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir='/raid/users/ryan_cheng/models/', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-07 19:16:50 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 112 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 05-07 19:16:50 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 10485760, 10, 'psm_979d9ce4'), local_subscribe_addr='ipc:///tmp/89d836c2-c12a-40b4-bb0a-35219e14b62d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 05-07 19:16:54 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-07 19:16:54 [__init__.py:239] Automatically detected platform cuda.\n",
      "WARNING 05-07 19:16:58 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ff5e3d71fa0>\n",
      "WARNING 05-07 19:16:58 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f2159185100>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3801804)\u001b[0;0m INFO 05-07 19:16:58 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3145bec7'), local_subscribe_addr='ipc:///tmp/d70086c7-a569-435b-9fa2-3fb65634c548', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3801805)\u001b[0;0m INFO 05-07 19:16:58 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6842e214'), local_subscribe_addr='ipc:///tmp/642defb6-2dd4-4dff-af52-102605ec23cb', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3801805)\u001b[0;0m INFO 05-07 19:17:00 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3801804)\u001b[0;0m INFO 05-07 19:17:00 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3801805)\u001b[0;0m INFO 05-07 19:17:00 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3801804)\u001b[0;0m INFO 05-07 19:17:00 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3801804)\u001b[0;0m INFO 05-07 19:17:03 [custom_all_reduce_utils.py:206] generating GPU P2P access cache in /home/ryan_cheng/.cache/vllm/gpu_p2p_access_cache_for_0,2.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3801804)\u001b[0;0m INFO 05-07 19:17:22 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/ryan_cheng/.cache/vllm/gpu_p2p_access_cache_for_0,2.json\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3801805)\u001b[0;0m INFO 05-07 19:17:22 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/ryan_cheng/.cache/vllm/gpu_p2p_access_cache_for_0,2.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3801804)\u001b[0;0m INFO 05-07 19:17:22 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_6d3150d2'), local_subscribe_addr='ipc:///tmp/82d5f439-9162-4bf5-ab2e-f974a030b155', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3801805)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=3801804)\u001b[0;0m INFO 05-07 19:17:22 [parallel_state.py:1004] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "INFO 05-07 19:17:22 [parallel_state.py:1004] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3801804)\u001b[0;0m INFO 05-07 19:17:22 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3801805)\u001b[0;0m INFO 05-07 19:17:22 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3801804)\u001b[0;0m WARNING 05-07 19:17:22 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3801805)\u001b[0;0m WARNING 05-07 19:17:22 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3801804)\u001b[0;0m INFO 05-07 19:17:23 [gpu_model_runner.py:1329] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3801805)\u001b[0;0m INFO 05-07 19:17:23 [gpu_model_runner.py:1329] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3801805)\u001b[0;0m INFO 05-07 19:17:24 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3801804)\u001b[0;0m INFO 05-07 19:17:24 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.10it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=3801805)\u001b[0;0m INFO 05-07 19:17:27 [loader.py:458] Loading weights took 2.74 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3801805)\u001b[0;0m INFO 05-07 19:17:27 [gpu_model_runner.py:1347] Model loading took 7.5104 GiB and 4.114556 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.47it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.28it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.34it/s]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3801804)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=3801804)\u001b[0;0m INFO 05-07 19:17:28 [loader.py:458] Loading weights took 3.09 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3801804)\u001b[0;0m INFO 05-07 19:17:29 [gpu_model_runner.py:1347] Model loading took 7.5104 GiB and 5.712503 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3801804)\u001b[0;0m INFO 05-07 19:17:37 [backends.py:420] Using cache directory: /home/ryan_cheng/.cache/vllm/torch_compile_cache/4e58d3c3c7/rank_0_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3801805)\u001b[0;0m INFO 05-07 19:17:37 [backends.py:420] Using cache directory: /home/ryan_cheng/.cache/vllm/torch_compile_cache/4e58d3c3c7/rank_1_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3801804)\u001b[0;0m INFO 05-07 19:17:37 [backends.py:430] Dynamo bytecode transform time: 8.75 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3801805)\u001b[0;0m INFO 05-07 19:17:37 [backends.py:430] Dynamo bytecode transform time: 8.75 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3801804)\u001b[0;0m INFO 05-07 19:17:42 [backends.py:136] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3801805)\u001b[0;0m INFO 05-07 19:17:42 [backends.py:136] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3801804)\u001b[0;0m INFO 05-07 19:18:04 [backends.py:148] Compiling a graph for general shape takes 26.03 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3801805)\u001b[0;0m INFO 05-07 19:18:04 [backends.py:148] Compiling a graph for general shape takes 26.43 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3801804)\u001b[0;0m INFO 05-07 19:18:16 [monitor.py:33] torch.compile takes 34.78 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3801805)\u001b[0;0m INFO 05-07 19:18:16 [monitor.py:33] torch.compile takes 35.17 s in total\n",
      "INFO 05-07 19:18:18 [kv_cache_utils.py:634] GPU KV cache size: 363,328 tokens\n",
      "INFO 05-07 19:18:18 [kv_cache_utils.py:637] Maximum concurrency for 131,072 tokens per request: 2.77x\n",
      "INFO 05-07 19:18:18 [kv_cache_utils.py:634] GPU KV cache size: 586,528 tokens\n",
      "INFO 05-07 19:18:18 [kv_cache_utils.py:637] Maximum concurrency for 131,072 tokens per request: 4.47x\n",
      "Failed: Cuda error /workspace/csrc/custom_all_reduce.cuh:453 'invalid argument'\n",
      "Failed: Cuda error /workspace/csrc/custom_all_reduce.cuh:453 'invalid argument'\n",
      "ERROR 05-07 19:18:53 [multiproc_executor.py:123] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.\n",
      "ERROR 05-07 19:18:53 [core.py:396] EngineCore failed to start.\n",
      "ERROR 05-07 19:18:53 [core.py:396] Traceback (most recent call last):\n",
      "ERROR 05-07 19:18:53 [core.py:396]   File \"/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\n",
      "ERROR 05-07 19:18:53 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 05-07 19:18:53 [core.py:396]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-07 19:18:53 [core.py:396]   File \"/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\n",
      "ERROR 05-07 19:18:53 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "ERROR 05-07 19:18:53 [core.py:396]   File \"/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 71, in __init__\n",
      "ERROR 05-07 19:18:53 [core.py:396]     self._initialize_kv_caches(vllm_config)\n",
      "ERROR 05-07 19:18:53 [core.py:396]   File \"/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 156, in _initialize_kv_caches\n",
      "ERROR 05-07 19:18:53 [core.py:396]     self.model_executor.initialize_from_config(kv_cache_configs)\n",
      "ERROR 05-07 19:18:53 [core.py:396]   File \"/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/v1/executor/abstract.py\", line 65, in initialize_from_config\n",
      "ERROR 05-07 19:18:53 [core.py:396]     self.collective_rpc(\"compile_or_warm_up_model\")\n",
      "ERROR 05-07 19:18:53 [core.py:396]   File \"/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 181, in collective_rpc\n",
      "ERROR 05-07 19:18:53 [core.py:396]     status, result = w.worker_response_mq.dequeue(\n",
      "ERROR 05-07 19:18:53 [core.py:396]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-07 19:18:53 [core.py:396]   File \"/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/distributed/device_communicators/shm_broadcast.py\", line 479, in dequeue\n",
      "ERROR 05-07 19:18:53 [core.py:396]     with self.acquire_read(timeout, cancel) as buf:\n",
      "ERROR 05-07 19:18:53 [core.py:396]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-07 19:18:53 [core.py:396]   File \"/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/contextlib.py\", line 137, in __enter__\n",
      "ERROR 05-07 19:18:53 [core.py:396]     return next(self.gen)\n",
      "ERROR 05-07 19:18:53 [core.py:396]            ^^^^^^^^^^^^^^\n",
      "ERROR 05-07 19:18:53 [core.py:396]   File \"/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/distributed/device_communicators/shm_broadcast.py\", line 438, in acquire_read\n",
      "ERROR 05-07 19:18:53 [core.py:396]     raise RuntimeError(\"cancelled\")\n",
      "ERROR 05-07 19:18:53 [core.py:396] RuntimeError: cancelled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process EngineCore_0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 400, in run_engine_core\n",
      "    raise e\n",
      "  File \"/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\n",
      "    engine_core = EngineCoreProc(*args, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\n",
      "    super().__init__(vllm_config, executor_class, log_stats,\n",
      "  File \"/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 71, in __init__\n",
      "    self._initialize_kv_caches(vllm_config)\n",
      "  File \"/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 156, in _initialize_kv_caches\n",
      "    self.model_executor.initialize_from_config(kv_cache_configs)\n",
      "  File \"/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/v1/executor/abstract.py\", line 65, in initialize_from_config\n",
      "    self.collective_rpc(\"compile_or_warm_up_model\")\n",
      "  File \"/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 181, in collective_rpc\n",
      "    status, result = w.worker_response_mq.dequeue(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/distributed/device_communicators/shm_broadcast.py\", line 479, in dequeue\n",
      "    with self.acquire_read(timeout, cancel) as buf:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/contextlib.py\", line 137, in __enter__\n",
      "    return next(self.gen)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/distributed/device_communicators/shm_broadcast.py\", line 438, in acquire_read\n",
      "    raise RuntimeError(\"cancelled\")\n",
      "RuntimeError: cancelled\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m config_llm[\u001b[33m'\u001b[39m\u001b[33mconvo_length_limit\u001b[39m\u001b[33m'\u001b[39m] = convo_length\n\u001b[32m      9\u001b[39m reset_stats()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m conversation = \u001b[43mgenerate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_llm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp_dict1\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpersona\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp_dict2\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpersona\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp_dict1\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp_dict2\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpturn\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     17\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(conversation)\n\u001b[32m     19\u001b[39m conversations.append(conversation)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mgenerate_chat\u001b[39m\u001b[34m(config_llm, p1, p2, p1_name, p2_name, pturn)\u001b[39m\n\u001b[32m     94\u001b[39m prompt+=\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mSPEAKER_ROLE\u001b[39m\u001b[33m%\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m prompt = prompt.replace(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mSPEAKER_ROLE\u001b[39m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m, config_chatting[\u001b[33m\"\u001b[39m\u001b[33magent2_role\u001b[39m\u001b[33m\"\u001b[39m]) \\\n\u001b[32m     96\u001b[39m                .replace(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mLISTENER_ROLE\u001b[39m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m, config_chatting[\u001b[33m\"\u001b[39m\u001b[33magent1_role\u001b[39m\u001b[33m\"\u001b[39m]) \\\n\u001b[32m     97\u001b[39m                .replace(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mSPEAKER_BACKSTORY\u001b[39m\u001b[33m\"\u001b[39m, p2) \\\n\u001b[32m     98\u001b[39m                .replace(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mCONVERSATION\u001b[39m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m, conversation)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m response = \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_llm\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43magent2_model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_chatting\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43magent1_role\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_chatting\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43magent2_role\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_llm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# while \"\\n\\n\" in response:\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m#     print(response)\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m#     response = generate_response(config_llm['agent2_model'], config_chatting[\"agent2_role\"], config_chatting[\"agent1_role\"], config_llm, prompt)\u001b[39;00m\n\u001b[32m    105\u001b[39m stats[\u001b[33m\"\u001b[39m\u001b[33mconversation\u001b[39m\u001b[33m\"\u001b[39m].append((round_num, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_chatting[\u001b[33m\"\u001b[39m\u001b[33magent2_role\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m + response + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mgenerate_response\u001b[39m\u001b[34m(agent_model, expected_role, other_role, config_llm, prompt, max_retries)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_response\u001b[39m(agent_model, expected_role, other_role, config_llm, prompt, max_retries=\u001b[32m3\u001b[39m):\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_retries):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m         response = \u001b[43mcompletion_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_llm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m         \u001b[38;5;28mprint\u001b[39m(expected_role)\n\u001b[32m     28\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_role_confused(response, other_role):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/kun2/users/ryan_cheng/consistency_LLMs/utils.py:223\u001b[39m, in \u001b[36mcompletion_create\u001b[39m\u001b[34m(model_name, config, prompt, keep_trying)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompletion_create\u001b[39m(model_name, config, prompt, keep_trying=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompletion_create_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (openai.APIError, openai.OpenAIError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    225\u001b[39m         \u001b[38;5;66;03m# print(\"ERROR\", e)\u001b[39;00m\n\u001b[32m    226\u001b[39m         \u001b[38;5;66;03m# print(\"sleeping for 10 seconds.\")\u001b[39;00m\n\u001b[32m    227\u001b[39m         time.sleep(\u001b[32m10\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/kun2/users/ryan_cheng/consistency_LLMs/utils.py:159\u001b[39m, in \u001b[36mcompletion_create_helper\u001b[39m\u001b[34m(model_name, config, prompt)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompletion_create_helper\u001b[39m(model_name, config, prompt):\n\u001b[32m    152\u001b[39m     \u001b[38;5;66;03m# # limit prompt in all cases\u001b[39;00m\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# if model_name not in vllm_alias:\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m#     # for some reason vLLM models simply repeat this last statement if present\u001b[39;00m\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m#     prompt += \" Limit your answer to three sentences or less!\"\u001b[39;00m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (model_name \u001b[38;5;129;01min\u001b[39;00m vllm_alias \u001b[38;5;129;01mand\u001b[39;00m model_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m llms) \u001b[38;5;129;01mor\u001b[39;00m (model_name[\u001b[32m0\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m model_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m llms):\n\u001b[32m    158\u001b[39m         \u001b[38;5;66;03m# set up vllm if not already set up\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m         \u001b[43msetup_vllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m     ret = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;66;03m# return the output ret at the end and use to calculate cost\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_name == \u001b[33m\"\u001b[39m\u001b[33mgpt-3.5-turbo-instruct\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/kun2/users/ryan_cheng/consistency_LLMs/utils.py:131\u001b[39m, in \u001b[36msetup_vllm\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mfp8\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config.keys() \u001b[38;5;129;01mand\u001b[39;00m config[\u001b[33m'\u001b[39m\u001b[33mfp8\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m    130\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUsing fp8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     llms[model] = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_alias\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgpus\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_dir\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.75\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m vllm_alias[model] == \u001b[33m'\u001b[39m\u001b[33mmeta-llama/Meta-Llama-3.1-70B\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m vllm_alias[model] == \u001b[33m'\u001b[39m\u001b[33mmeta-llama/Meta-Llama-3.1-70B-Instruct\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    133\u001b[39m     llms[model] = LLM(model=vllm_alias[model], tensor_parallel_size=config[\u001b[33m'\u001b[39m\u001b[33mgpus\u001b[39m\u001b[33m'\u001b[39m], download_dir=config[\u001b[33m'\u001b[39m\u001b[33mmodel_dir\u001b[39m\u001b[33m'\u001b[39m], gpu_memory_utilization=\u001b[32m0.95\u001b[39m, max_model_len=\u001b[32m12880\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/utils.py:1161\u001b[39m, in \u001b[36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1154\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1156\u001b[39m         warnings.warn(\n\u001b[32m   1157\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1158\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1159\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/entrypoints/llm.py:247\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[39m\n\u001b[32m    217\u001b[39m engine_args = EngineArgs(\n\u001b[32m    218\u001b[39m     model=model,\n\u001b[32m    219\u001b[39m     task=task,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m     **kwargs,\n\u001b[32m    244\u001b[39m )\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    251\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/engine/llm_engine.py:510\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    507\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv1\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[32m    508\u001b[39m     engine_cls = V1LLMEngine\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:112\u001b[39m, in \u001b[36mLLMEngine.from_vllm_config\u001b[39m\u001b[34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_vllm_config\u001b[39m(\n\u001b[32m    106\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    110\u001b[39m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    111\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLLMEngine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m               \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mExecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m               \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m               \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m               \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m               \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVLLM_ENABLE_V1_MULTIPROCESSING\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:92\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mself\u001b[39m.output_processor = OutputProcessor(\u001b[38;5;28mself\u001b[39m.tokenizer,\n\u001b[32m     89\u001b[39m                                         log_stats=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# FIXME: implement\u001b[39;49;00m\n\u001b[32m     98\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multiprocess_mode:\n\u001b[32m    101\u001b[39m     \u001b[38;5;66;03m# for v0 compatibility\u001b[39;00m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_executor = \u001b[38;5;28mself\u001b[39m.engine_core.engine_core.model_executor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:73\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m AsyncMPClient(vllm_config, executor_class, log_stats)\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:494\u001b[39m, in \u001b[36mSyncMPClient.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[32m    493\u001b[39m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m     \u001b[38;5;28mself\u001b[39m.outputs_queue = queue.Queue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n\u001b[32m    503\u001b[39m     \u001b[38;5;66;03m# Ensure that the outputs socket processing thread does not have\u001b[39;00m\n\u001b[32m    504\u001b[39m     \u001b[38;5;66;03m# a ref to the client which prevents gc.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:398\u001b[39m, in \u001b[36mMPClient.__init__\u001b[39m\u001b[34m(self, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    394\u001b[39m \u001b[38;5;28mself\u001b[39m._init_core_engines(vllm_config, new_core_engine,\n\u001b[32m    395\u001b[39m                         \u001b[38;5;28mself\u001b[39m.resources.core_engines)\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# Wait for engine core process(es) to start.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[38;5;28mself\u001b[39m.utility_results: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, AnyFuture] = {}\n\u001b[32m    402\u001b[39m \u001b[38;5;66;03m# Request objects which may contain pytorch-allocated tensors\u001b[39;00m\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# that we need to keep references to until zmq is done with the\u001b[39;00m\n\u001b[32m    404\u001b[39m \u001b[38;5;66;03m# underlying data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/kun2/users/ryan_cheng/miniconda3/envs/openrlhf3/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:430\u001b[39m, in \u001b[36mMPClient._wait_for_engine_startup\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    427\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(events) > \u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m events[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m] != sync_input_socket:\n\u001b[32m    429\u001b[39m     \u001b[38;5;66;03m# One of the core processes exited.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEngine core initialization failed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    431\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mSee root cause above.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    433\u001b[39m eng_id_bytes, msg = sync_input_socket.recv_multipart()\n\u001b[32m    434\u001b[39m eng_id = \u001b[38;5;28mint\u001b[39m.from_bytes(eng_id_bytes, byteorder=\u001b[33m\"\u001b[39m\u001b[33mlittle\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Engine core initialization failed. See root cause above."
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# index_offset = load_stats_file(write_file)\n",
    "conversations = []    \n",
    "\n",
    "for i in range(1):\n",
    "    for p_dict1, p_dict2 in tqdm(np.array(personas_chatting).reshape(-1, 2)):\n",
    "        for j, convo_length in enumerate(lengths):\n",
    "            index_offset = load_stats_file(write_files[j])\n",
    "            config_llm['convo_length_limit'] = convo_length\n",
    "            reset_stats()\n",
    "            conversation = generate_chat(\n",
    "                config_llm,\n",
    "                p_dict1[\"persona\"], \n",
    "                p_dict2[\"persona\"],\n",
    "                p_dict1[\"name\"], \n",
    "                p_dict2[\"name\"], \n",
    "                pturn=1\n",
    "            )\n",
    "            print(conversation)\n",
    "            conversations.append(conversation)\n",
    "            stats['index'] = index_offset\n",
    "            stats['timestamp'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            write_stats(write_files[j])\n",
    "            index_offset += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prompt_consistency(conv_dict):\n",
    "    #assert 'eval_prompt_consistency' not in conv_dict # warn if we are replacing metrics we don't mean to overwrite\n",
    "    conv_dict['eval_prompt_consistency'] = []\n",
    "    conv_dict['P1_prompt_consistency_score'] = 0\n",
    "    conv_dict['P2_prompt_consistency_score'] = 0\n",
    "    p2_utterances = 0\n",
    "    p1_utterances = 0\n",
    "    pturn = conv_dict[\"pturn\"]\n",
    "    for line in conv_dict[\"conversation\"]:\n",
    "        line_number = line[0]\n",
    "        convo_line = line[1]\n",
    "        if pturn == 1:\n",
    "            prompt = config_chatting[\"eval_prompts\"][\"prompt_consistency\"].replace(\"%SPEAKER_ROLE%\", config_chatting[\"agent1_role\"]) \\\n",
    "                                                                          .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P1\"]) \\\n",
    "                                                                          .replace(\"%SPEAKER_LINE%\", convo_line)\n",
    "            if config_llm['verbose']:\n",
    "                print(prompt)\n",
    "            output = completion_create(config_llm['eval_model'], config, prompt)\n",
    "            conv_dict['eval_prompt_consistency'].append((line_number, output))\n",
    "            if \"YES\" not in output: # no contradiction\n",
    "                conv_dict['P1_prompt_consistency_score'] += 1\n",
    "            p1_utterances += 1\n",
    "            pturn = 2\n",
    "        elif pturn == 2:\n",
    "            prompt = config_chatting[\"eval_prompts\"][\"prompt_consistency\"].replace(\"%SPEAKER_ROLE%\", config_chatting[\"agent2_role\"]) \\\n",
    "                                                                          .replace(\"%SPEAKER_BACKSTORY%\", conv_dict[\"P2\"]) \\\n",
    "                                                                          .replace(\"%SPEAKER_LINE%\", convo_line)\n",
    "            if config_llm['verbose']:\n",
    "                print(prompt)\n",
    "            output = completion_create(config_llm['eval_model'], config, prompt)\n",
    "            conv_dict['eval_prompt_consistency'].append((line_number, output))\n",
    "            if \"YES\" not in output: # no contradiction\n",
    "                conv_dict['P2_prompt_consistency_score'] += 1\n",
    "            p2_utterances += 1\n",
    "            pturn = 1 \n",
    "            \n",
    "    if p1_utterances > 0:\n",
    "        conv_dict['P1_prompt_consistency_score'] /= p1_utterances\n",
    "\n",
    "    if p2_utterances > 0:\n",
    "        conv_dict['P2_prompt_consistency_score'] /= p2_utterances\n",
    "        \n",
    "    print(conv_dict)\n",
    "    return conv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.config = config_llm\n",
    "\n",
    "with open(\"chatting/exp/04.22.25/Llama-3.1-8B-Instruct_0_724.json\", \"r\") as f:\n",
    "    conversations = json.load(f)\n",
    "\n",
    "for conversation in tqdm(conversations):\n",
    "    conversation = eval_prompt_consistency(conversation)\n",
    "\n",
    "write_stats(write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(write_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(conversations, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openrlhf3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
